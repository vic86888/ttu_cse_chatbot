# ingest.py
import os
import json
import re
import hashlib
import sys
from pathlib import Path
from typing import List, Dict, Any

# å¼·åˆ¶ä½¿ç”¨ safetensors æ ¼å¼ä»¥é¿å… PyTorch 2.5.1 çš„å®‰å…¨é™åˆ¶
os.environ["TRANSFORMERS_PREFER_SAFETENSORS"] = "1"
os.environ["SENTENCE_TRANSFORMERS_USE_SAFETENSORS"] = "1"

from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader
from langchain_community.document_loaders import CSVLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain.schema import Document

from datetime import datetime
from zoneinfo import ZoneInfo

from collections import defaultdict

# ingest.py é–‹é ­
from json_rewriter import rewrite_json_record

DATA_DIR = Path("data_qwen")
DB_DIR = "storage/chroma"
COLL_NAME = "campus_rag"

# =========================
# JSON schema è‡ªå‹•åµæ¸¬
# =========================
def detect_schema(obj: Any) -> str:
    """
    å›å‚³ "people" / "news" / "school" / "unknown"
    - people: æœ‰ã€Œäººç‰©ã€ã€Œé›»è©±ã€ã€Œä¿¡ç®±ã€ç­‰éµ
    - news:   æœ‰ "url","title","published_at","content"ï¼ˆå¯é¡å¤–å« "category"ï¼‰
    - school: æœ‰ã€Œåç¨±ã€ã€Œè‹±æ–‡åç¨±ã€ã€Œæ ¡è¨“ã€ç­‰éµï¼ˆä¾‹å¦‚ about_schoo.jsonï¼‰
    """
    sample = None
    if isinstance(obj, list) and obj:
        sample = obj[0]
    elif isinstance(obj, dict):
        # æª¢æŸ¥æ˜¯å¦æ˜¯æ–°æ ¼å¼çš„æ•™è·å“¡è³‡æ–™ï¼ˆæœ‰ "ç¸½è¦½" å’Œ "æˆå“¡åˆ—è¡¨" éµï¼‰
        if "ç¸½è¦½" in obj and isinstance(obj.get("ç¸½è¦½"), dict):
            overview = obj["ç¸½è¦½"]
            if "æˆå“¡åˆ—è¡¨" in overview:
                return "people"
        # æª¢æŸ¥æ˜¯å¦æ˜¯èˆŠæ ¼å¼çš„æ•™è·å“¡è³‡æ–™ï¼ˆæœ‰ "æˆå“¡åˆ—è¡¨" éµï¼‰
        if "æˆå“¡åˆ—è¡¨" in obj and isinstance(obj["æˆå“¡åˆ—è¡¨"], list):
            return "people"
        elif isinstance(obj, dict):
            # --- âœ… æ–°æ ¼å¼ï¼šèª²ç¨‹æ­·å²ï¼ˆå·¢ç‹€ï¼š113ä¸Š/113ä¸‹ â†’ å¹´ç´š â†’ èª²ç¨‹åˆ—è¡¨ï¼‰ ---
            term_keys = [
                k for k in obj.keys()
                if re.match(r"^\d{2,3}[ä¸Šä¸‹]$", str(k).strip())
            ]
            if term_keys:
                v0 = obj.get(term_keys[0])
                if isinstance(v0, dict):
                    # ä»»ä¸€ grade block å…§å«ã€Œèª²ç¨‹åˆ—è¡¨ã€å°±è¦–ç‚ºæ–°æ ¼å¼
                    if any(isinstance(gv, dict) and "èª²ç¨‹åˆ—è¡¨" in gv for gv in v0.values()):
                        return "course_history_nested"

            # 3) âœ… åœ¨é€™è£¡åŠ ï¼šå¿…ä¿®ç§‘ç›®(æª¢æ ¸)è¡¨ required_by_semester
            if "semesters" in obj and isinstance(obj["semesters"], dict):
                semesters = obj["semesters"]
                for v in semesters.values():
                    # æ‰¾åˆ°ç¬¬ä¸€å€‹æœ‰è³‡æ–™çš„å­¸æœŸä¾†çœ‹
                    if isinstance(v, list) and v:
                        first = v[0]
                        if isinstance(first, dict):
                            inner_keys = set(first.keys())
                            # é€™å¹¾å€‹æ˜¯é€™å€‹ JSON å¾ˆæœ‰ç‰¹è‰²çš„æ¬„ä½
                            if {"raw", "å­¸åˆ†", "å…±åŒå¿…ä¿®å°è¨ˆ", "å°ˆæ¥­å¿…ä¿®å°è¨ˆ"} <= inner_keys:
                                return "required_by_semester"
                        break  # çœ‹ä¸€å€‹æ¨£æœ¬å°±å¤ äº†

        sample = obj
    else:
        return "unknown"

    keys = set(sample.keys())
    # è€å¸«åéŒ„ (æ”¯æ´å…©ç¨®æ ¼å¼: èˆŠæ ¼å¼ç”¨ã€Œäººç‰©ã€, æ–°æ ¼å¼ç”¨ã€Œå§“åã€+ã€Œè·ç¨±ã€+ã€Œç³»æ‰€ã€)
    if {"äººç‰©", "é›»è©±", "ä¿¡ç®±"} & keys or {"å§“å", "è·ç¨±", "ä¿¡ç®±"} <= keys:
        return "people"
    # ç³»ç¶²æ–°è
    if {"url", "title", "published_at", "content"} <= keys:
        return "news"
    # å­¸æ ¡åŸºæœ¬è³‡æ–™ï¼ˆabout_schoo.jsonï¼‰
    if {"åç¨±", "è‹±æ–‡åç¨±", "æ ¡è¨“"} <= keys:
        return "school"
    if "é¡åˆ¥" in keys and ("å…§å®¹" in keys or "èªªæ˜" in keys):
        return "academic_rules"
    
    # ğŸ”¹ æ–°å¢ï¼šæ•¸ä½æ•™å­¸èª²ç¨‹å¯¦æ–½è¦é»ï¼‹å½ˆæ€§æ•™å­¸é€±
    if {"å¯¦æ–½è¦é»", "å½ˆæ€§æ•™å­¸é€±æ´»å‹•è¦åŠƒ"} <= keys:
        return "flexible_week_rules"

    if ("è¾¦ç†é …ç›®" in keys and "æ‰¿è¾¦äºº" in keys) or ("å­¸ç³»" in keys and "è¯çµ¡äººå“¡" in keys):
        return "contacts"
    if {"å­¸å¹´å­¸æœŸ", "èª²è™Ÿ", "èª²ç¨‹åç¨±", "æ•™å¸«"} <= keys:
        return "course_history"
    if {"é¸åˆ¥", "å­¸å¹´å­¸æœŸ", "æ‰€å±¬å¹´ç´š", "èª²ç¨‹åç¨±"} <= keys and not ({"èª²è™Ÿ", "æ•™å¸«"} & keys):
        return "course_overview"
    # äººå·¥æ™ºæ…§å­¸åˆ†å­¸ç¨‹ï¼ˆæˆ–å…¶ä»–å­¸åˆ†å­¸ç¨‹ï¼‰èª²ç¨‹æ¸…å–®
    # ç‰¹è‰²éµï¼šæœ‰ã€Œè¨­ç½®å®—æ—¨/é©ç”¨å°è±¡/èª²ç¨‹ä»£ç¢¼/èª²ç¨‹åç¨±/å­¸åˆ†æ•¸ã€
    if {"è¨­ç½®å®—æ—¨", "é©ç”¨å°è±¡", "èª²ç¨‹ä»£ç¢¼", "èª²ç¨‹åç¨±", "å­¸åˆ†æ•¸"} <= keys:
        return "program_courses"
    
    # ğŸ”¹ æ–°å¢ï¼šå§Šå¦¹æ ¡åˆ—è¡¨ï¼ˆå¤§åŒå¤§å­¸å§Šå¦¹æ ¡ï¼‰
    if "continents" in keys and "title" in keys and ("source" in keys or "ä¾†æº" in keys):
        return "sister_schools"
    
    # ğŸ”¹ 2025 æ˜¥å­£å§Šå¦¹æ ¡äº¤æ› / é›™è¯å­¸ä½å…¬å‘Šé€™é¡ JSON
    if {"title", "url", "section1", "section2", "section3", "section4"} <= keys:
        return "exchange_program_call"

    # ğŸ”¹ è¦ç«  / è¾¦æ³• / çå­¸é‡‘è¦é»ï¼ˆèˆŠæ ¼å¼ï¼šurl + title + articles[{article_no,text}]ï¼‰
    if {"url", "title", "articles"} <= keys:
        return "school_rule_articles"

    # ğŸ”¹ è¦ç«  / è¾¦æ³•ï¼ˆæ–°ç‰ˆï¼šsource_page + file_url + file_name + articles[{heading, body}]ï¼‰
    if {"source_page", "title", "file_name", "file_url", "articles"} <= keys:
        return "school_rule_file_articles"
    
    # ğŸ”¹ å–®é è¦å‰‡ï¼ˆä¾‹å¦‚ å¤§åŒå¤§å­¸å­¸ç”Ÿè«‹å‡è¦å‰‡ï¼‰
    if {"title", "page_title", "source_page", "pdf_url", "prefix", "items"} <= keys:
        return "single_page_rule"

        # è¡Œäº‹æ›† / æ ¡å‹™æ—¥ç¨‹
    # ç‰¹è‰²éµï¼šæœ‰ã€Œå¹´/æœˆ/æ—¥/æ´»å‹•äº‹é …ã€ï¼ˆé€šå¸¸é‚„æœ‰ æ˜ŸæœŸã€è³‡æ–™ä¾†æºï¼‰
    if {"å¹´", "æœˆ", "æ—¥", "æ´»å‹•äº‹é …"} <= keys:
        return "calendar"
    return "unknown"

# =========================
# ttu_single_page_rules.json adapter
# =========================

def single_page_rule_to_documents(
    obj: Dict[str, Any],
    source_path: str | Path,
) -> List[Document]:
    """
    å°‡ ttu_single_page_rules.json é€™ç¨®ã€Œå–®é èªªæ˜ + å¤šå€‹æ¢æ¬¾é …ç›®ã€çš„è¦å‰‡ï¼Œ
    è½‰æˆå¤šç­† Documentï¼š

    çµæ§‹ç¤ºæ„ï¼š
    {
      "title": "å¤§åŒå¤§å­¸å­¸ç”Ÿè«‹å‡è¦å‰‡",
      "page_title": "å¤§åŒå¤§å­¸ ç”Ÿæ´»è¼”å°çµ„",
      "source_page": "...",
      "pdf_url": "...",
      "pdf_link_text": "...",
      "prefix": "é•·æ®µæ–‡å­—ï¼Œå«ä¿®æ­£ç´€éŒ„ + ä¸€ã€äºŒã€ä¸‰â€¦",
      "items": [
        { "item": "ä¸€", "text": "å…¬å‡ï¼š..." },
        { "item": "äºŒ", "text": "ç—…å‡ï¼š..." },
        ...
      ]
    }

    è¼¸å‡ºï¼š
    1) ä¸€ç­†ã€Œè¦å‰‡ç¸½è¦½ã€ school_rule_overview
    2) å¤šç­†ã€Œæ¢æ¬¾æ‘˜è¦ã€ school_rule_articleï¼ˆæ¯å€‹ items[*] ä¸€ç­†ï¼‰
    """
    docs: List[Document] = []

    source_path_str = str(source_path)
    title = str(obj.get("title") or "").strip()
    page_title = str(obj.get("page_title") or "").strip()
    source_page = str(obj.get("source_page") or "").strip()
    pdf_url = str(obj.get("pdf_url") or "").strip()
    pdf_link_text = str(obj.get("pdf_link_text") or "").strip()
    prefix = str(obj.get("prefix") or "").strip()

    items = obj.get("items") or []
    if not isinstance(items, list):
        items = []

    # å°å¤–ä½¿ç”¨çš„ä¸»ç¶²å€ï¼ˆæœ‰ source_page å°±å„ªå…ˆç”¨å®ƒï¼‰
    main_url = source_page or pdf_url

    # è¦å‰‡é¡å‹ï¼ˆé€™ä»½æ˜¯è«‹å‡è¦å‰‡ï¼Œå°±ç•¶æˆ leave_ruleï¼‰
    rule_kind = "leave_rule"

    idx = 0

    # === (1) è¦å‰‡ç¸½è¦½ Doc ===

    # å¾ items æŠ“å¹¾æ¢ç•¶ã€Œæ¢æ¬¾æ‘˜è¦ã€
    summary_items: List[Dict[str, str]] = []
    for it in items[:6]:  # æœ€å¤šæ‹¿å‰ 6 å€‹é …ç›®åšæˆæ¦‚è¦
        if not isinstance(it, dict):
            continue
        item_label = str(it.get("item") or "").strip()
        text = str(it.get("text") or "").strip()
        if not text:
            continue
        summary_items.append(
            {
                "é …ç›®ä»£è™Ÿ": item_label,
                "å…§å®¹é–‹é ­": text[:80],
            }
        )

    overview_record: Dict[str, Any] = {
        "è¦ç« æ¨™é¡Œ": title,
        "é é¢æ¨™é¡Œ": page_title,
        "è¦ç« ç¶²å€": main_url,
        "è¦ç« é¡å‹": rule_kind,  # leave_rule
        "PDFæª”å": pdf_link_text,
        "PDFç¶²å€": pdf_url,
        "å‰è¨€æ‘˜è¦": prefix[:400],  # å‰ 400 å­—ç•¶æ‘˜è¦ï¼Œé¿å…å¤ªé•·
        "æ¢æ¬¾é …ç›®æ‘˜è¦åˆ—è¡¨": summary_items,
        "è³‡æ–™ä¾†æº": main_url,
    }

    try:
        overview_text = rewrite_json_record(
            record=overview_record,
            schema_hint="school_rule_overview",
            max_chars=900,
        )
    except Exception as e:
        print(
            "[single_page_rule_to_documents] "
            f"rewrite_json_record (overview) ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}"
        )
        sys.exit(1)

    overview_meta = {
        "source": source_path_str,
        "file_type": "json",
        "content_type": "school_rule_overview",

        "title": title,
        "url": main_url,
        "rule_kind": rule_kind,
        "item_count": len(items),

        "page_title": page_title,
        "pdf_url": pdf_url,
        "pdf_link_text": pdf_link_text,

        "idx": idx,
        "needs_split": False,
    }
    docs.append(Document(page_content=overview_text.strip(), metadata=overview_meta))
    idx += 1

    # === (2) æ¯ä¸€å€‹ items[*] ç”¢ç”Ÿä¸€ç­†æ¢æ¬¾ Doc ===

    for it in items:
        if not isinstance(it, dict):
            continue

        item_label = str(it.get("item") or "").strip()
        text = str(it.get("text") or "").strip()
        if not text:
            continue

        record_item: Dict[str, Any] = {
            "è¦ç« æ¨™é¡Œ": title,
            "è¦ç« ç¶²å€": main_url,
            "è¦ç« é¡å‹": rule_kind,
            "é …ç›®ä»£è™Ÿ": item_label,
            "æ¢æ¬¾å…§å®¹": text,
            "è³‡æ–™ä¾†æº": main_url,
        }

        try:
            item_text = rewrite_json_record(
                record=record_item,
                schema_hint="school_rule_article",
                max_chars=500,
            )
        except Exception as e:
            print(
                "[single_page_rule_to_documents] "
                f"rewrite_json_record (item {item_label}) ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}"
            )
            sys.exit(1)

        item_meta = {
            "source": source_path_str,
            "file_type": "json",
            "content_type": "school_rule_article",

            "title": title,
            "url": main_url,
            "rule_kind": rule_kind,
            "item_label": item_label,

            "idx": idx,
            "needs_split": False,
        }

        docs.append(Document(page_content=item_text.strip(), metadata=item_meta))
        idx += 1

    return docs

# =========================
# cse_shishi_banfa.json adapter
# =========================

def school_rule_file_articles_to_documents(
    obj: Dict[str, Any],
    source_path: str | Path,
) -> List[Document]:
    """
    å°‡ cse_shishi_banfa.json é€™ç¨®ã€Œæœ‰é™„æª”çš„ç³»è¦ / è¾¦æ³•ã€è½‰æˆå¤šç­† Documentã€‚

    çµæ§‹ï¼š
    {
      "source_page": "...",
      "title": "å¤§åŒå¤§å­¸è³‡è¨Šå·¥ç¨‹ç³»(æ‰€)å­¸ç”Ÿä¿®è®€å­¸ã€ç¢©å£« äº”å¹´ä¸€è²«å­¸ç¨‹è¾¦æ³•",
      "file_name": "545079179.docx",
      "file_url": "https://cse.ttu.edu.tw/var/file/58/1058/img/104/545079179.docx",
      "articles": [
        { "heading": "ç¬¬ä¸€æ¢", "body": "..." },
        { "heading": "ç¬¬äºŒæ¢", "body": "..." },
        ...
      ]
    }

    è¼¸å‡ºï¼š
    1) ä¸€ç­†ã€Œè¾¦æ³•ç¸½è¦½ã€ school_rule_overview
    2) å¤šç­†ã€Œæ¢æ–‡æ‘˜è¦ã€ school_rule_article
    """
    docs: List[Document] = []

    source_path_str = str(source_path)
    source_page = str(obj.get("source_page") or "").strip()
    title = str(obj.get("title") or "").strip()
    file_name = str(obj.get("file_name") or "").strip()
    file_url = str(obj.get("file_url") or "").strip()
    articles = obj.get("articles") or []
    if not isinstance(articles, list):
        articles = []

    # ğŸ”¹ åˆ¤æ–·æ˜¯ã€Œçå­¸é‡‘é¡ã€é‚„æ˜¯ä¸€èˆ¬å­¸å‰‡ / ç³»è¦
    if "çå­¸é‡‘" in title or "å‹µå­¸" in title:
        rule_kind = "scholarship_rule"
    else:
        rule_kind = "academic_rule"

    # å°å¤–çµ±ä¸€ç”¨é€™å€‹ç¶²å€æ¬„ä½
    main_url = source_page or file_url

    idx = 0

    # === (1) è¾¦æ³•ç¸½è¦½ Doc ===
    summary_items: List[Dict[str, str]] = []
    for art in articles[:6]:  # æœ€å¤šæ‹¿å‰ 6 æ¢ä¾†ç•¶æ¦‚è¦
        if not isinstance(art, dict):
            continue
        heading = str(art.get("heading") or "").strip()
        body = str(art.get("body") or "").strip()
        if not body:
            continue
        summary_items.append(
            {
                "æ¢è™Ÿ": heading,
                "æ¢æ–‡é–‹é ­": body[:80],  # åªæˆªå‰é¢ä¸€å°æ®µï¼Œè®“é‡å¯«å™¨æŒæ¡é‡é»
            }
        )

    overview_record: Dict[str, Any] = {
        "è¦ç« æ¨™é¡Œ": title,
        "è¦ç« ç¶²å€": main_url,
        "è¦ç« é¡å‹": rule_kind,  # scholarship_rule / academic_rule
        "é™„ä»¶æª”å": file_name,
        "é™„ä»¶ç¶²å€": file_url,
        "æ¢æ–‡ç¸½æ•¸": len(articles),
        "æ¢æ–‡æ‘˜è¦åˆ—è¡¨": summary_items,
        "è³‡æ–™ä¾†æº": main_url,
    }

    try:
        overview_text = rewrite_json_record(
            record=overview_record,
            schema_hint="school_rule_overview",
            max_chars=900,
        )
    except Exception as e:
        print(
            "[school_rule_file_articles_to_documents] "
            f"rewrite_json_record (overview) ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}"
        )
        sys.exit(1)

    overview_meta = {
        "source": source_path_str,
        "file_type": "json",
        "content_type": "school_rule_overview",

        "title": title,
        "url": main_url,
        "rule_kind": rule_kind,
        "article_count": len(articles),

        "source_page": source_page,
        "file_name": file_name,
        "file_url": file_url,

        "idx": idx,
        "needs_split": False,
    }
    docs.append(Document(page_content=overview_text.strip(), metadata=overview_meta))
    idx += 1

    # === (2) æ¯æ¢æ¢æ–‡å„ä¸€å€‹ Doc ===
    for art in articles:
        if not isinstance(art, dict):
            continue

        heading = str(art.get("heading") or "").strip()
        body = str(art.get("body") or "").strip()
        if not body:
            continue

        record_article: Dict[str, Any] = {
            "è¦ç« æ¨™é¡Œ": title,
            "è¦ç« ç¶²å€": main_url,
            "è¦ç« é¡å‹": rule_kind,
            "æ¢è™Ÿ": heading,
            "æ¢æ–‡å…§å®¹": body,
            "é™„ä»¶æª”å": file_name,
            "é™„ä»¶ç¶²å€": file_url,
            "è³‡æ–™ä¾†æº": main_url,
        }

        try:
            article_text = rewrite_json_record(
                record=record_article,
                schema_hint="school_rule_article",
                max_chars=500,
            )
        except Exception as e:
            print(
                "[school_rule_file_articles_to_documents] "
                f"rewrite_json_record (article {heading}) ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}"
            )
            sys.exit(1)

        article_meta = {
            "source": source_path_str,
            "file_type": "json",
            "content_type": "school_rule_article",

            "title": title,
            "url": main_url,
            "rule_kind": rule_kind,
            "article_no": heading,

            "source_page": source_page,
            "file_name": file_name,
            "file_url": file_url,

            "idx": idx,
            "needs_split": False,
        }

        docs.append(
            Document(page_content=article_text.strip(), metadata=article_meta)
        )
        idx += 1

    return docs

# =========================
# activity.ttu.edu.tw_405-1036-4940_php.json
# cse.ttu.edu.tw_404-1058-2974_php.json
# cse.ttu.edu.tw_404-1058-35967_php.json
# rule_33.json
# rule_329.json
# =========================

def school_rule_articles_to_documents(
    obj: Dict[str, Any],
    source_path: str | Path,
) -> List[Document]:
    """
    å°‡ã€Œè¦ç«  / è¾¦æ³• / çå­¸é‡‘å¯¦æ–½è¦é»ã€é€™é¡ JSON è½‰æˆå¤šç­† Documentã€‚

    çµæ§‹å‡è¨­ç‚ºï¼š
    {
      "url": "...",
      "title": "...",
      "articles": [
        { "article_no": "ç¬¬ä¸€æ¢", "text": "..." },
        { "article_no": "ç¬¬äºŒæ¢", "text": "..." },
        ...
      ]
    }

    è¼¸å‡ºï¼š
    1) ä¸€ç­†ã€Œè¦ç« ç¸½è¦½ã€ï¼šæ•´ä»½è¾¦æ³•åœ¨åšä»€éº¼ï¼Œå¤§è‡´æ¶µè“‹ä¸»è¦æ¢æ–‡æ–¹å‘ã€‚
    2) å¤šç­†ã€Œæ¢æ–‡æ‘˜è¦ã€ï¼šæ¯ä¸€æ¢å„ä¸€ç­†ï¼Œæ–¹ä¾¿ç²¾æº–æŸ¥è©¢ã€‚
    """
    docs: List[Document] = []

    source_path_str = str(source_path)
    url = str(obj.get("url") or "").strip()
    title = str(obj.get("title") or "").strip()
    articles = obj.get("articles") or []
    if not isinstance(articles, list):
        articles = []

    # ğŸ”¹ åˆ¤æ–·æ˜¯ã€Œçå­¸é‡‘é¡ã€é‚„æ˜¯ã€Œä¸€èˆ¬å­¸å‰‡/è¦ç« ã€
    # ç”¨æ¨™é¡Œç²—ç•¥åˆ¤æ–·å°±å¥½ï¼šæœ‰ã€Œçå­¸é‡‘ã€æˆ–ã€Œå‹µå­¸ã€å­—çœ¼å°±ç•¶æˆ scholarship
    if "çå­¸é‡‘" in title or "å‹µå­¸" in title:
        rule_kind = "scholarship_rule"
    else:
        rule_kind = "academic_rule"

    idx = 0

    # === (1) è¦ç« ç¸½è¦½ Doc ===
    #   - ç”¨å°‘é‡æ¢æ–‡æ‘˜è¦ï¼ˆå‰å¹¾æ¢ã€æ¯æ¢æˆªå€‹é ­ï¼‰ä¾†å¹« LLM æŒæ¡æ•´é«”å…§å®¹ã€‚
    summary_items: List[Dict[str, str]] = []
    for a in articles[:6]:  # æœ€å¤šæ‹¿å‰ 6 æ¢ä¾†ç•¶æ¦‚è¦ï¼ˆé˜²çˆ†å­—æ•¸ï¼‰
        if not isinstance(a, dict):
            continue
        ano = str(a.get("article_no") or "").strip()
        txt = str(a.get("text") or "").strip()
        if not txt:
            continue
        summary_items.append(
            {
                "æ¢è™Ÿ": ano,
                "æ¢æ–‡é–‹é ­": txt[:80],  # åªæˆªå‰é¢ä¸€å°æ®µè®“ rewriter æœ‰æ„Ÿè¦ºå°±å¥½
            }
        )

    overview_record: Dict[str, Any] = {
        "è¦ç« æ¨™é¡Œ": title,
        "è¦ç« ç¶²å€": url,
        "è¦ç« é¡å‹": rule_kind,  # ä¾‹å¦‚ scholarship_rule / academic_rule
        "æ¢æ–‡ç¸½æ•¸": len(articles),
        "æ¢æ–‡æ‘˜è¦åˆ—è¡¨": summary_items,
        "è³‡æ–™ä¾†æº": url,
    }

    try:
        overview_text = rewrite_json_record(
            record=overview_record,
            schema_hint="school_rule_overview",
            max_chars=900,  # ç¸½è¦½å¯ä»¥ç¨å¾®é•·ä¸€é»
        )
    except Exception as e:
        print(
            "[school_rule_articles_to_documents] "
            f"rewrite_json_record (overview) ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}"
        )
        sys.exit(1)

    overview_meta = {
        "source": source_path_str,
        "file_type": "json",
        "content_type": "school_rule_overview",

        "title": title,
        "url": url,
        "rule_kind": rule_kind,          # scholarship_rule or academic_rule
        "article_count": len(articles),

        "idx": idx,
        "needs_split": False,
    }
    docs.append(Document(page_content=overview_text.strip(), metadata=overview_meta))
    idx += 1

    # === (2) æ¯æ¢æ¢æ–‡å„ä¸€å€‹ Doc ===
    for art in articles:
        if not isinstance(art, dict):
            continue

        article_no = str(art.get("article_no") or "").strip()
        article_text = str(art.get("text") or "").strip()
        if not article_text:
            continue

        record_article: Dict[str, Any] = {
            "è¦ç« æ¨™é¡Œ": title,
            "è¦ç« ç¶²å€": url,
            "è¦ç« é¡å‹": rule_kind,
            "æ¢è™Ÿ": article_no,
            "æ¢æ–‡å…§å®¹": article_text,
            "è³‡æ–™ä¾†æº": url,
        }

        try:
            article_rewritten = rewrite_json_record(
                record=record_article,
                schema_hint="school_rule_article",
                max_chars=500,  # æ¯ä¸€æ¢ç›®æ¨™æ§åˆ¶åœ¨ 500 å­—ä»¥å…§
            )
        except Exception as e:
            print(
                "[school_rule_articles_to_documents] "
                f"rewrite_json_record (article {article_no}) ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}"
            )
            sys.exit(1)

        # æœ‰æ™‚æ¢æ–‡æœ¬èº«å°±å¾ˆçŸ­ï¼Œrewriter ä¹Ÿæœƒè¼¸å‡ºå¾ˆçŸ­ï¼ŒOKã€‚
        # é€™è£¡ä¸å†åš [:500] çš„ç¡¬åˆ‡ï¼Œé¿å…æŠŠå¥å­ / æ•¸å­—æˆªæ–·ã€‚

        article_meta = {
            "source": source_path_str,
            "file_type": "json",
            "content_type": "school_rule_article",

            "title": title,
            "url": url,
            "rule_kind": rule_kind,    # scholarship_rule / academic_rule
            "article_no": article_no,

            "idx": idx,
            "needs_split": False,
        }

        docs.append(
            Document(page_content=article_rewritten.strip(), metadata=article_meta)
        )
        idx += 1

    return docs


# =========================
# ttu_exchange_2026_spring.json adapter
# =========================

def exchange_program_call_to_documents(
    obj: Dict[str, Any],
    source_path: str | Path,
) -> List[Document]:
    """
    å°‡ 2025 æ˜¥å­£å§Šå¦¹æ ¡äº¤æ› / é›™è¯å­¸ä½å…¬å‘Š JSON
    è½‰æˆå¤šç­†å¯å…¥åº«çš„ Documentï¼š

    1) ä¸€ä»½ã€Œæ•´é«”å…¬å‘Šç¸½è¦½ã€ï¼š
       - åŒ…å«æ¨™é¡Œã€ç¶²å€ã€æ™‚é–“èªªæ˜ã€æ³¨æ„äº‹é …ã€æ‰¿è¾¦äºº

    2) å¤šç­†ã€Œç”³è«‹æ‰€éœ€è³‡æ–™é …ç›®ã€ï¼š
       - ä¾†è‡ª section2.rowsï¼Œæ¯ä¸€é … (1,2,â€¦,11,113) ä¸€ç­†

    3) å¤šç­†ã€Œå§Šå¦¹æ ¡è©³æƒ…ã€ï¼š
       - ä¾†è‡ª section3.waves[*].schoolsï¼Œæ¯æ‰€å­¸æ ¡ä¸€ç­†
       - åŒ…å«ï¼šæ³¢æ¬¡ã€æˆªæ­¢æ™‚é–“ã€å­¸æ ¡åç¨±ã€é–€æª»/èªè¨€è¦æ±‚ç­‰

    4) å¤šç­†ã€Œæ¯ä¸€æ³¢å§Šå¦¹æ ¡ç¸½è¦½ã€ï¼š
       - ä¸€æ³¢å¯èƒ½åˆ‡æˆå¤šå€‹ chunkï¼Œæ¯å€‹ chunk æ§åˆ¶åœ¨ ~500 å­—å…§
       - æ–¹ä¾¿å•ã€Œç¬¬ä¸€æ³¢æœ‰å“ªäº›å­¸æ ¡ï¼Ÿã€æ™‚ï¼Œä¸€æ¬¡åˆ—å‡ºå¤šé–“
    """
    docs: List[Document] = []

    source_path_str = str(source_path)
    title = str(obj.get("title") or "").strip()
    url = str(obj.get("url") or "").strip()

    section1 = obj.get("section1") or {}
    sec1_title = str(section1.get("title") or "").strip()
    sec1_content = str(section1.get("content") or "").strip()

    section2 = obj.get("section2") or {}
    sec2_title = str(section2.get("title") or "").strip()
    rows = section2.get("rows") or []

    # section3ï¼ˆæ–°æ ¼å¼ï¼‰/ section3_noteï¼ˆèˆŠæ ¼å¼ï¼‰éƒ½æ”¯æ´
    section3 = obj.get("section3") or obj.get("section3_note") or {}
    sec3_title = str(section3.get("title") or "").strip()
    sec3_content = str(section3.get("content") or "").strip()


    section5 = obj.get("section5") or {}
    sec5_title = str(section5.get("title") or "").strip()
    waves = section5.get("waves") or []

    section4 = obj.get("section4") or {}
    sec4_title = str(section4.get("title") or "").strip()
    sec4_content = str(section4.get("content") or "").strip()

    idx = 0

    # === (1) æ•´é«”å…¬å‘Šç¸½è¦½ ===
    overview_record: Dict[str, Any] = {
        "å…¬å‘Šæ¨™é¡Œ": title,
        "å…¬å‘Šç¶²å€": url,
        "æ™‚é–“æ¨™é¡Œ": sec1_title,
        "æ™‚é–“å…§å®¹": sec1_content,
        "ç”³è«‹æ­¥é©Ÿæ¨™é¡Œ": sec2_title,
        "ç”³è«‹æ³¨æ„äº‹é …æ¨™é¡Œ": sec3_title,
        "ç”³è«‹æ³¨æ„äº‹é …å…§å®¹": sec3_content,
        "æ‰¿è¾¦äººæ¨™é¡Œ": sec4_title,
        "æ‰¿è¾¦äººè³‡è¨Š": sec4_content,
        "å§Šå¦¹æ ¡åˆ—è¡¨æ¨™é¡Œ": sec5_title
    }

    try:
        overview_text = rewrite_json_record(
            record=overview_record,
            schema_hint="exchange_program_overview",
            max_chars=900,
        )
    except Exception as e:
        print(
            "[exchange_program_call_to_documents] "
            f"rewrite_json_record (overview) ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}"
        )
        sys.exit(1)

    overview_meta = {
        "source": source_path_str,
        "file_type": "json",
        "content_type": "exchange_program_overview",

        "title": title or sec1_title,
        "url": url,

        "idx": idx,
        "needs_split": False,
    }
    docs.append(Document(page_content=overview_text.strip(), metadata=overview_meta))
    idx += 1

        # === (1b) ç”³è«‹æ³¨æ„äº‹é …ï¼ˆsection3ï¼‰ç¨ç«‹å¯«å…¥ï¼šå…§å®¹å¾ˆé•·ï¼Œåˆ‡æˆå¤šæ®µé¿å…éºæ¼ ===
    # æª”æ¡ˆè£¡ç¢ºå¯¦æœ‰ section3.title/content :contentReference[oaicite:2]{index=2}
    if sec3_title or sec3_content:
        MAX_RAW_CHARS_PER_NOTICE_CHUNK = 350   # å…ˆæŠŠåŸæ–‡åˆ‡çŸ­ï¼Œé¿å…é‡å¯«å¾Œçˆ†å­—æ•¸
        notice_parts: List[str] = []

        # ä»¥æ›è¡Œåšç²—åˆ‡ï¼Œå†ç´¯ç©åˆ° 350 å·¦å³ä¸€æ®µ
        lines = [ln.strip() for ln in (sec3_content or "").splitlines() if ln.strip()]
        buf: List[str] = []
        cur = 0
        for ln in lines:
            if buf and cur + len(ln) + 1 > MAX_RAW_CHARS_PER_NOTICE_CHUNK:
                notice_parts.append("\n".join(buf))
                buf, cur = [], 0
            buf.append(ln)
            cur += len(ln) + 1
        if buf:
            notice_parts.append("\n".join(buf))

        # å¦‚æœåŸæ–‡å¾ˆçŸ­ï¼Œè‡³å°‘ä¹Ÿæ”¾ä¸€æ®µ
        if not notice_parts and sec3_content:
            notice_parts = [sec3_content.strip()]

        total_parts = len(notice_parts) if notice_parts else 1

        for part_idx, part in enumerate(notice_parts, start=1):
            record_notice: Dict[str, Any] = {
                "å…¬å‘Šæ¨™é¡Œ": title,
                "å…¬å‘Šç¶²å€": url,
                "æ®µè½æ¨™é¡Œ": sec3_title or "ç”³è«‹æ³¨æ„äº‹é …",
                "åˆ†æ®µè³‡è¨Š": {"ç¬¬å¹¾éƒ¨åˆ†": part_idx, "ç¸½éƒ¨åˆ†æ•¸": total_parts},
                "æ³¨æ„äº‹é …å…§å®¹": part,
                "è³‡æ–™ä¾†æº": url,
            }
            try:
                notice_text = rewrite_json_record(
                    record=record_notice,
                    schema_hint="exchange_notice",
                    max_chars=500,   # çµ¦ 500ï¼›å› ç‚º part å·²å…ˆåˆ‡åˆ° ~350
                )
            except Exception as e:
                print(
                    "[exchange_program_call_to_documents] "
                    f"rewrite_json_record (section3 notice) ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}"
                )
                sys.exit(1)

            notice_meta = {
                "source": source_path_str,
                "file_type": "json",
                "content_type": "exchange_notice",

                "title": f"{title}-ç”³è«‹æ³¨æ„äº‹é …",
                "url": url,
                "section": "section3",
                "chunk": part_idx - 1,
                "chunk_total": total_parts,

                "idx": idx,
                "needs_split": False,
            }
            docs.append(Document(page_content=notice_text.strip(), metadata=notice_meta))
            idx += 1

    # === (1c) æ‰¿è¾¦äººï¼ˆsection4ï¼‰ç¨ç«‹å¯«å…¥ï¼šçŸ­ï¼Œä¸ç”¨åˆ‡ ===
    # æª”æ¡ˆè£¡ç¢ºå¯¦æœ‰ section4.title/content :contentReference[oaicite:3]{index=3}
    if sec4_title or sec4_content:
        record_contact: Dict[str, Any] = {
            "å…¬å‘Šæ¨™é¡Œ": title,
            "å…¬å‘Šç¶²å€": url,
            "æ‰¿è¾¦äººæ¨™é¡Œ": sec4_title or "æ‰¿è¾¦äºº",
            "æ‰¿è¾¦äººè³‡è¨Š": sec4_content,
            "è³‡æ–™ä¾†æº": url,
        }
        try:
            contact_text = rewrite_json_record(
                record=record_contact,
                schema_hint="exchange_contact",
                max_chars=400,
            )
        except Exception as e:
            print(
                "[exchange_program_call_to_documents] "
                f"rewrite_json_record (section4 contact) ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}"
            )
            sys.exit(1)

        contact_meta = {
            "source": source_path_str,
            "file_type": "json",
            "content_type": "exchange_contact",

            "title": f"{title}-æ‰¿è¾¦äºº",
            "url": url,
            "section": "section4",

            "idx": idx,
            "needs_split": False,
        }
        docs.append(Document(page_content=contact_text.strip(), metadata=contact_meta))
        idx += 1

    # === (2) ç”³è«‹æ‰€éœ€è³‡æ–™é …ç›®ï¼ˆsection2.rowsï¼‰ ===
    if isinstance(rows, list):
        for row in rows:
            if not isinstance(row, dict):
                continue

            item_no = str(row.get("ç·¨è™Ÿ") or "").strip()
            item_name = str(row.get("é …ç›®") or "").strip()
            item_desc = str(row.get("èªªæ˜") or "").strip()

            if not (item_name or item_desc):
                continue

            record_item: Dict[str, Any] = {
                "å…¬å‘Šæ¨™é¡Œ": title,
                "å…¬å‘Šç¶²å€": url,
                "é …ç›®ç·¨è™Ÿ": item_no,
                "é …ç›®åç¨±": item_name,
                "é …ç›®èªªæ˜": item_desc,
                "è³‡æ–™ä¾†æº": url,
            }

            try:
                item_text = rewrite_json_record(
                    record=record_item,
                    schema_hint="exchange_required_item",
                    max_chars=500,
                )
            except Exception as e:
                print(
                    "[exchange_program_call_to_documents] "
                    f"rewrite_json_record (required_item) ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}"
                )
                sys.exit(1)

            item_meta = {
                "source": source_path_str,
                "file_type": "json",
                "content_type": "exchange_required_item",

                "title": f"{title}-ç”³è«‹è³‡æ–™é …ç›®{item_no}",
                "url": url,

                "item_no": item_no,
                "item_name": item_name,

                "idx": idx,
                "needs_split": False,
            }

            docs.append(Document(page_content=item_text.strip(), metadata=item_meta))
            idx += 1

    # === (3) å§Šå¦¹æ ¡ï¼šæ¯æ‰€å­¸æ ¡ä¸€ç­† + (4) æ¯ä¸€æ³¢ç¸½è¦½ ===
    MAX_RAW_CHARS_PER_CHUNK = 400   # ç²—ä¼°åŸå§‹è³‡æ–™é•·åº¦
    MAX_WAVE_OVERVIEW_CHARS = 500   # çµ¦é‡å¯«å™¨çš„å­—æ•¸ä¸Šé™

    if isinstance(waves, list):
        for wave_obj in waves:
            if not isinstance(wave_obj, dict):
                continue

            wave_name = str(wave_obj.get("wave") or "").strip()
            wave_deadline = str(wave_obj.get("deadline") or "").strip()
            schools = wave_obj.get("schools") or []
            if not isinstance(schools, list) or not schools:
                # æœ‰äº› wave å¯èƒ½æ²’æœ‰åˆ—å­¸æ ¡ï¼ˆä¾‹å¦‚åªæœ‰æˆªæ­¢èªªæ˜ï¼‰ï¼Œå°±å…ˆç•¥é
                continue

            # çµ¦ã€Œæ³¢æ¬¡ç¸½è¦½ã€ç”¨çš„æš«å­˜åˆ—è¡¨
            wave_school_summaries: List[Dict[str, str]] = []

            # --- 3.1 æ¯æ‰€å­¸æ ¡ä¸€å€‹ doc ---
            for school in schools:
                if not isinstance(school, dict):
                    continue

                no = str(school.get("ç·¨è™Ÿ") or "").strip()
                school_name = str(school.get("å­¸æ ¡åç¨±") or "").strip()
                requirement = str(school.get("å§Šå¦¹æ ¡è¦æ±‚æ¢ä»¶") or "").strip()

                if not (school_name or requirement):
                    continue

                record_school: Dict[str, Any] = {
                    "å…¬å‘Šæ¨™é¡Œ": title,
                    "å…¬å‘Šç¶²å€": url,
                    "ç”³è«‹æ¢¯æ¬¡": wave_name,
                    "æˆªæ­¢æ™‚é–“èªªæ˜": wave_deadline,
                    "å­¸æ ¡ç·¨è™Ÿ": no,
                    "å­¸æ ¡åç¨±": school_name,
                    "å§Šå¦¹æ ¡è¦æ±‚æ¢ä»¶": requirement,
                    "è³‡æ–™ä¾†æº": url,
                }

                try:
                    school_text = rewrite_json_record(
                        record=record_school,
                        schema_hint="exchange_partner_school",
                        max_chars=650,
                    )
                except Exception as e:
                    print(
                        "[exchange_program_call_to_documents] "
                        f"rewrite_json_record (school) ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}"
                    )
                    sys.exit(1)

                school_meta = {
                    "source": source_path_str,
                    "file_type": "json",
                    "content_type": "exchange_partner_school",

                    "title": f"{title}-{wave_name}-{school_name or no}",
                    "url": url,

                    "wave": wave_name,
                    "wave_deadline": wave_deadline,
                    "school_no": no,
                    "school_name": school_name,

                    "idx": idx,
                    "needs_split": False,
                }

                docs.append(
                    Document(page_content=school_text.strip(), metadata=school_meta)
                )
                idx += 1

                wave_school_summaries.append(
                    {
                        "å­¸æ ¡ç·¨è™Ÿ": no,
                        "å­¸æ ¡åç¨±": school_name,
                        "å§Šå¦¹æ ¡è¦æ±‚æ¢ä»¶": requirement,
                    }
                )

            if not wave_school_summaries:
                continue

            # --- 4. æ¯ä¸€æ³¢å§Šå¦¹æ ¡ç¸½è¦½ï¼šä¾åŸå§‹é•·åº¦åˆ‡å¡Š ---
            chunks: List[List[Dict[str, str]]] = []
            current_chunk: List[Dict[str, str]] = []
            current_len = 0

            for s in wave_school_summaries:
                name = s.get("å­¸æ ¡åç¨±") or ""
                req = s.get("å§Šå¦¹æ ¡è¦æ±‚æ¢ä»¶") or ""
                est = len(name) + len(req) + 10  # å¾ˆç²—çš„ä¼°ç®—

                if current_chunk and current_len + est > MAX_RAW_CHARS_PER_CHUNK:
                    chunks.append(current_chunk)
                    current_chunk = []
                    current_len = 0

                current_chunk.append(s)
                current_len += est

            if current_chunk:
                chunks.append(current_chunk)

            num_chunks = len(chunks)

            for chunk_idx, schools_chunk in enumerate(chunks):
                record_wave: Dict[str, Any] = {
                    "å…¬å‘Šæ¨™é¡Œ": title,
                    "å…¬å‘Šç¶²å€": url,
                    "ç”³è«‹æ¢¯æ¬¡": wave_name,
                    "æˆªæ­¢æ™‚é–“èªªæ˜": wave_deadline,
                    "å­¸æ ¡ç¸½æ•¸": len(wave_school_summaries),
                    "æœ¬æ®µå­¸æ ¡æ•¸": len(schools_chunk),
                    "åˆ†æ®µè³‡è¨Š": {
                        "ç¬¬å¹¾éƒ¨åˆ†": chunk_idx + 1,
                        "ç¸½éƒ¨åˆ†æ•¸": num_chunks,
                    },
                    "å­¸æ ¡åˆ—è¡¨": schools_chunk,
                    "è³‡æ–™ä¾†æº": url,
                }

                try:
                    wave_text = rewrite_json_record(
                        record=record_wave,
                        schema_hint="exchange_wave_overview",
                        max_chars=MAX_WAVE_OVERVIEW_CHARS,
                    )
                except Exception as e:
                    print(
                        "[exchange_program_call_to_documents] "
                        f"rewrite_json_record (wave_overview) ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}"
                    )
                    sys.exit(1)

                wave_meta = {
                    "source": source_path_str,
                    "file_type": "json",
                    "content_type": "exchange_wave_overview",

                    "title": f"{title}-{wave_name}å§Šå¦¹æ ¡ç¸½è¦½",
                    "url": url,

                    "wave": wave_name,
                    "wave_deadline": wave_deadline,
                    "wave_school_count": len(wave_school_summaries),
                    "wave_chunk": chunk_idx,
                    "wave_chunk_total": num_chunks,

                    "idx": idx,
                    "needs_split": False,
                }

                docs.append(
                    Document(page_content=wave_text.strip(), metadata=wave_meta)
                )
                idx += 1

    return docs

# =========================
# ttu_sisters.json adapter
# =========================

def sister_schools_to_documents(
    obj: Dict[str, Any],
    source_path: str | Path,
) -> List[Document]:
    """
    å°‡ ttu_sisters.json è½‰æˆå¤šç­†å§Šå¦¹æ ¡æ–‡ä»¶ï¼š
    1) ä¸€æ‰€å§Šå¦¹æ ¡ï¼ˆå«æ´²åˆ¥ + åœ‹å®¶/åœ°å€ + ç¶²å€ï¼‰ = ä¸€ä»½ Document
    2) æ¯å€‹åœ‹å®¶/åœ°å€çš„å§Šå¦¹æ ¡ç¸½è¦½ï¼ˆåˆ‡æˆå¤šå€‹ chunkï¼Œæ§åˆ¶åœ¨ç´„ 500 å­—å…§ï¼‰
    3) å…¨ä¸–ç•Œå§Šå¦¹æ ¡åˆ†å¸ƒç¸½è¦½ï¼ˆä»¥å„åœ‹å­¸æ ¡æ•¸çµ±è¨ˆï¼Œä¸é€ä¸€åˆ—æ ¡åï¼‰
    """
    docs: List[Document] = []

    # çµ±ä¸€è½‰æˆå­—ä¸²ï¼Œé¿å… PosixPath è·‘é€² metadata
    source_path_str = str(source_path)
    title = str(obj.get("title") or "å¤§åŒå¤§å­¸å§Šå¦¹æ ¡").strip()
    source_url = str(obj.get("source") or source_path_str).strip()

    continents = obj.get("continents") or {}
    if not isinstance(continents, dict):
        continents = {}

    idx = 0

    # ç”¨ä¾†ä¹‹å¾Œåšã€Œæ¯åœ‹ overviewã€å’Œã€Œå…¨çƒç¸½è¦½ã€çš„èšåˆï¼škey = (æ´²åˆ¥, åœ‹å®¶/åœ°å€)
    grouped_by_country: dict[tuple[str, str], list[dict[str, str]]] = defaultdict(list)

    # === (1) ä¸€æ‰€å§Šå¦¹æ ¡ä¸€å€‹ doc ===
    for continent_label, region_dict in continents.items():
        if not isinstance(region_dict, dict):
            continue

        for country_label, schools in region_dict.items():
            if not isinstance(schools, list):
                continue

            for school in schools:
                if not isinstance(school, dict):
                    continue

                name = str(school.get("name", "")).strip()
                website = str(school.get("website", "")).strip()

                # å¦‚æœ name / website éƒ½ç©ºï¼Œå°±ç•¥é
                if not name and not website:
                    continue

                # çµ¦ã€Œæ¯åœ‹ overview / å…¨çƒç¸½è¦½ã€ç”¨çš„èšåˆè³‡æ–™ï¼ˆåªç•™åç¨± + ç¶²å€ï¼‰
                grouped_by_country[(continent_label, country_label)].append(
                    {
                        "å­¸æ ¡åç¨±": name,
                        "å­¸æ ¡ç¶²å€": website,
                    }
                )

                record: Dict[str, Any] = {
                    "æ¨™é¡Œ": title,
                    "è¨ˆç•«é¡å‹": "å§Šå¦¹æ ¡/åœ‹éš›åˆä½œå­¸æ ¡",
                    "æ´²åˆ¥": continent_label,
                    "åœ‹å®¶æˆ–åœ°å€": country_label,
                    "å­¸æ ¡åç¨±": name,
                    "å­¸æ ¡ç¶²å€": website,
                    "è³‡æ–™ä¾†æº": source_url,
                }

                try:
                    text = rewrite_json_record(
                        record=record,
                        schema_hint="sister_school",
                        max_chars=220,
                    )
                except Exception as e:
                    print(
                        "[sister_schools_to_documents] "
                        f"rewrite_json_record ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}"
                    )
                    sys.exit(1)

                metadata = {
                    "source": source_path_str,
                    "file_type": "json",
                    "content_type": "sister_school",

                    "title": title,
                    "source_url": source_url,
                    "continent_label": continent_label,
                    "country_label": country_label,
                    "school_name": name,
                    "school_website": website,

                    "idx": idx,
                    "needs_split": False,
                }

                docs.append(Document(page_content=text, metadata=metadata))
                idx += 1

    # === (2) æ¯ä¸€å€‹ã€Œåœ‹å®¶/åœ°å€ã€ç”¢ç”Ÿå¤šå€‹ overview chunkï¼ˆä¾åŸå§‹å­—æ•¸åˆ‡å¡Šï¼‰ ===
    MAX_RAW_CHARS_PER_CHUNK = 400   # äº‹å‰ä¼°ç®—ï¼šåŸå§‹è³‡æ–™ç›®æ¨™ <= 400 å­—
    MAX_OVERVIEW_CHARS = 500        # çµ¦é‡å¯«å™¨çš„å­—æ•¸ä¸Šé™

    for (continent_label, country_label), schools in sorted(
        grouped_by_country.items(),
        key=lambda kv: (kv[0][0], kv[0][1]),
    ):
        if not schools:
            continue

        total_schools = len(schools)

        # ä¾ã€Œä¼°ç®—å­—æ•¸ã€åˆ‡æˆå¤šå€‹ chunk
        chunks: List[List[Dict[str, str]]] = []
        current_chunk: List[Dict[str, str]] = []
        current_len = 0

        for s in schools:
            name = s.get("å­¸æ ¡åç¨±") or ""
            url = s.get("å­¸æ ¡ç¶²å€") or ""
            # ç²—ä¼°ï¼šåç¨±é•·åº¦ + ç¶²å€é•·åº¦ + ä¸€äº›æ¨™é»/é€£æ¥è©
            est = len(name) + len(url) + 10

            # è‹¥åŠ ä¸Šé€™ä¸€ç­†æœƒè¶…éä¸Šé™ï¼Œå°±å…ˆæ”¶æˆä¸€å€‹ chunk
            if current_chunk and current_len + est > MAX_RAW_CHARS_PER_CHUNK:
                chunks.append(current_chunk)
                current_chunk = []
                current_len = 0

            current_chunk.append(s)
            current_len += est

        if current_chunk:
            chunks.append(current_chunk)

        num_chunks = len(chunks)

        for chunk_idx, schools_chunk in enumerate(chunks):
            if not schools_chunk:
                continue

            record_country_overview: Dict[str, Any] = {
                "æ¨™é¡Œ": f"å¤§åŒå¤§å­¸{country_label}å§Šå¦¹æ ¡ç¸½è¦½",
                "æ´²åˆ¥": continent_label,
                "åœ‹å®¶æˆ–åœ°å€": country_label,
                "å­¸æ ¡ç¸½æ•¸": total_schools,
                "æœ¬æ®µå­¸æ ¡æ•¸": len(schools_chunk),
                "åˆ†æ®µè³‡è¨Š": {
                    "ç¬¬å¹¾éƒ¨åˆ†": chunk_idx + 1,
                    "ç¸½éƒ¨åˆ†æ•¸": num_chunks,
                },
                # é€™è£¡æ¯ç­†éƒ½æœ‰ã€Œå­¸æ ¡åç¨± / å­¸æ ¡ç¶²å€ã€ï¼ŒLLM æœƒä¾é€™äº›ä¾†å¯«å¥å­
                "å­¸æ ¡åˆ—è¡¨": schools_chunk,
                "è³‡æ–™ä¾†æº": source_url,
            }

            try:
                overview_text = rewrite_json_record(
                    record=record_country_overview,
                    schema_hint="sister_school_country_overview",
                    max_chars=MAX_OVERVIEW_CHARS,
                )
            except Exception as e:
                print(
                    "[sister_schools_to_documents] "
                    f"rewrite_json_record (country_overview) ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}"
                )
                sys.exit(1)

            overview_meta = {
                "source": source_path_str,
                "file_type": "json",
                "content_type": "sister_school_overview",

                "title": f"{country_label}å§Šå¦¹æ ¡ç¸½è¦½",
                "source_url": source_url,
                "continent_label": continent_label,
                "country_label": country_label,
                "school_count": total_schools,

                "chunk": chunk_idx,
                "chunk_total": num_chunks,
                "chunk_school_count": len(schools_chunk),
                "overview_scope": "country",

                "idx": idx,
                "needs_split": False,
            }

            docs.append(
                Document(page_content=overview_text.strip(), metadata=overview_meta)
            )
            idx += 1

    # === (3) åŠ å›ã€Œå…¨çƒå§Šå¦¹æ ¡åˆ†å¸ƒç¸½è¦½ã€ä¸€å€‹å¤§ chunk ===
    if grouped_by_country:
        total_schools_global = sum(len(v) for v in grouped_by_country.values())

        # ä»¥ã€Œæ´²åˆ¥ + åœ‹å®¶ã€æ•´ç†æ¯åœ‹çš„å­¸æ ¡æ•¸
        country_items: List[Dict[str, Any]] = []
        for (continent_label, country_label), schools in sorted(
            grouped_by_country.items(),
            key=lambda kv: (kv[0][0], kv[0][1]),
        ):
            country_items.append(
                {
                    "æ´²åˆ¥": continent_label,
                    "åœ‹å®¶æˆ–åœ°å€": country_label,
                    "å­¸æ ¡æ•¸": len(schools),
                }
            )

        overview_record_global: Dict[str, Any] = {
            "æ¨™é¡Œ": f"{title}å…¨çƒç¸½è¦½",
            "èªªæ˜": "å¤§åŒå¤§å­¸æ‰€æœ‰å§Šå¦¹æ ¡èˆ‡åœ‹éš›åˆä½œå­¸æ ¡çš„å…¨çƒåˆ†å¸ƒç¸½è¦½ï¼Œ"
                    "ä¾æ´²åˆ¥èˆ‡åœ‹å®¶/åœ°å€åˆ—å‡ºå„åœ‹å§Šå¦¹æ ¡æ•¸é‡ã€‚",
            "ç¸½å­¸æ ¡æ•¸": total_schools_global,
            "åœ‹å®¶åˆ†å¸ƒåˆ—è¡¨": country_items,
            "è³‡æ–™ä¾†æº": source_url,
        }

        try:
            overview_text_global = rewrite_json_record(
                record=overview_record_global,
                schema_hint="sister_school_global_overview",
                max_chars=1500,  # é€™é¡†å…è¨±é•·ä¸€é»ï¼Œè®“çµ±è¨ˆæ•˜è¿°å®Œæ•´
            )
        except Exception as e:
            print(
                "[sister_schools_to_documents] "
                f"rewrite_json_record (global_overview) ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}"
            )
            sys.exit(1)

        overview_meta_global = {
            "source": source_path_str,
            "file_type": "json",
            "content_type": "sister_school_global_overview",

            "title": f"{title}å…¨çƒç¸½è¦½",
            "source_url": source_url,
            "overview_scope": "global",
            "total_school_count": total_schools_global,

            "idx": idx,
            "needs_split": False,
        }

        docs.append(
            Document(page_content=overview_text_global.strip(), metadata=overview_meta_global)
        )

    return docs

# =========================
# ttu_flexible_week.json adapterï¼ˆå‡ç´šç‰ˆï¼šç¸½è¦½ + é€æ¢ï¼‰
# =========================

def flexible_week_rules_to_documents(
    obj: Dict[str, Any], source_path: str
) -> List[Document]:
    """
    å°‡ ttu_flexible_week.json è½‰æˆå¤šç­† academic_rule é¡å‹çš„ Documentï¼š

    ä¸€ã€æ•¸ä½æ•™å­¸èª²ç¨‹å¯¦æ–½è¦é»
        1) ä¸€ä»½ç¸½è¦½ï¼ˆæ¶µè“‹ 1~9 é»çš„å¤§æ„ï¼‰
        2) æ¯ä¸€æ¢å¯¦æ–½è¦é»å„ä¸€ä»½ï¼ˆç¬¬ 1~9 é»ï¼Œé€æ¢åˆ‡å¡Šï¼‰

    äºŒã€å½ˆæ€§æ•™å­¸é€±æ´»å‹•è¦åŠƒ
        3) ä¸€ä»½ç¸½è¦½
        4) æ¯ä¸€æ¢æ´»å‹•è¦å®šå„ä¸€ä»½ï¼ˆå…± 3 æ¢ï¼‰

    å¦‚æ­¤ä¸€ä¾†ï¼Œæ—¢æœ‰ã€Œå¤§ç¶±ã€ä¹Ÿæœ‰ã€Œå®Œæ•´å…§å®¹ã€ã€‚
    """
    docs: List[Document] = []

    title = str(obj.get("title", "")).strip()
    source_url = str(obj.get("source_url", "")).strip()
    pdf_url = str(obj.get("pdf_url", "")).strip()

    guidelines_raw = obj.get("å¯¦æ–½è¦é»", []) or []
    if not isinstance(guidelines_raw, list):
        guidelines_raw = [guidelines_raw]

    flex_raw = obj.get("å½ˆæ€§æ•™å­¸é€±æ´»å‹•è¦åŠƒ", []) or []
    if not isinstance(flex_raw, list):
        flex_raw = [flex_raw]

    def parse_numbered(items: List[Any]) -> List[Dict[str, Any]]:
        """
        æŠŠã€Œ1. xxxã€ã€Œ2. yyyyã€é€™ç¨®æ¢æ–‡ï¼Œæ‹†æˆæœ‰ æ¢æ¬¡ / å…§å®¹ / åŸå§‹æ–‡å­— çš„åˆ—è¡¨ï¼Œ
        è®“ LLM å¯ä»¥æ¸…æ¥šçŸ¥é“æ˜¯ç¬¬å¹¾æ¢ã€‚
        """
        parsed: List[Dict[str, Any]] = []
        for line in items:
            s = str(line).strip()
            if not s:
                continue
            m = re.match(r"(\d+)\.\s*(.*)", s)
            if m:
                try:
                    num = int(m.group(1))
                except Exception:
                    num = None
                content = m.group(2).strip() or s
            else:
                num = None
                content = s
            parsed.append(
                {
                    "æ¢æ¬¡": num,
                    "å…§å®¹": content,
                    "åŸå§‹æ–‡å­—": s,
                }
            )
        return parsed

    guidelines_entries = parse_numbered(guidelines_raw)
    flex_entries = [
        {"èªªæ˜": str(line).strip()}
        for line in flex_raw
        if str(line).strip()
    ]

    idx = 0

    # === (1) æ•¸ä½æ•™å­¸èª²ç¨‹å¯¦æ–½è¦é» - ç¸½è¦½ ===
    if guidelines_entries:
        idx += 1
        record_guidelines_overview: Dict[str, Any] = {
            "æ¨™é¡Œ": title or "å¤§åŒå¤§å­¸æ•¸ä½æ•™å­¸èª²ç¨‹å¯¦æ–½è¦é»åŠå½ˆæ€§æ•™å­¸é€±ç›¸é—œè¦å®š",
            "è¦å®šä¸»é¡Œ": "æ•¸ä½æ•™å­¸èª²ç¨‹å¯¦æ–½è¦é»",
            "æ¢æ–‡æ•¸é‡": len(guidelines_entries),
            "æ¢æ–‡åˆ—è¡¨": guidelines_entries,
            "ä¾†æºç¶²å€": source_url,
            "PDFä¸‹è¼‰": pdf_url,
            "ä¾†æºæª”æ¡ˆ": source_path,
        }
        try:
            # ç¸½è¦½ä»ç„¶æ˜¯ã€Œæ¿ƒç¸®ç‰ˆã€
            text_guidelines_overview = rewrite_json_record(
                record=record_guidelines_overview,
                schema_hint="academic_rules_digital_teaching_overview",
                max_chars=700,  # å¯ä»¥ç¨å¾®é•·ä¸€é»
            )
        except Exception as e:
            print(
                "[flexible_week_rules_to_documents] "
                f"rewrite_json_record (å¯¦æ–½è¦é»ç¸½è¦½) ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}"
            )
            sys.exit(1)

        meta_guidelines_overview = {
            "source": source_path,
            "file_type": "json",
            "type": "academic_rules_digital_teaching_overview",
            "content_type": "academic_rule",

            "title": title,
            "category": "æ•¸ä½æ•™å­¸èª²ç¨‹å¯¦æ–½è¦é»",
            "topic": "digital_teaching",
            "section": "digital_teaching_overview",
            "source_url": source_url,
            "pdf_url": pdf_url,
            "rule_count": len(guidelines_entries),

            "idx": idx,
            "needs_split": False,
        }

        docs.append(
            Document(
                page_content=text_guidelines_overview.strip(),
                metadata=meta_guidelines_overview,
            )
        )

        # === (1b) æ•¸ä½æ•™å­¸èª²ç¨‹å¯¦æ–½è¦é» - é€æ¢ï¼ˆå®Œæ•´å…§å®¹ï¼‰ ===
        for entry in guidelines_entries:
            idx += 1
            article_no = entry.get("æ¢æ¬¡")
            raw_text = entry.get("åŸå§‹æ–‡å­—") or entry.get("å…§å®¹") or ""

            # ä¸€æ¢ä¸€æ¢ä¸Ÿçµ¦é‡å¯«å™¨ï¼Œé•·åº¦ä¸Šé™ > åŸæ–‡ï¼ˆæœ€å¤§é‚£æ¢ç´„ 200 å¤šå­—ï¼‰
            record_guideline_article: Dict[str, Any] = {
                "æ¨™é¡Œ": title or "å¤§åŒå¤§å­¸æ•¸ä½æ•™å­¸èª²ç¨‹å¯¦æ–½è¦é»åŠå½ˆæ€§æ•™å­¸é€±ç›¸é—œè¦å®š",
                "è¦å®šä¸»é¡Œ": "æ•¸ä½æ•™å­¸èª²ç¨‹å¯¦æ–½è¦é»",
                "æ¢æ¬¡": article_no,
                "æ¢æ–‡å…§å®¹": raw_text,
                "ä¾†æºç¶²å€": source_url,
                "PDFä¸‹è¼‰": pdf_url,
                "ä¾†æºæª”æ¡ˆ": source_path,
            }
            try:
                text_guideline_article = rewrite_json_record(
                    record=record_guideline_article,
                    schema_hint="academic_rules_digital_teaching_article",
                    max_chars=400,  # > åŸæ–‡é•·åº¦ï¼Œä¸éœ€è¦å£“ç¸®æˆå¤§ç¶±
                )
            except Exception as e:
                print(
                    "[flexible_week_rules_to_documents] "
                    f"rewrite_json_record (å¯¦æ–½è¦é» ç¬¬ {article_no} æ¢) ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}"
                )
                sys.exit(1)

            meta_guideline_article = {
                "source": source_path,
                "file_type": "json",
                "type": "academic_rules_digital_teaching_article",
                "content_type": "academic_rule",

                "title": title,
                "category": "æ•¸ä½æ•™å­¸èª²ç¨‹å¯¦æ–½è¦é»",
                "topic": "digital_teaching",
                "section": "digital_teaching_article",
                "article_no": article_no,
                "source_url": source_url,
                "pdf_url": pdf_url,

                "idx": idx,
                "needs_split": False,
            }

            docs.append(
                Document(
                    page_content=text_guideline_article.strip(),
                    metadata=meta_guideline_article,
                )
            )

    # === (2) å½ˆæ€§æ•™å­¸é€±æ´»å‹•è¦åŠƒ - ç¸½è¦½ ===
    if flex_entries:
        idx += 1
        record_flex_overview: Dict[str, Any] = {
            "æ¨™é¡Œ": title or "å¤§åŒå¤§å­¸æ•¸ä½æ•™å­¸èª²ç¨‹å¯¦æ–½è¦é»åŠå½ˆæ€§æ•™å­¸é€±ç›¸é—œè¦å®š",
            "è¦å®šä¸»é¡Œ": "å½ˆæ€§æ•™å­¸é€±æ´»å‹•è¦åŠƒ",
            "æ´»å‹•èˆ‡è¦å®šåˆ—è¡¨": flex_entries,
            "ä¾†æºç¶²å€": source_url,
            "PDFä¸‹è¼‰": pdf_url,
            "ä¾†æºæª”æ¡ˆ": source_path,
        }
        try:
            text_flex_overview = rewrite_json_record(
                record=record_flex_overview,
                schema_hint="academic_rules_flexible_week_overview",
                max_chars=400,
            )
        except Exception as e:
            print(
                "[flexible_week_rules_to_documents] "
                f"rewrite_json_record (å½ˆæ€§æ•™å­¸é€±ç¸½è¦½) ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}"
            )
            sys.exit(1)

        meta_flex_overview = {
            "source": source_path,
            "file_type": "json",
            "type": "academic_rules_flexible_week_overview",
            "content_type": "academic_rule",

            "title": title,
            "category": "å½ˆæ€§æ•™å­¸é€±æ´»å‹•è¦åŠƒ",
            "topic": "flexible_week",
            "section": "flexible_week_overview",
            "source_url": source_url,
            "pdf_url": pdf_url,
            "activity_count": len(flex_entries),

            "idx": idx,
            "needs_split": False,
        }

        docs.append(
            Document(
                page_content=text_flex_overview.strip(),
                metadata=meta_flex_overview,
            )
        )

        # === (2b) å½ˆæ€§æ•™å­¸é€±æ´»å‹•è¦åŠƒ - é€æ¢ ===
        for i, entry in enumerate(flex_entries, start=1):
            idx += 1
            desc = entry.get("èªªæ˜") or ""

            record_flex_item: Dict[str, Any] = {
                "æ¨™é¡Œ": title or "å¤§åŒå¤§å­¸æ•¸ä½æ•™å­¸èª²ç¨‹å¯¦æ–½è¦é»åŠå½ˆæ€§æ•™å­¸é€±ç›¸é—œè¦å®š",
                "è¦å®šä¸»é¡Œ": "å½ˆæ€§æ•™å­¸é€±æ´»å‹•è¦åŠƒ",
                "é …ç›®åºè™Ÿ": i,
                "æ¢æ–‡å…§å®¹": desc,
                "ä¾†æºç¶²å€": source_url,
                "PDFä¸‹è¼‰": pdf_url,
                "ä¾†æºæª”æ¡ˆ": source_path,
            }
            try:
                text_flex_item = rewrite_json_record(
                    record=record_flex_item,
                    schema_hint="academic_rules_flexible_week_item",
                    max_chars=300,
                )
            except Exception as e:
                print(
                    "[flexible_week_rules_to_documents] "
                    f"rewrite_json_record (å½ˆæ€§æ•™å­¸é€± ç¬¬ {i} é …) ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}"
                )
                sys.exit(1)

            meta_flex_item = {
                "source": source_path,
                "file_type": "json",
                "type": "academic_rules_flexible_week_item",
                "content_type": "academic_rule",

                "title": title,
                "category": "å½ˆæ€§æ•™å­¸é€±æ´»å‹•è¦åŠƒ",
                "topic": "flexible_week",
                "section": "flexible_week_item",
                "item_no": i,
                "source_url": source_url,
                "pdf_url": pdf_url,

                "idx": idx,
                "needs_split": False,
            }

            docs.append(
                Document(
                    page_content=text_flex_item.strip(),
                    metadata=meta_flex_item,
                )
            )

    return docs

# =========================
# cse_required_by_semester.json adapter
# =========================

import sys
from typing import Any, Dict, List
from langchain_core.documents import Document
from json_rewriter import rewrite_json_record


def _parse_semester_label(label: str) -> Dict[str, Any]:
    """æŠŠã€Œä¸€ä¸Š / ä¸€ä¸‹ / äºŒä¸Š / ...ã€æ‹†æˆå¹´ç´š / å­¸æœŸç­‰æ¬„ä½ã€‚"""
    grade_map = {"ä¸€": 1, "äºŒ": 2, "ä¸‰": 3, "å››": 4}
    grade_name_map = {
        1: "ä¸€å¹´ç´š",
        2: "äºŒå¹´ç´š",
        3: "ä¸‰å¹´ç´š",
        4: "å››å¹´ç´š",
    }
    term_name_map = {"ä¸Š": "ä¸Šå­¸æœŸ", "ä¸‹": "ä¸‹å­¸æœŸ"}

    grade = None
    grade_name = None
    term = None
    term_name = None

    if isinstance(label, str) and len(label) >= 2:
        g = label[0]
        t = label[1]
        grade = grade_map.get(g)
        grade_name = grade_name_map.get(grade)
        term = t
        term_name = term_name_map.get(t)

    return {
        "grade": grade,
        "grade_name": grade_name,
        "term": term,           # "ä¸Š" / "ä¸‹"
        "term_name": term_name, # "ä¸Šå­¸æœŸ" / "ä¸‹å­¸æœŸ"
    }


def required_by_semester_to_documents(obj: Dict[str, Any], source_path: str) -> List[Document]:
    """
    å°‡ã€å¤§åŒå¤§å­¸è³‡è¨Šå·¥ç¨‹å­¸ç³»å¤§å­¸éƒ¨å¿…ä¿®ç§‘ç›®(æª¢æ ¸)è¡¨ã€è½‰æˆ RAG æ–‡ä»¶ã€‚
    - æ¯å­¸æœŸä¸€å€‹ overview doc
    - é¡å¤–ä¸€å€‹ã€Œå‚™è¨» / å…ˆä¿®æ¢ä»¶ã€doc
    """
    docs: List[Document] = []

    title = obj.get("title") or "å¤§åŒå¤§å­¸è³‡è¨Šå·¥ç¨‹å­¸ç³»å¤§å­¸éƒ¨å¿…ä¿®ç§‘ç›®(æª¢æ ¸)è¡¨"
    source_pdf = obj.get("source_pdf")
    semesters = obj.get("semesters") or {}
    notes_text = obj.get("å‚™è¨»")

    # å…ˆä¾å­¸æœŸåç¨±æ’åºï¼Œé¿å…æ¯æ¬¡ ingest é †åºé£„ç§»
    semester_items = sorted(semesters.items(), key=lambda kv: kv[0])

    for idx, (sem_label, course_list) in enumerate(semester_items):
        course_list = course_list or []
        parsed = _parse_semester_label(sem_label)

        # å¾ç¬¬ä¸€ç­†èª²æ‹¿å…±åŒå¿…ä¿®/å°ˆæ¥­å¿…ä¿®å°è¨ˆï¼ˆJSON æ¯ç­†éƒ½ä¸€æ¨£ï¼‰
        common_total = None
        major_total = None
        if course_list:
            first = course_list[0]
            common_total = first.get("å…±åŒå¿…ä¿®å°è¨ˆ")
            major_total = first.get("å°ˆæ¥­å¿…ä¿®å°è¨ˆ")

        # ç°¡åŒ–èª²ç¨‹åˆ—è¡¨çµ¦ rewriter ç”¨
        simple_courses = []
        for c in course_list:
            simple_courses.append({
                "èª²ç¨‹åç¨±": c.get("raw"),
                "é¡åˆ¥": c.get("é¡åˆ¥"),
                "å­¸åˆ†": c.get("å­¸åˆ†"),
            })

        record: Dict[str, Any] = {
            "ç³»æ‰€": "è³‡è¨Šå·¥ç¨‹å­¸ç³»",
            "å­¸åˆ¶": "å¤§å­¸éƒ¨",
            "æ¨™é¡Œ": title,
            "å­¸æœŸä»£ç¢¼": sem_label,  # ä¾‹å¦‚ã€Œä¸€ä¸Šã€ã€Œä¸€ä¸‹ã€
            "å¹´ç´š": parsed.get("grade_name"),
            "å­¸æœŸåˆ¥": parsed.get("term_name"),
            "èª²ç¨‹æ•¸": len(simple_courses),
            "å…±åŒå¿…ä¿®ç¸½å­¸åˆ†": common_total,
            "å°ˆæ¥­å¿…ä¿®ç¸½å­¸åˆ†": major_total,
            "èª²ç¨‹åˆ—è¡¨": simple_courses,
            "è³‡æ–™ä¾†æº": source_pdf or source_path,
        }

        # âœ… å¥—ç”¨ä½ åŸæœ¬çš„ try/except å¯«æ³•
        try:
            overview_text = rewrite_json_record(
                record=record,
                schema_hint="required_courses_by_semester",
                max_chars=500,
            )
        except Exception as e:
            print(f"[required_by_semester_to_documents] rewrite_json_record ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}")
            sys.exit(1)

        metadata = {
            "source": source_path,
            "file_type": "json",
            "content_type": "required_courses_by_semester",
            "title": title,
            "source_pdf": source_pdf,
            "semester_label": sem_label,
            "grade": parsed.get("grade"),
            "grade_name": parsed.get("grade_name"),
            "term": parsed.get("term"),
            "term_name": parsed.get("term_name"),
            "course_count": len(simple_courses),
            "required_common_credits": common_total,
            "required_major_credits": major_total,
            "idx": idx,
            "needs_split": False,  # å·²æ˜¯çŸ­ overviewï¼Œä¸éœ€å†åˆ‡ chunk
        }

        docs.append(Document(page_content=overview_text, metadata=metadata))

    # å†åšä¸€å€‹ã€Œå‚™è¨» / å…ˆä¿®æ¢ä»¶ã€ç¨ç«‹æ–‡ä»¶
    if isinstance(notes_text, str) and notes_text.strip():
        note_record: Dict[str, Any] = {
            "ç³»æ‰€": "è³‡è¨Šå·¥ç¨‹å­¸ç³»",
            "å­¸åˆ¶": "å¤§å­¸éƒ¨",
            "æ¨™é¡Œ": title,
            "èªªæ˜": "å¿…ä¿®ç§‘ç›®ç›¸é—œå‚™è¨»èˆ‡ä¿®èª²é †åºèªªæ˜",
            "å‚™è¨»": notes_text,
            "è³‡æ–™ä¾†æº": source_pdf or source_path,
        }

        # âœ… å‚™è¨»é€™é‚Šä¹Ÿä¸€æ¨£ç”¨ try/except
        try:
            note_text = rewrite_json_record(
                record=note_record,
                schema_hint="required_courses_note",
                max_chars=400,
            )
        except Exception as e:
            print(f"[required_by_semester_to_documents] å‚™è¨» rewrite_json_record ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}")
            sys.exit(1)

        note_meta = {
            "source": source_path,
            "file_type": "json",
            "content_type": "required_courses_note",
            "title": title,
            "source_pdf": source_pdf,
            "note_type": "prerequisite_rules",
            "idx": len(docs),  # æ¥åœ¨å¾Œé¢
            "needs_split": False,
        }

        docs.append(Document(page_content=note_text, metadata=note_meta))

    return docs

# =========================
# æ–°æ ¼å¼ course_historyï¼ˆå·¢ç‹€ï¼šå­¸æœŸâ†’å¹´ç´šâ†’èª²ç¨‹åˆ—è¡¨ï¼‰ adapter
#  - overview çš„ idx ä½¿ç”¨å…¨åŸŸéå¢ intï¼ˆé¿å… stable_id é‡è¤‡ï¼‰
#  - overview å…§å®¹å«ã€Œæ•™å¸« / é¸åˆ¥ / å­¸åˆ†ã€
#  - overview ä¾ token â‰¤ 500 å‹•æ…‹åˆ†æ‰¹ï¼Œä¸”ä¸æ‹†å–®ä¸€èª²ç¨‹æ¢ç›®
#  - æ¯å€‹ overview chunk æœ€å¾Œé™„è³‡æ–™ä¾†æº URL
#  - term/grade å…ˆæ’åºï¼Œç¢ºä¿ idx ç©©å®š
# =========================

def course_history_nested_to_documents(
    obj: Dict[str, Any], source_path: str
) -> List[Document]:
    docs: List[Document] = []

    overview_global_idx = 0   # overview å…¨åŸŸ idxï¼ˆint, æª”å…§å”¯ä¸€ï¼‰
    global_course_idx = 0     # æ¯é–€èª²å…¨åŸŸ idxï¼ˆç›®å‰æœªç”¨ï¼Œå…ˆä¿ç•™ä»¥å¾Œå¯èƒ½æœƒç”¨ï¼‰

    def as_int(x, default=None):
        try:
            s = str(x).strip()
            if not s:
                return default
            return int(s)
        except Exception:
            return default

    def parse_year_term(s: str) -> tuple[int | None, str]:
        """æŠŠ '113ä¸Š' / '113ä¸‹' æ‹†æˆ (113, 'ä¸Š'/'ä¸‹')"""
        s = (s or "").strip()
        if not s:
            return None, ""
        for i, ch in enumerate(s):
            if not ch.isdigit():
                year = as_int(s[:i], None)
                term = s[i:]
                return year, term
        return as_int(s, None), ""

    def term_sort_key(t: str):
        """å­¸æœŸæ’åºï¼šå¹´å°â†’å¤§ï¼›åŒå¹´ ä¸Šâ†’ä¸‹"""
        year, term = parse_year_term(t)
        term_order = 0 if term == "ä¸Š" else 1 if term == "ä¸‹" else 9
        return (year or 0, term_order, t)

    GRADE_ORDER = {
        "ä¸€å¹´ç´š": 1, "äºŒå¹´ç´š": 2, "ä¸‰å¹´ç´š": 3, "å››å¹´ç´š": 4, "ç ”ç©¶æ‰€": 10,
    }

    def grade_sort_key(g: str):
        return (GRADE_ORDER.get(g, 99), g)

    def parse_credits(x: Any) -> float | None:
        try:
            s = str(x).strip()
            if not s or s.lower() == "nan":
                return None
            return float(s)
        except Exception:
            return None

    # ===== token è¨ˆæ•¸èˆ‡ä¸æ‹†èª²ç¨‹çš„ batching =====
    def count_chars(text: str) -> int:
        # ç›´æ¥ç”¨ Python å­—ä¸²é•·åº¦ï¼ˆä»¥ Unicode å­—å…ƒè¨ˆï¼‰
        return len(text or "")

    def batch_courses_by_chars(
        header_lines: List[str],
        course_entries: List[Dict[str, Any]],
        tail_lines: List[str],
        max_chars: int = 500,
    ) -> List[List[Dict[str, Any]]]:
        """
        course_entries æ¯ä¸€ç­†æ˜¯ä¸€é–€èª²ï¼ˆä¸å¯æ‹†ï¼‰ã€‚
        ä¾å­—å…ƒæ•¸ä¸Šé™åˆ†æ‰¹ï¼ˆå« header+tailï¼‰ï¼Œå›å‚³ã€Œèª²ç¨‹ entry çš„åˆ—è¡¨åˆ—è¡¨ã€ï¼Œ
        æ¯ä¸€å€‹ batch ä¹‹å¾Œæœƒä¸Ÿçµ¦é‡å¯«å™¨ã€‚
        """
        batches: List[List[Dict[str, Any]]] = []

        fixed_text = "\n".join(header_lines + tail_lines)
        fixed_chars = count_chars(fixed_text)

        # header + tail å·²ç¶“è¶…éä¸Šé™ï¼Œå°±å…¨éƒ¨å¡åŒä¸€æ‰¹ï¼ˆäº¤çµ¦ LLM è‡ªå·±æ§åˆ¶å­—æ•¸ï¼‰
        if fixed_chars >= max_chars:
            batches.append(course_entries)
            return batches

        cur_chars = fixed_chars
        current_batch: List[Dict[str, Any]] = []

        for entry in course_entries:
            line = f"- {entry.get('æ¦‚è¦', '')}"
            line_chars = count_chars(line)

            if current_batch and (cur_chars + line_chars) > max_chars:
                batches.append(current_batch)
                current_batch = []
                cur_chars = fixed_chars

            # å–®ä¸€èª²ç¨‹è‡ªå·±å°±è¶…é max_charsï¼šä»è¦æ”¾ï¼ˆä¸æ‹†èª²ï¼‰
            current_batch.append(entry)
            cur_chars += line_chars

        if current_batch:
            batches.append(current_batch)

        return batches

    # ===== term å…ˆæ’åºï¼ˆ113ä¸Š â†’ 113ä¸‹ï¼‰=====
    for year_term in sorted(obj.keys(), key=term_sort_key):
        grades_block = obj.get(year_term)
        if not isinstance(grades_block, dict):
            continue

        year, term = parse_year_term(str(year_term))

        # ===== grade å…ˆæ’åºï¼ˆä¸€ â†’ äºŒ â†’ ä¸‰ â†’ å››ï¼‰=====
        for grade_name in sorted(grades_block.keys(), key=grade_sort_key):
            grade_data = grades_block.get(grade_name)
            if not isinstance(grade_data, dict):
                continue

            course_list = grade_data.get("èª²ç¨‹åˆ—è¡¨", []) or []
            if not isinstance(course_list, list):
                course_list = [course_list]

            course_count = grade_data.get("èª²ç¨‹æ•¸")
            try:
                course_count = int(course_count)
            except Exception:
                course_count = len(course_list)

            # ========= é å…ˆæ•´ç† overview çš„èª²ç¨‹ entryï¼ˆä¸å¯æ‹†åŸå­ï¼‰ =========
            course_entries: List[Dict[str, Any]] = []
            data_sources_all: List[str] = []

            for c in course_list:
                if not isinstance(c, dict):
                    continue

                name     = str(c.get("èª²ç¨‹åç¨±", "")).strip()
                code     = str(c.get("èª²è™Ÿ", "")).strip()
                teacher  = str(c.get("æ•™å¸«", "")).strip()
                category = str(c.get("é¸åˆ¥", "")).strip()
                credits  = parse_credits(c.get("å­¸åˆ†"))
                ds       = str(c.get("è³‡æ–™ä¾†æº", "")).strip()

                if ds:
                    data_sources_all.append(ds)

                if not name:
                    continue

                summary = f"{name}"
                if code:
                    summary += f"({code})"
                if teacher:
                    summary += f" / {teacher}"
                if category:
                    summary += f" / {category}"
                if credits is not None:
                    summary += f" / {credits}å­¸åˆ†"

                entry: Dict[str, Any] = {
                    "èª²ç¨‹åç¨±": name,
                    "èª²è™Ÿ": code,
                    "æ•™å¸«": teacher,
                    "é¸åˆ¥": category,
                    "å­¸åˆ†": credits,
                    "è³‡æ–™ä¾†æº": ds,
                    "æ¦‚è¦": summary,
                }
                course_entries.append(entry)

            if not course_entries:
                continue

            data_source_str = "ï¼›".join(sorted(set(data_sources_all)))

            # ========== (A) overview docsï¼ˆâ‰¤500 å­—å…ƒï¼Œä¸æ‹†èª²ï¼‰ ==========
            header_lines = [
                f"å­¸å¹´å­¸æœŸï¼š{year_term}",
                f"æ‰€å±¬å¹´ç´šï¼š{grade_name}",
                f"èª²ç¨‹æ•¸ï¼š{course_count}",
                "",
                "èª²ç¨‹åå–®ï¼š",
            ]
            tail_lines: List[str] = []
            if data_source_str:
                tail_lines = ["", f"è³‡æ–™ä¾†æºï¼š{data_source_str}"]

            # ä¾å­—æ•¸æ‹†æˆå¤šå€‹ã€Œèª²ç¨‹ç¸½è¦½ chunkã€
            batches = batch_courses_by_chars(
                header_lines=header_lines,
                course_entries=course_entries,
                tail_lines=tail_lines,
                max_chars=500,
            )

            num_chunks = len(batches)

            for chunk_idx, course_batch in enumerate(batches):
                overview_global_idx += 1

                # æº–å‚™çµ¦é‡å¯«å™¨çš„ recordï¼šä¸€å€‹ chunk = æŸå­¸æœŸæŸå¹´ç´šçš„ä¸€éƒ¨åˆ†èª²ç¨‹ç¸½è¦½
                record: Dict[str, Any] = {
                    "å­¸å¹´å­¸æœŸ": str(year_term),
                    "å¹´ç´š": str(grade_name),
                    "èª²ç¨‹ç¸½æ•¸": course_count,
                    "æœ¬æ‰¹èª²ç¨‹æ•¸": len(course_batch),
                    "èª²ç¨‹åˆ—è¡¨": course_batch,          # list[dict]ï¼Œæ¯ç­†æ˜¯ä¸€é–€èª²
                    "è³‡æ–™ä¾†æº": data_source_str,
                    "ä¾†æºæª”æ¡ˆ": source_path,
                }

                if overview_global_idx == 9:  # åªå°å‰å…©å€‹ chunkï¼Œé¿å…çˆ† log
                    print("[DEBUG course_history record]")
                    print(json.dumps(record, ensure_ascii=False, indent=2))
                    # ç„¶å¾Œå†å‘¼å« rewrite_json_record(...)

                try:
                    overview_text = rewrite_json_record(
                        record=record,
                        schema_hint="course_history_overview",
                        max_chars=500,
                    )
                except Exception as e:
                    print(f"[course_history_nested_to_documents] rewrite_json_record ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}")
                    sys.exit(1)

                # metadataï¼šé€™å€‹ chunk å…§çš„èª²ç¨‹æ¦‚è¦å­—ä¸²ï¼ˆåŸæœ¬å« course_namesï¼‰
                course_summaries_in_chunk = [
                    entry.get("æ¦‚è¦", "") for entry in course_batch if entry.get("æ¦‚è¦")
                ]

                docs.append(Document(
                    page_content=overview_text.strip(),
                    metadata={
                        "source": source_path,
                        "file_type": "json",
                        "type": "course_history_overview",
                        "content_type": "course_history_overview",

                        "year_term": str(year_term),
                        "year": year,
                        "term": term,
                        "grade": str(grade_name),

                        "course_count": course_count,
                        "course_names": "ã€".join(course_summaries_in_chunk),
                        "data_source": data_source_str,

                        "idx": overview_global_idx,  # int
                        "chunk": chunk_idx,
                        "total_chunks": num_chunks,
                        "needs_split": False,
                    }
                ))

    return docs


# =========================
# calendar.jsonï¼ˆè¡Œäº‹æ›†ï¼šä¾æœˆåˆ†åˆ‡å¡Šï¼‰ adapter
# =========================

def calendar_months_to_documents(
    data: List[Dict[str, Any]], source_path: str
) -> List[Document]:
    docs: List[Document] = []
    if not data:
        return docs

    def to_int(x) -> int | None:
        try:
            s = str(x).strip()
            return int(s) if s else None
        except Exception:
            return None

    def parse_day_start(day_raw: str) -> int | None:
        """
        æŠ“ã€Œèµ·å§‹æ—¥ã€æ’åºç”¨ï¼š
        - "1" -> 1
        - "8~12" -> 8
        - "10/13~11/3" -> 13 (å–èµ·å§‹æ—¥)
        è§£æå¤±æ•—å°± None
        """
        s = (day_raw or "").strip()
        if not s:
            return None
        # å–ç¬¬ä¸€æ®µå¯èƒ½çš„æ•¸å­—
        m = re.search(r"(\d+)", s)
        if not m:
            return None
        try:
            return int(m.group(1))
        except Exception:
            return None

    # 1) ä¾ (å¹´, æœˆ) åˆ†çµ„
    grouped: Dict[tuple[int | None, int | None], List[Dict[str, Any]]] = {}
    for rec in data:
        y = to_int(rec.get("å¹´"))
        m = to_int(rec.get("æœˆ"))
        grouped.setdefault((y, m), []).append(rec)

    # 2) ä¾å¹´/æœˆæ’åºè¼¸å‡º
    month_items = sorted(grouped.items(), key=lambda kv: (kv[0][0] or 0, kv[0][1] or 0))

    idx = 0
    for (year_roc, month), items in month_items:
        idx += 1
        year_ad = year_roc + 1911 if year_roc is not None else None

        # å…ˆæŒ‰ã€Œèµ·å§‹æ—¥ã€ç²—æ’åºï¼ˆNone çš„ä¿æŒåŸé †åºï¼‰
        items_sorted = sorted(
            items,
            key=lambda r: (
                parse_day_start(str(r.get("æ—¥", ""))) is None,
                parse_day_start(str(r.get("æ—¥", ""))) or 0,
            ),
        )

        # 3) æ•´ç†æˆæ´»å‹•åˆ—è¡¨ï¼ˆçµ¦é‡å¯«å™¨ & metadata ç”¨ï¼‰
        events_entries: List[Dict[str, Any]] = []
        events_for_meta: List[str] = []
        data_sources: List[str] = []

        for r in items_sorted:
            day_raw = str(r.get("æ—¥", "")).strip()
            weekday = str(r.get("æ˜ŸæœŸ", "")).strip()
            event = str(r.get("æ´»å‹•äº‹é …", "")).strip()
            ds = str(r.get("è³‡æ–™ä¾†æº", "")).strip()

            if ds:
                data_sources.append(ds)

            # çµ¦ metadata ç”¨çš„ç°¡å–®å­—ä¸²
            if event:
                events_for_meta.append(f"{month}/{day_raw}:{event}")

            # çµ¦é‡å¯«å™¨ç”¨çš„çµæ§‹åŒ–æ´»å‹•è³‡è¨Š
            events_entries.append(
                {
                    "æ—¥": day_raw,
                    "æ˜ŸæœŸ": weekday,
                    "æ´»å‹•äº‹é …": event,
                    "è³‡æ–™ä¾†æº": ds,
                }
            )

        data_source_str = "ï¼›".join(sorted(set(data_sources)))

        # 4) æº–å‚™é€™å€‹ã€Œæœˆä»½ç¸½è¦½ã€çš„ recordï¼Œä¸Ÿçµ¦é‡å¯«å™¨
        header_title = f"{year_roc if year_roc is not None else ''}å¹´{month if month is not None else ''}æœˆè¡Œäº‹æ›†"

        record: Dict[str, Any] = {
            "è¡Œäº‹æ›†æ¨™é¡Œ": header_title,
            "æ°‘åœ‹å¹´": year_roc,
            "è¥¿å…ƒå¹´": year_ad,
            "æœˆä»½": month,
            "æ´»å‹•æ•¸é‡": len(events_entries),
            "æ´»å‹•åˆ—è¡¨": events_entries,
            "è³‡æ–™ä¾†æº": data_source_str,
            "ä¾†æºæª”æ¡ˆ": source_path,
        }

        try:
            text = rewrite_json_record(
                record=record,
                schema_hint="calendar_month",  # å°æ‡‰é€™ç¨®ã€Œæœˆä»½ç¸½è¦½ã€è³‡æ–™
                max_chars=500,
            )
        except Exception as e:
            print(f"[calendar_months_to_documents] rewrite_json_record ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}")
            sys.exit(1)

        # 5) ç‰¹åŒ– metadataï¼ˆç¶­æŒåŸæœ¬æ¬„ä½ï¼‰
        meta = {
            "source": source_path,
            "file_type": "json",
            "type": "calendar_month",
            "content_type": "calendar_month",

            "title": str(items_sorted[0].get("title", "")).strip(),
            "year_roc": year_roc,
            "year_ad": year_ad,
            "month": month,

            "event_count": len(items_sorted),
            "events": "ã€".join(events_for_meta),   # å­˜æˆå­—ä¸²ï¼Œæ–¹ä¾¿ filter / æª¢ç´¢
            "data_source": data_source_str,

            "idx": idx,
            "needs_split": False,  # æœˆ chunk ä¸å†äºŒæ¬¡åˆ‡
        }

        docs.append(Document(page_content=text.strip(), metadata=meta))

    return docs

def calendar_events_to_documents(
    data: List[Dict[str, Any]], source_path: str
) -> List[Document]:
    """
    å°‡è¡Œäº‹æ›†æ¯ä¸€ç­†æ´»å‹•ç¨ç«‹æˆä¸€ä»½ Documentï¼Œ
    ä¸¦è£œ event_date/event_date_ts è®“ retriever èƒ½ç”¨æ™‚é–“ filterã€‚
    å…§å®¹æœ¬é«”æ”¹ç”¨ rewrite_json_record åšè‡ªç„¶èªå¥é‡å¯«ã€‚
    """
    docs: List[Document] = []
    if not data:
        return docs

    tz = ZoneInfo("Asia/Taipei")

    def to_int(x) -> int | None:
        try:
            s = str(x).strip()
            return int(s) if s else None
        except Exception:
            return None

    def parse_range(day_raw: str) -> tuple[int | None, int | None, int | None, int | None]:
        """
        å¾ã€Œæ—¥ã€æ¬„æŠ“èµ·è¨–(æœˆ,æ—¥)ï¼š
        - "1" -> (None, 1, None, 1)
        - "8~12" -> (None, 8, None, 12)
        - "10/13~11/3" -> (10, 13, 11, 3)
        """
        s = (day_raw or "").strip()
        if not s:
            return None, None, None, None

        m = re.match(r"(\d+)\s*/\s*(\d+)\s*~\s*(\d+)\s*/\s*(\d+)", s)
        if m:
            return int(m.group(1)), int(m.group(2)), int(m.group(3)), int(m.group(4))

        m = re.match(r"(\d+)\s*~\s*(\d+)", s)
        if m:
            return None, int(m.group(1)), None, int(m.group(2))

        m = re.search(r"(\d+)", s)
        if m:
            d = int(m.group(1))
            return None, d, None, d

        return None, None, None, None
    
    def fmt_roc_ad(y_roc: int, y_ad: int, m: int, d: int) -> str:
        # å›ºå®šæ ¼å¼ï¼šæ°‘åœ‹115å¹´6æœˆ8æ—¥ï¼ˆè¥¿å…ƒ2026å¹´6æœˆ8æ—¥ï¼‰
        return f"æ°‘åœ‹{y_roc}å¹´{m}æœˆ{d}æ—¥ï¼ˆè¥¿å…ƒ{y_ad}å¹´{m}æœˆ{d}æ—¥ï¼‰"

    def build_unified_date(
        y_roc: int | None,
        y_ad: int | None,
        month: int | None,
        day_raw: str,
    ) -> tuple[str | None, str | None, int | None]:
        """
        å›å‚³ï¼š
        - unified_date_strï¼šå›ºå®šæ ¼å¼æ—¥æœŸï¼ˆå«å€é–“ï¼‰
        - event_date_isoï¼šèµ·å§‹æ—¥ ISOï¼ˆYYYY-MM-DDï¼Œç”¨ metadata/filterï¼‰
        - event_date_tsï¼šèµ·å§‹æ—¥ timestampï¼ˆç”¨ metadata/filterï¼‰
        """
        if not (y_roc and y_ad and month and day_raw):
            return None, None, None

        sm_raw, sd, em_raw, ed = parse_range(day_raw)
        sm = sm_raw if sm_raw is not None else month
        em = em_raw if em_raw is not None else month

        if not (sm and sd):
            return None, None, None

        # èµ·å§‹æ—¥ metadata
        event_date_iso = f"{y_ad:04d}-{sm:02d}-{sd:02d}"
        event_date_ts = int(datetime(y_ad, sm, sd, tzinfo=tz).timestamp())

        # çµ±ä¸€æ—¥æœŸå­—ä¸²ï¼ˆå«å€é–“ï¼‰
        start_str = fmt_roc_ad(y_roc, y_ad, sm, sd)

        if em and ed and (em != sm or ed != sd):
            end_str = fmt_roc_ad(y_roc, y_ad, em, ed)
            unified = f"{start_str}è‡³{end_str}"
        else:
            unified = start_str

        return unified, event_date_iso, event_date_ts


    idx = 0
    for rec in data:
        year_roc = to_int(rec.get("å¹´"))
        month = to_int(rec.get("æœˆ"))
        day_raw = str(rec.get("æ—¥", "")).strip()

        year_ad = year_roc + 1911 if year_roc is not None else None

        unified_date_str, event_date_iso, event_date_ts = build_unified_date(
            y_roc=year_roc,
            y_ad=year_ad,
            month=month,
            day_raw=day_raw,
        )

        # è§£æèµ·å§‹æœˆä»½/æ—¥ï¼ˆçµ¦ metadata ç”¨ï¼Œç¶­æŒä½ åŸæœ¬çš„ month/day æ’åºèªæ„ï¼‰
        sm_raw, sd, _, _ = parse_range(day_raw)
        start_m = sm_raw if sm_raw is not None else month
        start_d = sd


        weekday = str(rec.get("æ˜ŸæœŸ", "")).strip()
        activity = str(rec.get("æ´»å‹•äº‹é …", "")).strip()
        url = str(rec.get("è³‡æ–™ä¾†æº", "")).strip()
        title = str(rec.get("title", "è¡Œäº‹æ›†")).strip()

        idx += 1

        # === æº–å‚™çµ¦é‡å¯«å™¨çš„ record ===
        record: Dict[str, Any] = {
            "æ¨™é¡Œ": title,
            "æ—¥æœŸ": unified_date_str,      # âœ… å”¯ä¸€æ—¥æœŸä¾†æºï¼šå›ºå®šæ ¼å¼ï¼ˆå«æ°‘åœ‹+è¥¿å…ƒï¼‰
            "åŸå§‹æ—¥æ¬„ä½": day_raw,          # âœ… ä»ä¿ç•™åŸå§‹å­—ä¸²ï¼ˆæ–¹ä¾¿é™¤éŒ¯ã€ä¹Ÿé¿å…è³‡è¨Šæå¤±ï¼‰
            "æ˜ŸæœŸ": weekday,
            "æ´»å‹•äº‹é …": activity,
            "è³‡æ–™ä¾†æº": url,
            "ä¾†æºæª”æ¡ˆ": source_path,
        }

        try:
            text = rewrite_json_record(
                record=record,
                schema_hint="calendar_event",  # å–®ç­†è¡Œäº‹æ›†æ´»å‹•
                max_chars=400,
            )
        except Exception as e:
            print(f"[calendar_events_to_documents] rewrite_json_record ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}")
            sys.exit(1)

        meta = {
            "source": source_path,
            "file_type": "json",

            # âœ… çµ¦ retriever/filter ç”¨
            "type": "calendar_event",
            "content_type": "calendar_event",

            "title": title,
            "year_roc": year_roc,
            "year_ad": year_ad,
            "month": start_m,
            "day_raw": day_raw,
            "weekday": weekday,

            "event_date": event_date_iso,
            "event_date_ts": event_date_ts,   # âœ… é—œéµï¼šepoch int

            "activity": activity,
            "source_url": url,
            "idx": idx,
            "needs_split": False,
        }

        docs.append(Document(page_content=text.strip(), metadata=meta))

    return docs
    
# =========================
# program_courses.jsonï¼ˆä»¥èª²ç¨‹é¡åˆ¥åˆ†çµ„åˆ‡å¡Š + å­¸ç¨‹ç¸½è¦½ï¼‰ adapter
# =========================

def program_courses_to_documents(
    data: List[Dict[str, Any]], source_path: str
) -> List[Document]:
    docs: List[Document] = []

    if not data:
        return []

    # å–å­¸ç¨‹å±¤ç´šè³‡è¨Šï¼ˆæ¯ç­†éƒ½ä¸€æ¨£ï¼Œæ‹¿ç¬¬ä¸€ç­†å³å¯ï¼‰
    program_title = str(data[0].get("title", "")).strip()
    program_purpose = str(data[0].get("è¨­ç½®å®—æ—¨", "")).strip()
    program_target = str(data[0].get("é©ç”¨å°è±¡", "")).strip()

    # å¾ JSON ä¸­æŠ“å­¸æ ¡ç¶²å€ç•¶ä½œä¸»è¦ä¾†æºï¼ˆæ²’æœ‰å°±é€€å›æœ¬æ©Ÿè·¯å¾‘ï¼‰
    source_url = ""
    for rec in data:
        src = str(rec.get("è³‡æ–™ä¾†æº") or "").strip()
        if src:
            source_url = src
            break
    if not source_url:
        source_url = source_path  # æœ€å£æƒ…æ³æ‰ç”¨æª”å

    def parse_credits(x: Any) -> float | None:
        try:
            s = str(x).strip()
            if not s or s.lower() == "nan":
                return None
            return float(s)
        except Exception:
            return None

    def extract_substitutes(note: str) -> str:
        note = note or ""
        alts = re.findall(r"ã€([^ã€‘]+)ã€‘", note)
        alts = [a.strip() for a in alts if a.strip()]
        return "ã€".join(alts)

    # 1) ä¾èª²ç¨‹é¡åˆ¥åˆ†çµ„
    grouped: Dict[str, List[Dict[str, Any]]] = {}
    for rec in data:
        cat = str(rec.get("èª²ç¨‹é¡åˆ¥", "")).strip() or "æœªåˆ†é¡"
        grouped.setdefault(cat, []).append(rec)

    # çµ¦ã€Œå­¸ç¨‹ç¸½è¦½ã€ç”¨çš„å½™ç¸½
    stats_by_cat: Dict[str, Dict[str, Any]] = {}
    total_courses = 0
    total_required = 0
    total_credits = 0.0

    # 2) æ¯å€‹é¡åˆ¥ â†’ ä¸€ä»½ Documentï¼ˆä¸Ÿçµ¦é‡å¯«å™¨ï¼‰
    for idx, (cat, items) in enumerate(grouped.items(), 1):
        course_names: List[str] = []
        required_count = 0
        credits_sum = 0.0

        # æº–å‚™çµ¦é‡å¯«å™¨ç”¨çš„ã€Œèª²ç¨‹åˆ—è¡¨ã€
        course_entries: List[Dict[str, Any]] = []

        for rec in items:
            code = str(rec.get("èª²ç¨‹ä»£ç¢¼", "")).strip()
            name = str(rec.get("èª²ç¨‹åç¨±", "")).strip()
            credits = parse_credits(rec.get("å­¸åˆ†æ•¸"))
            note = str(rec.get("å‚™è¨»", "")).strip()
            note = "" if note.lower() == "nan" else note

            required = ("å¿…ä¿®" in note) or ("å¿…é¸" in note)
            if required:
                required_count += 1

            if credits is not None:
                credits_sum += credits

            substitutes = extract_substitutes(note)

            # çµ¦é‡å¯«å™¨çœ‹çš„å–®ç­†èª²ç¨‹è³‡è¨Š
            entry: Dict[str, Any] = {
                "èª²ç¨‹ä»£ç¢¼": code,
                "èª²ç¨‹åç¨±": name,
                "å­¸åˆ†æ•¸": credits,
                "å‚™è¨»": note,
                "æ˜¯å¦å¿…ä¿®": required,
                "å¯æ›¿ä»£èª²ç¨‹": substitutes,  # å¾å‚™è¨»ä¸­æŠ½å‡ºçš„æ›¿ä»£èª²ç¨‹è³‡è¨Š
            }
            course_entries.append(entry)

            # çµ¦ metadata ç”¨çš„ç°¡å–®åç¨±å­—ä¸²
            if name and code:
                course_names.append(f"{name}({code})")
            elif name:
                course_names.append(name)

        # é¡åˆ¥å±¤ç´šçš„å½™ç¸½ï¼Œä¹‹å¾Œçµ¦ã€Œå­¸ç¨‹ç¸½è¦½ã€ç”¨
        stats_by_cat[cat] = {
            "èª²ç¨‹é¡åˆ¥": cat,
            "èª²ç¨‹æ•¸": len(items),
            "å¿…ä¿®èª²ç¨‹æ•¸": required_count,
            "ç¸½å­¸åˆ†æ•¸": credits_sum,
            "èª²ç¨‹åç¨±åˆ—è¡¨": course_names,
        }
        total_courses += len(items)
        total_required += required_count
        total_credits += credits_sum

        # æº–å‚™æ•´å€‹ã€Œå­¸ç¨‹ï¼‹é¡åˆ¥ã€çš„ recordï¼Œä¸Ÿçµ¦é‡å¯«å™¨
        record: Dict[str, Any] = {
            "å­¸ç¨‹åç¨±": program_title,
            "å­¸ç¨‹è¨­ç½®å®—æ—¨": program_purpose,
            "å­¸ç¨‹é©ç”¨å°è±¡": program_target,
            "èª²ç¨‹é¡åˆ¥": cat,
            "èª²ç¨‹åˆ—è¡¨": course_entries,
            "èª²ç¨‹ç¸½æ•¸": len(items),
            "å¿…ä¿®èª²ç¨‹æ•¸": required_count,
            "ç¸½å­¸åˆ†æ•¸": credits_sum,
            "è³‡æ–™ä¾†æº": source_url,
            "ä¾†æºæª”æ¡ˆ": source_path,
        }

        try:
            text = rewrite_json_record(
                record=record,
                schema_hint="program_courses",   # å°æ‡‰é€™ç¨®ã€Œå­¸ç¨‹èª²ç¨‹ã€è³‡æ–™
                max_chars=500,
            )
            # ğŸ” ä»ä¿ç•™ sample debug
            if idx == 1:
                print("\n[DEBUG program_courses_to_documents] sample output:")
                print(text[:1000])
        except Exception as e:
            print(f"[program_courses_to_documents] rewrite_json_record ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}")
            sys.exit(1)

        meta = {
            # âœ… æŠŠ source æ”¹å›ã€Œæª”åã€ï¼Œè®“ inspect_chroma å¯ä»¥ç”¨æª”åæœå°‹
            "source": source_path,
            "source_path": source_path,  # å¦‚æœä¹‹å¾Œé‚„æƒ³çŸ¥é“æœ¬æ©Ÿä¾†æºï¼Œå¯ä»¥å¤šç•™ä¸€ä»½
            "source_url": source_url,

            "file_type": "json",
            "type": "program_course_category",
            "content_type": "program_course_category",

            # å­¸ç¨‹å±¤ç´š
            "program_title": program_title,
            "program_purpose": program_purpose,
            "program_target": program_target,

            # é¡åˆ¥å±¤ç´šï¼ˆåˆ‡å¡Š keyï¼‰
            "course_category": cat,
            "course_count": len(items),
            "required_count": required_count,
            "credits_sum": credits_sum,

            # ç‚ºäº† metadata å¯ç´¢å¼•ã€ä¸èƒ½æ”¾ list â†’ è½‰å­—ä¸²
            "courses": "ã€".join(course_names),

            "idx": idx,
            "needs_split": False,
        }

        docs.append(Document(page_content=text.strip(), metadata=meta))

    # 3) åŠ ä¸Šä¸€å€‹ã€Œå­¸ç¨‹ç¸½è¦½ã€ chunk
    if stats_by_cat:
        # æŠŠå„é¡åˆ¥çš„å½™ç¸½æ•´ç†æˆ listï¼Œçµ¦é‡å¯«å™¨ç”¨
        category_overviews: List[Dict[str, Any]] = []
        for cat, s in stats_by_cat.items():
            category_overviews.append(
                {
                    "èª²ç¨‹é¡åˆ¥": cat,
                    "èª²ç¨‹æ•¸": s["èª²ç¨‹æ•¸"],
                    "å¿…ä¿®èª²ç¨‹æ•¸": s["å¿…ä¿®èª²ç¨‹æ•¸"],
                    "ç¸½å­¸åˆ†æ•¸": s["ç¸½å­¸åˆ†æ•¸"],
                    "èª²ç¨‹åç¨±åˆ—è¡¨": s["èª²ç¨‹åç¨±åˆ—è¡¨"],
                }
            )

        overview_record: Dict[str, Any] = {
            "å­¸ç¨‹åç¨±": program_title,
            "å­¸ç¨‹è¨­ç½®å®—æ—¨": program_purpose,
            "å­¸ç¨‹é©ç”¨å°è±¡": program_target,
            "èª²ç¨‹é¡åˆ¥ç¸½æ•¸": len(stats_by_cat),
            "èª²ç¨‹ç¸½æ•¸": total_courses,
            "å¿…ä¿®èª²ç¨‹ç¸½æ•¸": total_required,
            "ç¸½å­¸åˆ†æ•¸": total_credits,
            "å„é¡åˆ¥èª²ç¨‹æ¦‚è¦½": category_overviews,
            "è³‡æ–™ä¾†æº": source_url,
            "ä¾†æºæª”æ¡ˆ": source_path,
        }

        try:
            overview_text = rewrite_json_record(
                record=overview_record,
                schema_hint="program_courses_overview",
                max_chars=800,   # ç¸½è¦½å¯ä»¥ç¨å¾®é•·ä¸€é»
            )
        except Exception as e:
            print(
                "[program_courses_to_documents] rewrite_json_record "
                f"(overview) ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}"
            )
            sys.exit(1)

        overview_meta = {
            "source": source_path, # é€™æ¨£ inspect_chroma çœ‹åˆ°çš„ Source ä¹Ÿæœƒæ˜¯æª”å
            "source_path": source_path,
            "source_url": source_url,

            "file_type": "json",
            "type": "program_overview",
            "content_type": "program_overview",

            "program_title": program_title,
            "program_purpose": program_purpose,
            "program_target": program_target,

            "course_category_count": len(stats_by_cat),
            "course_total_count": total_courses,
            "course_total_required": total_required,
            "course_total_credits": total_credits,

            "idx": len(docs) + 1,
            "needs_split": False,
        }

        docs.append(Document(page_content=overview_text.strip(), metadata=overview_meta))

    return docs

# =========================
# course_overview.jsonï¼ˆèª²ç¨‹ç¸½è¦½ï¼‰ adapter
# =========================

def course_overview_to_documents(
    data: List[Dict[str, Any]], source_path: str
) -> List[Document]:
    docs: List[Document] = []

    def parse_year_term(s: str) -> tuple[int | None, str]:
        """ç°¡å–®æŠŠ '113ä¸Š' æ‹†æˆ (113, 'ä¸Š')ï¼Œå¤±æ•—å°± (None, '')ã€‚"""
        s = (s or "").strip()
        if not s:
            return None, ""
        year = None
        for i, ch in enumerate(s):
            if not ch.isdigit():
                try:
                    year = int(s[:i])
                except Exception:
                    year = None
                term = s[i:]
                return year, term
        try:
            return int(s), ""
        except Exception:
            return None, ""

    for i, rec in enumerate(data, 1):
        if not isinstance(rec, dict):
            # ä¿éšªè™•ç†ï¼šé dict å°±åŒ…æˆä¸€å€‹æ¬„ä½
            rec = {"value": rec}

        year_term_raw = str(rec.get("å­¸å¹´å­¸æœŸ", "")).strip()
        year, term = parse_year_term(year_term_raw)

        select_type = str(rec.get("é¸åˆ¥", "")).strip()        # å¿…ä¿® / é¸ä¿®
        grade = str(rec.get("æ‰€å±¬å¹´ç´š", "")).strip()          # ä¸€å¹´ç´š / äºŒå¹´ç´š / ä¸‰å¹´ç´š / å››å¹´ç´š
        data_source = str(rec.get("è³‡æ–™ä¾†æº", "")).strip()    # URL

        names = rec.get("èª²ç¨‹åç¨±") or []
        if not isinstance(names, list):
            names = [names]
        # æ¸…æ‰ç©ºå­—ä¸²ï¼Œå…¨éƒ¨è½‰æˆ str
        names = [str(n).strip() for n in names if str(n).strip()]
        course_count = len(names)
        courses_str = "ã€".join(names)

        # æº–å‚™çµ¦é‡å¯«å™¨çš„ record
        record: Dict[str, Any] = dict(rec)  # è¤‡è£½ä¸€ä»½ï¼Œé¿å…å‹•åˆ°åŸè³‡æ–™

        # è£œå……çµæ§‹åŒ–è³‡è¨Šçµ¦ LLM åƒè€ƒ
        record.setdefault("å­¸å¹´å­¸æœŸ", year_term_raw)
        record.setdefault("è§£æå­¸å¹´åº¦", year)           # int æˆ– None
        record.setdefault("è§£æå­¸æœŸ", term)             # "ä¸Š" / "ä¸‹" / ""
        record.setdefault("æ‰€å±¬å¹´ç´š", grade)
        record.setdefault("é¸åˆ¥", select_type)
        record.setdefault("èª²ç¨‹åç¨±åˆ—è¡¨", names)
        record.setdefault("èª²ç¨‹æ•¸", course_count)
        record.setdefault("è³‡æ–™ä¾†æº", data_source)
        record.setdefault("ä¾†æºæª”æ¡ˆ", source_path)

        try:
            rewritten = rewrite_json_record(
                record=record,
                schema_hint="course_overview",   # å°æ‡‰ detect_schema ä¸­çš„é¡å‹
                max_chars=400,
            )
        except Exception as e:
            print(f"[course_overview_to_documents] rewrite_json_record ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}")
            sys.exit(1)

        text = rewritten.strip()

        meta = {
            "source": source_path,
            "file_type": "json",
            "type": "course_overview",         # çµ¦çµ±è¨ˆ/é™¤éŒ¯ç”¨
            "content_type": "course_overview", # ä¹‹å¾Œ filter ç”¨é€™å€‹

            "year_term": year_term_raw,
            "year": year,                      # int æˆ– None
            "term": term,                      # "ä¸Š" / "ä¸‹" / ""
            "grade": grade,                    # ä¸€å¹´ç´š / äºŒå¹´ç´š / ä¸‰å¹´ç´š / å››å¹´ç´š

            "select_type": select_type,        # å¿…ä¿® / é¸ä¿®
            "course_count": course_count,      # int
            "courses": courses_str,            # âœ… å­—ä¸²ï¼Œä¸æ˜¯ list
            "data_source": data_source,        # è³‡æ–™ä¾†æº URL

            "idx": i,
            "needs_split": False,              # ä¸å†åˆ‡å¡Š
        }

        docs.append(Document(page_content=text, metadata=meta))

    return docs

# =========================
# course_history.jsonï¼ˆæ­·å¹´èª²ç¨‹è³‡æ–™ï¼‰ adapter
# =========================

def course_records_to_documents(
    data: List[Dict[str, Any]], source_path: str
) -> List[Document]:
    docs: List[Document] = []

    def parse_year_term(s: str) -> tuple[int | None, str]:
        # e.g. "113ä¸Š" / "113ä¸‹"
        s = (s or "").strip()
        if not s:
            return None, ""
        year = None
        for i, ch in enumerate(s):
            if not ch.isdigit():
                try:
                    year = int(s[:i])
                except Exception:
                    year = None
                term = s[i:]
                return year, term
        try:
            return int(s), ""
        except Exception:
            return None, ""

    for i, rec in enumerate(data, 1):
        yt = rec.get("å­¸å¹´å­¸æœŸ", "")
        year, term = parse_year_term(yt)

        code = rec.get("èª²è™Ÿ", "") or ""
        name = rec.get("èª²ç¨‹åç¨±", "") or ""
        teacher = rec.get("æ•™å¸«", "") or ""
        category = rec.get("é¸åˆ¥", "") or ""
        dept = rec.get("æ‰€å±¬ç³»æ‰€", "") or ""
        grade = rec.get("æ‰€å±¬å¹´ç´š", "") or ""
        data_source = rec.get("è³‡æ–™ä¾†æº", "") or ""

        credit_raw = rec.get("å­¸åˆ†", None)
        try:
            credits = float(credit_raw) if credit_raw not in (None, "", " ") else None
        except Exception:
            credits = None

        # çµ¦ LLM çš„æ–‡å­—
        lines = [
            f"å­¸å¹´å­¸æœŸï¼š{yt}",
            f"æ‰€å±¬å¹´ç´šï¼š{grade}",
            f"èª²è™Ÿï¼š{code}",
            f"èª²ç¨‹åç¨±ï¼š{name}",
            f"æ•™å¸«ï¼š{teacher}",
            f"é¸åˆ¥ï¼š{category}",
            f"å­¸åˆ†ï¼š{credits if credits is not None else ''}",
            f"æ‰€å±¬ç³»æ‰€ï¼š{dept}",
        ]
        if data_source:
            lines.append(f"è³‡æ–™ä¾†æºï¼š{data_source}")
        text = "\n".join(lines)

        meta = {
            "source": source_path,
            "file_type": "json",
            "type": "course_history",      # çµ±è¨ˆç”¨
            "content_type": "course",      # ä¹‹å¾Œ filter ç”¨é€™å€‹

            "year_term": yt,
            "year": year,                  # int or None
            "term": term,                  # "ä¸Š" / "ä¸‹" / ""
            "grade": grade,                # ä¸€å¹´ç´š / äºŒå¹´ç´š / ä¸‰å¹´ç´š / å››å¹´ç´š

            "course_code": code,
            "course_name": name,
            "teacher": teacher,
            "category": category,          # "å¿…ä¿®" / "é¸ä¿®"
            "required": (category == "å¿…ä¿®"),
            "credits": credits,            # float or None
            "department": dept,
            "data_source": data_source,    # è³‡æ–™ä¾†æº URL

            "idx": i,
            "needs_split": False,
        }

        docs.append(Document(page_content=text, metadata=meta))

    return docs

# =========================
# contact.jsonï¼ˆè¯çµ¡è³‡è¨Šï¼‰ adapter
# =========================

def contact_records_to_documents(
    data: List[Dict[str, Any]], source_path: str
) -> List[Document]:
    docs: List[Document] = []

    for i, rec in enumerate(data, 1):
        data_source = rec.get("è³‡æ–™ä¾†æº", "") or ""

        # === åˆ¤æ–·è¯çµ¡é¡å‹ï¼ˆroleï¼‰ï¼†æŠ½ metadata ç”¨çš„æ¬„ä½ ===
        if "è¾¦ç†é …ç›®" in rec:  # è¡Œæ”¿/æ‹›ç”Ÿé¡
            role = "service"
            item = str(rec.get("è¾¦ç†é …ç›®", "")).strip()
            person = str(rec.get("æ‰¿è¾¦äºº", "")).strip()
            ext = str(rec.get("åˆ†æ©Ÿ", "")).strip()
        elif "å­¸ç³»" in rec:   # å„å­¸ç³»è¯çµ¡äºº
            role = "department"
            item = ""  # é€™é¡æ²’æœ‰ã€Œè¾¦ç†é …ç›®ã€
            dept = str(rec.get("å­¸ç³»", "")).strip()
            person = str(rec.get("è¯çµ¡äººå“¡", "")).strip()
            ext = str(rec.get("åˆ†æ©Ÿ", "")).strip()
        else:
            role = "unknown"
            item = ""
            dept = str(rec.get("å­¸ç³»", "")).strip() if "å­¸ç³»" in rec else ""
            person = str(rec.get("æ‰¿è¾¦äºº") or rec.get("è¯çµ¡äººå“¡") or "").strip()
            ext = str(rec.get("åˆ†æ©Ÿ") or "").strip()

        # === æº–å‚™çµ¦é‡å¯«å™¨çš„ record ===
        record: Dict[str, Any] = dict(rec)  # æ‹·è²ä¸€ä»½ï¼Œé¿å…ç›´æ¥æ”¹åˆ°åŸå§‹è³‡æ–™

        # è£œå……ä¸€äº›èªæ„æç¤ºæ¬„ä½ï¼Œè®“ LLM å¥½å¯«ä¸€é»
        if role == "service":
            record.setdefault("è¯çµ¡é¡å‹", "è¡Œæ”¿æˆ–æ‹›ç”Ÿç›¸é—œæœå‹™è¯çµ¡è³‡è¨Š")
        elif role == "department":
            record.setdefault("è¯çµ¡é¡å‹", "å­¸ç³»è¯çµ¡äººè³‡è¨Š")
        else:
            record.setdefault("è¯çµ¡é¡å‹", "ä¸€èˆ¬è¯çµ¡è³‡è¨Š")

        if data_source:
            record.setdefault("è³‡æ–™ä¾†æº", data_source)
        record.setdefault("ä¾†æºæª”æ¡ˆ", source_path)

        try:
            rewritten = rewrite_json_record(
                record=record,
                schema_hint="contacts",   # å°æ‡‰ä½  detect_schema çš„é¡å‹
                max_chars=400,
            )
        except Exception as e:
            print(f"[contact_records_to_documents] rewrite_json_record ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}")
            sys.exit(1)

        text = rewritten.strip()

        # === metadata ä¿ç•™åŸæœ¬è¨­è¨ˆ ===
        meta = {
            "source": source_path,
            "file_type": "json",
            "type": "contact",           # çµ¦ä½ çµ±è¨ˆç”¨
            "content_type": "contact",   # ä¹‹å¾Œ filter ç”¨é€™å€‹
            "role": role,                # "service" or "department" or "unknown"
            "item": rec.get("è¾¦ç†é …ç›®") or "",
            "department": rec.get("å­¸ç³»") or "",
            "person": rec.get("æ‰¿è¾¦äºº") or rec.get("è¯çµ¡äººå“¡") or "",
            "phone": rec.get("åˆ†æ©Ÿ") or "",
            "data_source": data_source,
            "idx": i,
            "needs_split": False,
        }

        docs.append(Document(page_content=text, metadata=meta))

    return docs

# =========================
# academic_requirements.jsonï¼ˆå­¸å‰‡/ç•¢æ¥­è¦å®šï¼‰ adapter
# =========================

def academic_records_to_documents(
    data: List[Dict[str, Any]], source_path: str
) -> List[Document]:
    docs: List[Document] = []

    def infer_topic(category: str) -> str:
        if "ä¿®æ¥­è¦å®š" in category:
            return "graduation"
        if "å°ˆé¡Œ" in category:
            return "capstone"
        if "è¼”ç³»" in category:
            return "minor"
        if "è½‰ç³»" in category:
            return "transfer"
        if "å¯¦ç¿’" in category:
            return "internship"
        if "å£è©¦" in category:
            return "thesis_oral"
        return "general"

    for i, rec in enumerate(data, 1):
        category = str(rec.get("é¡åˆ¥", "")).strip()
        topic = infer_topic(category)

        # æº–å‚™çµ¦é‡å¯«å™¨çš„ recordï¼š
        # å…ˆè¤‡è£½åŸæœ¬çš„ recï¼Œä¸¦è£œä¸Šæ¨è«–å‡ºä¾†çš„ topicã€ä¾†æºç­‰è³‡è¨Š
        record: Dict[str, Any] = dict(rec)
        record["æ¨è«–ä¸»é¡Œ"] = topic              # çµ¦ LLM ä¸€é»èªæ„æç¤º
        record["ä¾†æºæª”æ¡ˆ"] = source_path

        try:
            rewritten = rewrite_json_record(
                record=record,
                schema_hint="academic_rules",    # å­¸ç± / ä¿®æ¥­è¦å®š / å°ˆé¡Œ / å¯¦ç¿’ ç­‰è¦å®š
                max_chars=500,                   # å¯ä»¥ç¨å¾®é•·ä¸€é»ï¼Œè¦–éœ€è¦å†èª¿æ•´
            )
        except Exception as e:
            print(f"[academic_records_to_documents] rewrite_json_record ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}")
            sys.exit(1)

        text = rewritten.strip()

        meta = {
            "source": source_path,
            "file_type": "json",
            "type": "academic_rules",
            "content_type": "academic_rule",
            "category": category,
            "topic": topic,   # å–®ä¸€å­—ä¸²ï¼Œæ–¹ä¾¿ filter
            "idx": i,
            "needs_split": False,
        }

        docs.append(Document(page_content=text, metadata=meta))

    return docs

# =========================
# schoolï¼ˆå­¸æ ¡è³‡è¨Šï¼‰ adapter
# =========================

def school_info_to_documents(obj: Any, source_path: str) -> List[Document]:
    """
    å°‡ about_school.json é€™é¡ã€Œå­¸æ ¡è³‡è¨Šã€æ•´ç†æˆä¸€ä»½ Documentï¼Œ
    ä¸¦åœ¨ metadata è£¡è£œå……å¸¸ç”¨æ¬„ä½ï¼ˆæ ¡åã€æ ¡è¨“ã€ç¶²å€ç­‰ï¼‰ã€‚
    ä¸»é«”å…§å®¹æ”¹ç‚ºäº¤çµ¦ rewrite_json_record åšè‡ªç„¶èªå¥é‡å¯«ã€‚
    """
    # é æœŸæ ¼å¼ï¼šlist[dict]
    if not isinstance(obj, list) or not obj:
        # éé æœŸæ ¼å¼å°±å…ˆèµ°åŸæœ¬çš„ç°¡å–® fallbackï¼Œä¸å‘¼å«é‡å¯«å™¨
        text = str(obj)
        meta = {
            "source": source_path,
            "file_type": "json",
            "content_type": "school",
            "needs_split": False,
            "idx": 1,
        }
        return [Document(page_content=text, metadata=meta)]

    # åšå€‹å®‰å…¨çš„ helperï¼šå¾ä¸åŒ block æŠ“ key
    def find_key(key: str, default: str = "") -> Any:
        for block in obj:
            if isinstance(block, dict) and key in block:
                return block[key]
        return default

    # ---------- æŠ½å‡ºçµæ§‹åŒ–æ¬„ä½ï¼ˆä½œç‚º metadata ç”¨ï¼‰ ----------
    # 1) åŸºæœ¬æ ¡å‹™
    name = find_key("åç¨±", "")
    name_en = find_key("è‹±æ–‡åç¨±", "")
    motto = find_key("æ ¡è¨“", "")
    founded_at = find_key("æˆç«‹æ™‚é–“", "")
    founder = find_key("å‰µè¾¦äºº", "")
    school_type = find_key("é¡å‹", "")

    # 2) è¯çµ¡è³‡è¨Š
    address = find_key("åœ°å€", "")
    phone = find_key("é›»è©±", "")
    emergency_phone = find_key("ç·Šæ€¥æ ¡å®‰å°ˆç·š", "")
    fax = find_key("å‚³çœŸ", "")
    president_phone = find_key("æ ¡é•·å®¤é›»è©±", "")
    president_fax = find_key("æ ¡é•·å®¤å‚³çœŸ", "")
    president_email = find_key("æ ¡é•·å®¤ email", "")

    # 3) å…¶ä»–æ ¡å‹™
    school_code = find_key("å­¸æ ¡ä»£ç¢¼", "")
    url = find_key("ç¶²å€", "")
    departments = find_key("ç³»æ‰€çµæ§‹", [])
    if isinstance(departments, list):
        departments_str = "ã€".join(map(str, departments))
    else:
        departments_str = str(departments) if departments else ""

    student_count = find_key("å­¸ç”Ÿäººæ•¸", "")
    mascots = find_key("æ ¡å‹å‰ç¥¥ç‰©", [])
    if isinstance(mascots, list):
        mascots_str = "ã€".join(map(str, mascots))
    else:
        mascots_str = str(mascots) if mascots else ""

    # 4) æ­·å²æ²¿é©
    prev_name = find_key("å‰èº«", "")
    reorg_at = find_key("æ”¹åˆ¶æ™‚é–“", "")
    rename_at = find_key("æ›´åæ™‚é–“", "")

    # 5) è¾¦å­¸ç‰¹è‰²
    feature = find_key("ç‰¹è‰²", "")
    focus_fields = find_key("é‡é»é ˜åŸŸ", [])
    if isinstance(focus_fields, list):
        focus_fields_str = "ã€".join(map(str, focus_fields))
    else:
        focus_fields_str = str(focus_fields) if focus_fields else ""

    philosophy = find_key("è¾¦å­¸ç†å¿µ", "")
    alliance = find_key("è¯ç›Ÿ", "")

    # ---------- æº–å‚™çµ¦é‡å¯«å™¨çš„ recordï¼ˆåŒ…å«åŸå§‹å€å¡Šï¼‰ ----------
    record: Dict[str, Any] = {
        # åŸºæœ¬æ ¡å‹™
        "åç¨±": name,
        "è‹±æ–‡åç¨±": name_en,
        "æ ¡è¨“": motto,
        "æˆç«‹æ™‚é–“": founded_at,
        "å‰µè¾¦äºº": founder,
        "é¡å‹": school_type,

        # è¯çµ¡è³‡è¨Š
        "åœ°å€": address,
        "é›»è©±": phone,
        "ç·Šæ€¥æ ¡å®‰å°ˆç·š": emergency_phone,
        "å‚³çœŸ": fax,
        "æ ¡é•·å®¤é›»è©±": president_phone,
        "æ ¡é•·å®¤å‚³çœŸ": president_fax,
        "æ ¡é•·å®¤ email": president_email,

        # å…¶ä»–æ ¡å‹™
        "å­¸æ ¡ä»£ç¢¼": school_code,
        "ç¶²å€": url,
        "ç³»æ‰€çµæ§‹": departments,      # ä¿ç•™åŸå§‹ listï¼ˆå¦‚æœæœ‰ï¼‰
        "å­¸ç”Ÿäººæ•¸": student_count,
        "æ ¡å‹å‰ç¥¥ç‰©": mascots,         # ä¿ç•™åŸå§‹ listï¼ˆå¦‚æœæœ‰ï¼‰

        # æ­·å²æ²¿é©
        "å‰èº«": prev_name,
        "æ”¹åˆ¶æ™‚é–“": reorg_at,
        "æ›´åæ™‚é–“": rename_at,

        # è¾¦å­¸ç‰¹è‰²
        "ç‰¹è‰²": feature,
        "é‡é»é ˜åŸŸ": focus_fields,      # ä¿ç•™åŸå§‹ listï¼ˆå¦‚æœæœ‰ï¼‰
        "è¾¦å­¸ç†å¿µ": philosophy,
        "è¯ç›Ÿ": alliance,

        # ä¿éšªï¼šæŠŠåŸå§‹ blocks ä¹Ÿæ”¾é€²å»ï¼Œè®“ LLM å¯ä»¥çœ‹åˆ°å®Œæ•´ JSON
        "åŸå§‹å€å¡Šåˆ—è¡¨": obj,
    }

    try:
        # max_chars å¯ä»¥è¦–æƒ…æ³èª¿æ•´ï¼Œå­¸æ ¡ç°¡ä»‹é€šå¸¸å¯ä»¥ç¨å¾®é•·ä¸€é»
        rewritten = rewrite_json_record(
            record=record,
            schema_hint="school_info",
            max_chars=500,
        )
    except Exception as e:
        print(f"[school_info_to_documents] rewrite_json_record ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}")
        sys.exit(1)

    text = rewritten.strip()

    # ---------- metadata ä¿æŒä½ åŸæœ¬çš„è¨­è¨ˆ ----------
    meta = {
        "source": source_path,
        "file_type": "json",
        "content_type": "school",

        # 1) åŸºæœ¬æ ¡å‹™
        "name": name,
        "name_en": name_en,
        "motto": motto,
        "founded_at": founded_at,
        "founder": founder,
        "school_type": school_type,

        # 2) è¯çµ¡è³‡è¨Š
        "address": address,
        "phone": phone,
        "emergency_phone": emergency_phone,
        "fax": fax,
        "president_phone": president_phone,
        "president_fax": president_fax,
        "president_email": president_email,

        # 3) å…¶ä»–æ ¡å‹™
        "school_code": school_code,
        "url": url,
        "departments": departments_str,    # å·²ç¶“æ˜¯ "ã€" ä¸²å¥½çš„å­—ä¸²
        "student_count": student_count,
        "mascots": mascots_str,

        # 4) æ­·å²æ²¿é©
        "prev_name": prev_name,
        "reorg_at": reorg_at,
        "rename_at": rename_at,

        # 5) è¾¦å­¸ç‰¹è‰²
        "feature": feature,
        "focus_fields": focus_fields_str,
        "philosophy": philosophy,
        "alliance": alliance,

        "needs_split": False,
        "idx": 1,
    }

    return [Document(page_content=text, metadata=meta)]

# =========================
# peopleï¼ˆè€å¸«åéŒ„ï¼‰ adapter
# =========================

def people_overview_to_documents(
    data: List[Dict[str, Any]],
    source_path: str,
    max_chars: int = 500,
) -> List[Document]:
    docs: List[Document] = []
    if not data:
        return docs

    # -------- helpers --------
    def count_chars(text: str) -> int:
        return len(text or "")

    def batch_lines_by_chars(
        header_lines: List[str],
        item_lines: List[str],
        tail_lines: List[str],
        max_chars: int,
    ) -> List[List[str]]:
        """
        ä¾ç…§å­—æ•¸æŠŠ header + item_lines + tail åˆ‡æˆå¤šæ‰¹ï¼Œæ¯æ‰¹å­—æ•¸ä¸è¶…é max_charsï¼ˆç›¡é‡ï¼‰ã€‚
        å›å‚³çš„æ¯å€‹ batch ä»æ˜¯ã€Œå­—ä¸²åˆ—è¡¨ã€ï¼Œæˆ‘å€‘å¾Œé¢æœƒå†å¾ä¸­è§£æå‡ºæˆå“¡æ¸…å–®ã€‚
        """
        batches: List[List[str]] = []

        fixed_text = "\n".join(header_lines + tail_lines)
        fixed_chars = count_chars(fixed_text)
        if fixed_chars >= max_chars:
            # header + tail å·²ç¶“è¶…éä¸Šé™ï¼Œå°±ä¸å†ç´°åˆ‡ï¼Œå…¨éƒ¨å¡ä¸€æ‰¹
            batches.append(header_lines + item_lines + tail_lines)
            return batches

        cur_chars = fixed_chars
        cur_items: List[str] = []

        for line in item_lines:
            lc = count_chars(line)
            if cur_items and (cur_chars + lc) > max_chars:
                batches.append(header_lines + cur_items + tail_lines)
                cur_items = []
                cur_chars = fixed_chars

            cur_items.append(line)
            cur_chars += lc

        if cur_items:
            batches.append(header_lines + cur_items + tail_lines)
        return batches

    def is_faculty_title(title: str) -> bool:
        t = title or ""
        if "ç³»å‹™åŠ©ç†" in t:
            return False
        # åªè¦å«æ•™æˆç³»è·ç¨±å°±ç®— facultyï¼ˆå«å…¼ä»»ï¼‰
        return any(k in t for k in ["è¬›åº§æ•™æˆ", "æ•™æˆ", "å‰¯æ•™æˆ", "åŠ©ç†æ•™æˆ"])

    def rank_group(title: str) -> str:
        t = title or ""
        if "è¬›åº§æ•™æˆ" in t:
            return "chair_professor"
        if "å…¼ä»»" in t and "æ•™æˆ" in t:
            return "adjunct_professor"
        # æ³¨æ„åˆ¤æ–·é †åºï¼šå…ˆå‰¯æ•™æˆ/åŠ©ç†æ•™æˆï¼Œå†æ•™æˆ
        if "å‰¯æ•™æˆ" in t:
            return "associate_professor"
        if "åŠ©ç†æ•™æˆ" in t:
            return "assistant_professor"
        if "æ•™æˆ" in t:
            return "professor"
        return "other"

    # -------- collect faculty --------
    faculty_rows = []
    dept_set = set()
    ds_set = set()

    for rec in data:
        title = str(rec.get("è·ç¨±", "") or rec.get("äººç‰©", "")).strip()
        if not is_faculty_title(title):
            continue

        name = str(rec.get("å§“å", "")).strip()
        if not name:
            # èˆŠæ ¼å¼ fallback
            name = str(rec.get("äººç‰©", "")).strip()

        dept = str(rec.get("ç³»æ‰€", "")).strip()
        if dept:
            dept_set.add(dept)

        ds = str(rec.get("è³‡æ–™ä¾†æº", "")).strip()
        if ds:
            ds_set.add(ds)

        # overview å–®è¡Œï¼ˆä¸å¯æ‹†çš„åŸå­ï¼‰ï¼Œå¾Œé¢ç”¨ä¾†åˆ‡ batch & é‚„åŸæˆå“¡åˆ—è¡¨
        line = f"{name} / {title}"
        faculty_rows.append((rank_group(title), line, name))

    if not faculty_rows:
        return docs

    data_source_str = "ï¼›".join(sorted(ds_set))
    departments_str = "ã€".join(sorted(dept_set))

    # -------- build overview scopes --------
    overview_idx = 0

    def emit_scope(scope: str, group: str, lines: List[str]):
        """
        scope: "faculty_all" æˆ– "rank_group"
        group: è·ç´šä»£ç¢¼ï¼ˆrank_group æ™‚æœ‰å€¼ï¼Œfaculty_all æ™‚ç‚º ""ï¼‰
        lines: ä¾‹å¦‚ ["å¼µä¸‰ / æ•™æˆ", "æå›› / å‰¯æ•™æˆ", ...]
        """
        nonlocal overview_idx, docs

        header = "æ•™æˆç¸½è¦½" if scope == "faculty_all" else f"{group} ç¸½è¦½"
        header_lines = [header, "æˆå“¡åˆ—è¡¨ï¼š"]
        item_lines = [f"- {ln}" for ln in lines]
        tail_lines = ["", f"è³‡æ–™ä¾†æºï¼š{data_source_str}"] if data_source_str else []

        # å…ˆç”¨åŸæœ¬çš„å­—æ•¸é‚è¼¯åˆ‡æˆå¤šå€‹ batchï¼Œå†å°æ¯å€‹ batch ä¸Ÿçµ¦é‡å¯«å™¨
        batches = batch_lines_by_chars(header_lines, item_lines, tail_lines, max_chars)
        total_chunks = len(batches)

        for chunk_i, batch_lines in enumerate(batches):
            # å¾ batch_lines ä¸­è§£æå‡ºè©² chunk çš„æˆå“¡æ¸…å–®ï¼ˆå§“å / è·ç¨±ï¼‰
            member_items = []
            for ln in batch_lines:
                ln = ln.strip()
                if not ln.startswith("- "):
                    continue
                raw = ln[2:].strip()  # å»æ‰å‰é¢çš„ "- "
                if " / " in raw:
                    name_part, title_part = raw.split(" / ", 1)
                else:
                    name_part, title_part = raw, ""
                member_items.append({
                    "å§“å": name_part.strip(),
                    "è·ç¨±": title_part.strip(),
                })

            # å¦‚æœé€™å€‹ chunk æ²’æœ‰ä»»ä½•æˆå“¡ï¼Œå°±ç•¥é
            if not member_items:
                continue

            overview_idx += 1

            # æº–å‚™çµ¦é‡å¯«å™¨ç”¨çš„ JSON record
            record = {
                "ç¸½è¦½æ¨™é¡Œ": header,
                "ç¯„åœé¡å‹": "å…¨éƒ¨æ•™æˆ" if scope == "faculty_all" else "è·ç´šåˆ†çµ„",
                "è·ç´šä»£ç¢¼": group if scope == "rank_group" else "",
                "ç³»æ‰€": departments_str,
                "æˆå“¡åˆ—è¡¨": member_items,
                "è³‡æ–™ä¾†æº": data_source_str,
            }

            try:
                text = rewrite_json_record(
                    record=record,
                    schema_hint="department_members_overview",
                    max_chars=max_chars,
                )
            except Exception as e:
                print(f"[people_overview_to_documents] rewrite_json_record ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}")
                sys.exit(1)

            names_in_chunk = [m["å§“å"] for m in member_items]

            docs.append(Document(
                page_content=text.strip(),
                metadata={
                    "source": source_path,
                    "file_type": "json",
                    "type": "people_overview",
                    "content_type": "people_overview",

                    "overview_scope": scope,
                    "rank_group": group if scope == "rank_group" else "",

                    "people_count": len(member_items),
                    "departments": departments_str,
                    "names": "ã€".join(names_in_chunk),
                    "data_source": data_source_str,

                    "idx": overview_idx,     # overview å…§å…¨åŸŸ int
                    "chunk": chunk_i,
                    "total_chunks": total_chunks,
                    "needs_split": False,
                }
            ))

    # (1) faculty_allï¼šå…¨é«”æ•™æˆ
    all_lines = [line for _, line, _ in faculty_rows]
    emit_scope("faculty_all", "", all_lines)

    # (2) rank_groupï¼šä¾è·ç´šåˆ†çµ„
    grouped: Dict[str, List[str]] = {}
    for rg, line, _name in faculty_rows:
        grouped.setdefault(rg, []).append(line)

    # å›ºå®šè¼¸å‡ºé †åº
    order = ["chair_professor", "professor", "associate_professor", "assistant_professor", "adjunct_professor"]
    for rg in order:
        lines = grouped.get(rg, [])
        if not lines:
            continue
        emit_scope("rank_group", rg, lines)

    return docs

_name_title_pat = re.compile(
    r"^\s*(?P<name>[\u4e00-\u9fa5A-Za-z0-9ï¼ãƒ»]+)\s*(?P<title>.+)?$"
)

def _parse_name_title(s: str) -> Dict[str, str]:
    m = _name_title_pat.match(s or "")
    if not m:
        return {"name": s or "", "title": ""}

    name = (m.group("name") or "").strip().replace("\u00a0", " ")
    title = (m.group("title") or "").strip(" ,ï¼Œ").replace("\u00a0", " ")

    # æœ‰äº›ä¾†æºæœƒæŠŠã€Œè·ç¨±/è·å‹™ï¼šã€é€™æ®µä¹Ÿå¡é€²ä¾†ï¼Œé€™é‚Šé †ä¾¿æ¸…æ‰é–‹é ­çš„æ¨™ç±¤
    if title.startswith("è·ç¨±/è·å‹™"):
        title = re.sub(r"^è·ç¨±/è·å‹™[:ï¼š]?\s*", "", title)

    return {"name": name, "title": title}

def people_records_to_documents(
    data: List[Dict[str, Any]], source_path: str
) -> List[Document]:
    docs: List[Document] = []
    for i, rec in enumerate(data, 1):
        # ===== é€™ä¸€æ®µæ˜¯ä½ åŸæœ¬æŠ“å§“å/è·ç¨±/ç³»æ‰€/ä¾†æº =====
        if "å§“å" in rec:
            who = {"name": rec.get("å§“å", "").strip(), "title": rec.get("è·ç¨±", "").strip()}
        else:
            who = _parse_name_title(rec.get("äººç‰©", ""))

        dept = rec.get("ç³»æ‰€") or rec.get("department") or "å¤§åŒå¤§å­¸ è³‡è¨Šå·¥ç¨‹å­¸ç³»"
        src_url = rec.get("è³‡æ–™ä¾†æº") or rec.get("source_url") or ""

        # ===== æ–°å¢ï¼šç”¨ LLM æŠŠé€™ä¸€ç­† JSON è½‰æˆæ•˜è¿°å¥ =====
        try:
            rewritten = rewrite_json_record(
                record=rec,
                schema_hint="department_members",   # æˆ– "è³‡å·¥ç³»å¸«è³‡åå–®"
                max_chars=400,
            )
        except Exception as e:
            print(f"rewrite_json_record ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            sys.exit(1)

        # ===== çµ„æˆ Document =====
        content = rewritten.strip()

        metadata = {
            "source": source_path,
            "idx": i,
            "name": who["name"],
            "title": who["title"],
            "department": dept,
            "url": src_url,
            "content_type": "people",
        }

        docs.append(Document(page_content=content, metadata=metadata))

    return docs

# =========================
# newsï¼ˆç³»ç¶²æ–°èï¼‰ adapter
# =========================
def _fmt_news_page_content(meta: Dict[str, Any], content: str) -> str:
    return "\n".join([
        f"é¡åˆ¥ï¼š{meta.get('category','')}",   # â† æ–°å¢        
        f"æ¨™é¡Œï¼š{meta.get('title','')}",
        f"æ—¥æœŸï¼š{meta.get('published_at','')}",
        f"é€£çµï¼š{meta.get('url','')}",        # â† æ–°å¢ï¼ˆæ–¹ä¾¿ LLM/æª¢ç´¢çŸ¥é“ä¾†æºï¼‰
        "å…§æ–‡ï¼š",
        content or "",
    ])

def news_records_to_documents(data: List[Dict[str, Any]], source_path: str) -> List[Document]:
    docs: List[Document] = []

    TARGET_CHARS = 1000
    OVERLAP_CHARS = 80

    total = len(data)
    print(f"[news] {source_path}ï¼šå…± {total} ç­†æ–°èï¼Œé–‹å§‹é‡å¯«â€¦")

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=TARGET_CHARS,
        chunk_overlap=OVERLAP_CHARS,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ã€", "ï¼š", "â€”â€”", " ", ",", ".", "ï¼Œ", ":"]
    )

    def to_ts(s: str | None) -> int | None:
        if not s:
            return None
        for fmt in ("%Y-%m-%d", "%Y/%m/%d", "%Y-%m-%d %H:%M", "%Y/%m/%d %H:%M"):
            try:
                return int(datetime.strptime(s, fmt).timestamp())
            except Exception:
                pass
        return None

    for i, rec in enumerate(data, 1):
        if not isinstance(rec, dict):
            rec = {"value": rec}

        title = rec.get("title") or ""
        content = rec.get("content") or ""
        published_at = rec.get("published_at")
        published_ts = to_ts(published_at)
        url = rec.get("url")
        category = rec.get("category") or ""   # å°æ‡‰ ttu_cse_news.sorted.json

        # ç‚ºæ¯ç¯‡æ–°èç”¢ç”Ÿç©©å®š article_idï¼ˆåˆ©æ–¼é‡çµ„èˆ‡å»é‡ï¼‰
        article_key = f"{source_path}|{url or title}|{published_at or ''}|{i}"
        article_id = hashlib.sha1(article_key.encode("utf-8")).hexdigest()[:12]

        base_meta = {
            "source": source_path,
            "file_type": "json",

            "type": "news",
            "content_type": "news",

            "url": url,
            "title": title,
            "category": category,
            "published_at": published_at,
            "published_at_ts": published_ts,

            "idx": i,
            "article_id": article_id,
            "needs_split": False,
        }

        # === æƒ…æ³ä¸€ï¼šå…§æ–‡é•·åº¦ä¸è¶…é TARGET_CHARSï¼Œæ•´ç¯‡ç•¶ä¸€å€‹ doc é‡å¯« ===
        if len(content) <= TARGET_CHARS:
            record: Dict[str, Any] = {
                "æ¨™é¡Œ": title,
                "åˆ†é¡": category,
                "ç™¼å¸ƒæ™‚é–“": published_at,
                "ç¶²å€": url,
                "æ–‡ç« å…§å®¹": content,
                "ä¾†æºæª”æ¡ˆ": source_path,
                "article_id": article_id,
                "published_at_ts": published_ts,
            }

            try:
                rewritten = rewrite_json_record(
                    record=record,
                    schema_hint="news_article",
                    max_chars=TARGET_CHARS,
                )
            except Exception as e:
                print(f"[news_records_to_documents] rewrite_json_record ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}")
                sys.exit(1)

            docs.append(Document(
                page_content=rewritten.strip(),
                metadata=base_meta
            ))
            if i == 1 or i % 10 == 0 or i == total:
                print(f"[news] {source_path}ï¼šå·²å®Œæˆ {i}/{total} ç­†ï¼ˆçŸ­æ–‡ï¼‰")
            continue

        # === æƒ…æ³äºŒï¼šå…§æ–‡å¤ªé•· â†’ å…ˆåˆ‡æˆå¤šå€‹ chunkï¼Œå†é€ chunk é‡å¯« ===
        parts = splitter.split_text(content)

        # å¦‚æœæœ€å¾Œä¸€å¡Šå¤ªçŸ­ï¼Œä½µå›å‰ä¸€å¡Šï¼ˆä½ åŸæœ¬çš„é‚è¼¯ï¼‰
        if len(parts) >= 2 and len(parts[-1]) < TARGET_CHARS // 3:
            parts[-2] = parts[-2] + ("\n" if not parts[-2].endswith("\n") else "") + parts[-1]
            parts.pop()

        for j, part in enumerate(parts):
            meta = dict(base_meta)
            meta.update({"chunk": j})

            # é‡å°ã€Œæ–‡ç« æŸä¸€æ®µã€çµ„æˆ recordï¼Œè®“ LLM çŸ¥é“é€™æ˜¯åŒä¸€ç¯‡æ–°èçš„å…¶ä¸­ä¸€éƒ¨åˆ†
            record_chunk: Dict[str, Any] = {
                "æ¨™é¡Œ": title,
                "åˆ†é¡": category,
                "ç™¼å¸ƒæ™‚é–“": published_at,
                "ç¶²å€": url,
                "æ–‡ç« å…§å®¹ç‰‡æ®µ": part,
                "æ‰€å±¬ç¯‡ç«  article_id": article_id,
                "chunk_index": j,
                "ä¾†æºæª”æ¡ˆ": source_path,
                "published_at_ts": published_ts,
            }

            try:
                rewritten_chunk = rewrite_json_record(
                    record=record_chunk,
                    schema_hint="news_article_chunk",
                    max_chars=TARGET_CHARS,
                )
            except Exception as e:
                print(f"[news_records_to_documents] rewrite_json_record (chunk) ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}")
                sys.exit(1)

            docs.append(Document(
                page_content=rewritten_chunk.strip(),
                metadata=meta
            ))

            # ä¸€ç¯‡é•·æ–‡æ‰€æœ‰ chunk éƒ½è™•ç†å®Œ
            if i == 1 or i % 10 == 0 or i == total:
                print(f"[news] {source_path}ï¼šå·²å®Œæˆ {i}/{total} ç­†ï¼ˆé•·æ–‡å¤š chunkï¼‰")

    return docs



# =========================
# JSON è¼‰å…¥èˆ‡åˆ†æ´¾
# =========================
def load_json_as_documents(path: Path) -> List[Document]:
    with open(path, "r", encoding="utf-8") as f:
        obj = json.load(f)

    schema = detect_schema(obj)
    if schema == "people":
        # 1) å–å‡º member_listï¼ˆæ”¯æ´å¤šç¨® people JSON æ ¼å¼ï¼‰
        if isinstance(obj, dict) and "ç¸½è¦½" in obj:
            overview = obj["ç¸½è¦½"] or {}
            member_list = overview.get("æˆå“¡åˆ—è¡¨", []) or []
        elif isinstance(obj, dict) and "æˆå“¡åˆ—è¡¨" in obj:
            member_list = obj.get("æˆå“¡åˆ—è¡¨", []) or []
        else:
            member_list = obj if isinstance(obj, list) else [obj]

        # 2) å€‹åˆ¥è€å¸« documents
        docs = people_records_to_documents(member_list, str(path))

        # 3) âœ… æ•™æˆç¸½è¦½ documentsï¼ˆä¸ç®¡æ ¼å¼éƒ½åŠ ï¼‰
        docs.extend(
            people_overview_to_documents(member_list, str(path), max_chars=500)
        )

        return docs
    elif schema == "news":
        data = obj if isinstance(obj, list) else [obj]
        return news_records_to_documents(data, str(path))
    elif schema == "school":
        # ç›´æ¥äº¤çµ¦ school adapter è™•ç†ï¼ˆä¸å† flattenï¼‰
        return school_info_to_documents(obj, str(path))
    elif schema == "academic_rules":
        data = obj if isinstance(obj, list) else [obj]
        return academic_records_to_documents(data, str(path))
    
    # ğŸ”¹ æ–°å¢ï¼šæ•¸ä½æ•™å­¸èª²ç¨‹å¯¦æ–½è¦é»ï¼‹å½ˆæ€§æ•™å­¸é€±
    elif schema == "flexible_week_rules":
        return flexible_week_rules_to_documents(obj, str(path))

    elif schema == "contacts":
        data = obj if isinstance(obj, list) else [obj]
        return contact_records_to_documents(data, str(path))
        # --- âœ… æ–°æ ¼å¼å·¢ç‹€èª²ç¨‹æ­·å² ---
    elif schema == "course_history_nested":
        return course_history_nested_to_documents(obj, str(path))
    elif schema == "course_overview":
        data = obj if isinstance(obj, list) else [obj]
        return course_overview_to_documents(data, str(path))
    elif schema == "program_courses":
        data = obj if isinstance(obj, list) else [obj]
        return program_courses_to_documents(data, str(path))
    
    # ğŸ”¹ æ–°å¢ï¼šå§Šå¦¹æ ¡åˆ—è¡¨
    elif schema == "sister_schools":
        return sister_schools_to_documents(obj, str(path))
    
    elif schema == "exchange_program_call":
        return exchange_program_call_to_documents(obj, str(path))

    elif schema == "calendar":
        data = obj if isinstance(obj, list) else [obj]
        docs = []
        docs.extend(calendar_months_to_documents(data, str(path)))   # æœˆç¸½è¦½ï¼ˆåŸæœ¬çš„ï¼‰
        docs.extend(calendar_events_to_documents(data, str(path)))   # âœ… æ–°å¢ï¼šå–®ç­†æ´»å‹•
        return docs
    elif schema == "required_by_semester":
        return required_by_semester_to_documents(obj, str(path))
    elif schema == "school_rule_articles":
        return school_rule_articles_to_documents(obj, str(path))
     # ğŸ”¹ æ–°å¢ï¼šæœ‰é™„æª”çš„ç³»è¦ / è¾¦æ³• JSON
    elif schema == "school_rule_file_articles":
        return school_rule_file_articles_to_documents(obj, str(path))
    elif schema == "single_page_rule":
        return single_page_rule_to_documents(obj, str(path))
    


        # å…¶ä»–å·²çŸ¥ schema éƒ½åœ¨ä¸Šé¢è™•ç†å®Œ
    else:
        # --- é€šç”¨ï¼šç”¨ LLM å…ˆæŠŠã€Œä¸€ç­† JSON è¨˜éŒ„ã€æ”¹å¯«æˆè‡ªç„¶èªå¥ï¼Œå†ç•¶æˆ doc ---
        # æ­£è¦åŒ–æˆ list
        if isinstance(obj, list):
            json_data = obj
        else:
            json_data = [obj]

        docs: List[Document] = []
        for idx, row in enumerate(json_data):
            if not isinstance(row, dict):
                row = {"value": row}

            text = rewrite_json_record(
                row,
                schema_hint=schema or path.stem,   # ä¾‹å¦‚ "faculty", "scholarship"
                max_chars=400,
            )

            docs.append(
                Document(
                    page_content=text,
                    metadata={
                        "source": str(path),        # é€™è£¡ç›´æ¥ç”¨ path å°±å¥½
                        "idx": idx,
                        "type": "json",
                        "schema": schema or "unknown",
                    },
                )
            )

        return docs


# =========================
# å…¶ä»–æ ¼å¼è¼‰å…¥
# =========================
def load_documents(data_dir: Path) -> List[Document]:
    docs: List[Document] = []
    for path in data_dir.rglob("*"):
        suf = path.suffix.lower()
        if suf == ".pdf":
            loader = PyPDFLoader(str(path))
            # PDF loader å·²ç¶“ä¸€é ä¸€ä»½ï¼Œå¾Œé¢ä»å»ºè­°åˆ‡ä¸€ä¸‹é•·é ï¼ˆäº¤ç”±ä¸»æµç¨‹ï¼‰
            for i, d in enumerate(loader.load(), 1):
                d.metadata.update({"source": str(path), "type": "pdf", "needs_split": True, "idx": i})
                docs.append(d)
        elif suf == ".docx":
            loader = Docx2txtLoader(str(path))
            for i, d in enumerate(loader.load(), 1):
                d.metadata.update({"source": str(path), "type": "docx", "needs_split": True, "idx": i})
                docs.append(d)
        elif suf == ".txt":
            with open(path, "r", encoding="utf-8") as f:
                text = f.read()
            docs.append(Document(page_content=text, metadata={"source": str(path), "type": "txt", "needs_split": True,"idx": 1}))
        elif suf == ".csv":
            loader = CSVLoader(file_path=str(path), encoding="utf-8")
            for i, d in enumerate(loader.load(), 1):
                d.metadata.update({"source": str(path), "type": "csv", "needs_split": True, "idx": i})
                docs.append(d)
        elif suf in {".json", ".jsonl"}:
            # ä½ çš„å…©å€‹ JSONï¼ˆdepartment_members.json, ttu_cse_news.sorted.jsonï¼‰æœƒèµ°é€™æ¢
            docs.extend(load_json_as_documents(path))
        else:
            # å¿½ç•¥å…¶ä»–æ ¼å¼
            continue
    return docs


# =========================
# ä¸»æµç¨‹ï¼ˆå»ºç«‹/æ›´æ–°ç´¢å¼•ï¼‰
# =========================
def main():
    assert DATA_DIR.exists(), "è«‹å…ˆæŠŠæª”æ¡ˆæ”¾é€² data/ ç›®éŒ„ï¼ˆæ”¯æ´ JSON/PDF/DOCX/CSV/TXTï¼‰"

    print("â–¶ è®€å–æª”æ¡ˆâ€¦")
    docs = load_documents(DATA_DIR)
    print(f"â–¶ è®€åˆ° {len(docs)} ä»½åŸå§‹æ–‡ä»¶/ç‰‡æ®µï¼ˆå«å·²åˆ‡å¥½çš„ JSON æ–‡ä»¶ï¼‰")

    # å°‡éœ€è¦å†åˆ‡å¡Šçš„æ–‡ä»¶æŒ‘å‡ºä¾†åˆ‡ï¼Œå…¶ä»–ç›´æ¥ä¿ç•™
    need_split: List[Document] = [d for d in docs if d.metadata.get("needs_split", False)]
    keep: List[Document] = [d for d in docs if not d.metadata.get("needs_split", False)]

    if need_split:
        print(f"â–¶ å° {len(need_split)} ä»½æ–‡ä»¶é€²ä¸€æ­¥åˆ‡å¡Šâ€¦")
        splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=200)
        
        split_more: List[Document] = []
        for doc in need_split:
            parts = splitter.split_documents([doc])  # å°å–®ä¸€æ–‡ä»¶åˆ‡ï¼Œä¿æŒåˆ†çµ„
            for j, part in enumerate(parts):
                part.metadata["needs_split"] = False
                part.metadata["chunk"] = j          # æ¯ä»½åŸå§‹æ–‡ä»¶çš„å¡Šåº
            split_more.extend(parts)

        final_docs = keep + split_more
        print(f"â–¶ ç”¢ç”Ÿ {len(split_more)} å€‹åˆ‡å¡Šï¼›åˆè¨ˆ {len(final_docs)} ä»½å¯å…¥åº«æ–‡ä»¶")
    else:
        final_docs = keep
        print(f"â–¶ ç„¡éœ€é¡å¤–åˆ‡å¡Šï¼›åˆè¨ˆ {len(final_docs)} ä»½å¯å…¥åº«æ–‡ä»¶")

    # çµ±è¨ˆå„ type æ–¹ä¾¿ä½ æª¢æŸ¥
    from collections import Counter
    ctype_count = Counter(
        d.metadata.get("content_type", d.metadata.get("type", "unknown"))
        for d in final_docs
    )
    print("â–¶ é¡å‹çµ±è¨ˆï¼š", dict(ctype_count))

    print("â–¶ æº–å‚™åµŒå…¥æ¨¡å‹(å¤šèª)â€¦")
    embeddings = HuggingFaceEmbeddings(
        model_name="BAAI/bge-m3",
        # model_kwargs={"device": "cpu"},
        model_kwargs={"device": "cuda"},
        # model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True},  # ğŸ‘ˆ è·Ÿ query.py ä¸€æ¨£
    )

    print("â–¶ å»ºç«‹/æ›´æ–° Chroma å‘é‡åº«â€¦")
    vectordb = Chroma(
        collection_name=COLL_NAME,
        embedding_function=embeddings,
        persist_directory=DB_DIR,
        collection_metadata={"hnsw:space": "cosine"},   # è·é›¢åº¦é‡ï¼šcosine / l2 / ip
    )

    # ç©©å®š IDï¼šé¿å…é‡è¤‡å¯«å…¥ï¼›åŒå…§å®¹+åŒä¾†æºæœƒå¾—åˆ°ç›¸åŒ id
    def stable_id(doc: Document) -> str:
        src = str(doc.metadata.get("source", ""))
        typ = str(doc.metadata.get("type", ""))
        file_type = str(doc.metadata.get("file_type", ""))
        content_type = str(doc.metadata.get("content_type", doc.metadata.get("type", "")))  # èˆ‡èˆŠ 'type' ç›¸å®¹
        aid = str(doc.metadata.get("article_id", ""))  # people/å…¶ä»–å‹åˆ¥æ²’æœ‰å°±ç•™ç©º
        idx = str(doc.metadata.get("idx", ""))
        chk = str(doc.metadata.get("chunk", 0))
        raw = f"{src}|{file_type}|{content_type}|{aid}|{idx}|{chk}".encode("utf-8")
        return hashlib.sha1(raw).hexdigest()

    ids = [stable_id(d) for d in final_docs]
    # é™¤éŒ¯
    from collections import Counter
    dups = [k for k, v in Counter(ids).items() if v > 1]
    if dups:
        raise RuntimeError(f"stable_id é‡è¦† {len(dups)} ç­†ï¼Œä¾‹ï¼š{dups[:3]}")

    vectordb.add_documents(final_docs, ids=ids)

    print("âœ… å®Œæˆç´¢å¼•å»ºç«‹/æ›´æ–°ï¼è³‡æ–™åº«ä½ç½®ï¼š", DB_DIR)
    print(f"âœ… collection = {COLL_NAME}ï¼Œå…±æ–°å¢/å»é‡å¾Œå¯«å…¥ {len(final_docs)} ä»½æ–‡ä»¶")


if __name__ == "__main__":
    main()
