import json
import re
import pandas as pd
from pathlib import Path
from datetime import datetime
from zoneinfo import ZoneInfo

from datasets import Dataset
from ragas import evaluate
from ragas.run_config import RunConfig
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_recall,
    context_precision,
    answer_correctness,
)

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from sentence_transformers import CrossEncoder

# å‡è¨­æ‚¨çš„ query.py åœ¨åŒä¸€ç›®éŒ„ä¸‹
from query import build_chain, extract_clean_query
import backend.main_lamma as app_main

# =================è¨­å®šå€=================
EVAL_QA_PATH = Path("eval/test2.jsonl")  # è«‹ç¢ºèªè·¯å¾‘æ­£ç¢º
run_config = RunConfig(
    timeout=600,    # å–®é¡Œè©•åˆ†è¶…æ™‚è¨­å®š
    max_workers=1,  # ä½µç™¼æ•¸
    max_retries=3,
)

# ä¿®æ”¹æ­£å‰‡è¡¨é”å¼ä»¥æ•æ‰å…§å®¹ (Group 1)
THINK_PATTERN = re.compile(r"<think>(.*?)</think>", flags=re.DOTALL)
# =======================================

def parse_model_output(text: str):
    """
    è§£ææ¨¡å‹è¼¸å‡ºï¼Œåˆ†é›¢æ€è€ƒéç¨‹èˆ‡æœ€çµ‚å›ç­”ã€‚
    å›å‚³: (thinking_process, clean_answer)
    """
    if not isinstance(text, str):
        return "", text
    
    match = THINK_PATTERN.search(text)
    thinking_process = ""
    clean_answer = text

    if match:
        thinking_process = match.group(1).strip()
        # å°‡ <think>...</think> æ•´æ®µç§»é™¤ï¼Œåªç•™å‰©ä¸‹çš„å›ç­”
        clean_answer = THINK_PATTERN.sub("", text).strip()
    else:
        # å¦‚æœæ²’æœ‰ <think> æ¨™ç±¤ï¼Œå‡è¨­æ•´æ®µéƒ½æ˜¯å›ç­”ï¼Œæ€è€ƒéç¨‹ç‚ºç©º
        clean_answer = text.strip()
        
    return thinking_process, clean_answer

def load_eval_qa(path: Path):
    """è®€å– jsonl æ ¼å¼çš„æ¸¬è©¦é¡Œåº«"""
    items = []
    if not path.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ¸¬è©¦æª”æ¡ˆ: {path}")
        
    with path.open("r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                items.append(json.loads(line))
            except json.JSONDecodeError:
                print(f"è·³éç„¡æ•ˆçš„ JSON è¡Œ: {line[:50]}...")
    return items

def get_now_and_term():
    """å–å¾—ç¾åœ¨æ™‚é–“èˆ‡å­¸æœŸè³‡è¨Š"""
    now = datetime.now(ZoneInfo("Asia/Taipei"))
    roc_year = now.year - 1911
    m, d = now.month, now.day

    if (m, d) >= (8, 1):
        acad_year = roc_year
        sem = "ç¬¬ä¸€å­¸æœŸ"
    elif (m, d) >= (2, 1):
        acad_year = roc_year - 1
        sem = "ç¬¬äºŒå­¸æœŸ"
    else:
        acad_year = roc_year - 1
        sem = "ç¬¬ä¸€å­¸æœŸ"

    acad_term = f"{acad_year}å­¸å¹´{sem}"
    now_str = f"æ°‘åœ‹{roc_year}å¹´{m}æœˆ{d}æ—¥ {now.strftime('%H:%M')}"
    return now_str, acad_term

def run_rag_for_eval(chains, question: str):
    """
    ä½¿ç”¨ main.py çš„ ask å‡½å¼åŸ·è¡Œ RAGã€‚
    æ³¨æ„ï¼šmain.py çš„ ask æœƒè‡ªå‹•è™•ç† rewrite å’Œ remove_thinking_tags
    """
    try:
        # ç›´æ¥å‘¼å« API çš„æ ¸å¿ƒé‚è¼¯
        # æ³¨æ„ï¼šmain.ask å…§éƒ¨æœƒè‡ªå·±è¨ˆç®— now å’Œ acad_termï¼Œæ‰€ä»¥ä¸ç”¨å‚³é€²å»
        res = app_main.ask(chains, question)
        
        # å–å¾—ä¹¾æ·¨çš„å›ç­” (main.py å·²ç¶“æŠŠ <think> ç§»é™¤äº†)
        answer = res.get("answer", "")
        
        # å–å¾—ä¸Šä¸‹æ–‡
        context_docs = res.get("context", [])
        contexts_text = [d.page_content for d in context_docs]
        
        # å–å¾—å…¶ä»–è³‡è¨Š
        rewritten_q = res.get("rewritten_query", "")
        

        # ğŸ”¥ ç›´æ¥å¾ result æ‹¿æ€è€ƒéç¨‹ (å¦‚æœ main.py æ”¹å¥½äº†)
        thinking_process = res.get("thinking_process", "")
        raw_answer = answer 

    except Exception as e:
        print(f"RAG Chain error: {e}")
        answer = f"Error: {str(e)}"
        raw_answer = str(e)
        thinking_process = ""
        contexts_text = []
        context_docs = []
        rewritten_q = ""

    return {
        "original_question": question,
        "rewritten_question": rewritten_q,
        "answer": answer,                 
        "thinking_process": thinking_process, 
        "raw_answer": raw_answer,         
        "contexts": contexts_text,
        "raw_context_docs": context_docs,
    }

def main():
    # 1. è¼‰å…¥é¡Œç›®
    eval_items = load_eval_qa(EVAL_QA_PATH)
    total_items = len(eval_items)
    print(f"è¼‰å…¥ {total_items} é¡Œæ¸¬è©¦é¡Œç›®")

    # ==========================================
    # ğŸ”¥ é—œéµä¿®æ”¹ï¼šæ‰‹å‹•åˆå§‹åŒ– main.py çš„å…¨åŸŸè®Šæ•¸
    # ==========================================
    print("ğŸ”„ [Eval] æ‰‹å‹•åˆå§‹åŒ– Reranker (æ¨¡æ“¬ API å•Ÿå‹•)...")
    # å¼·åˆ¶è¨­å®šç‚º GPUï¼Œç¢ºä¿è·Ÿ API ç’°å¢ƒä¸€è‡´
    app_main.reranker = CrossEncoder(app_main.RERANK_MODEL_NAME, device="cuda")
    
    print("ğŸ”„ [Eval] å»ºç«‹ RAG chains...")
    chains = app_main.build_chain()
    print("âœ… ç³»çµ±åˆå§‹åŒ–å®Œæˆï¼")
    # ==========================================

    rows = []
    print("=== é–‹å§‹åŸ·è¡Œ RAG ç”Ÿæˆ ===")
    
    # 3. è·‘è¿´åœˆç”Ÿæˆç­”æ¡ˆ
    for i, item in enumerate(eval_items):
        q = item["question"]
        gt = item["expected_answer"]

        # åŸ·è¡Œ RAG
        out = run_rag_for_eval(chains, q)

        rows.append({
            "question": q,
            "answer": out["answer"],
            "thinking_process": out["thinking_process"], 
            "contexts": out["contexts"],
            "ground_truth": gt,
            "original_question": out["original_question"],
            "rewritten_question": out["rewritten_question"],
            "raw_answer": out["raw_answer"], 
            "category": item.get("category", ""),
        })

        # é€²åº¦æ¢
        current_count = i + 1
        if current_count % 5 == 0 or current_count == total_items:
            print(f"[{datetime.now().strftime('%H:%M:%S')}] â³ é€²åº¦ï¼š{current_count} / {total_items}")

    # 4. æº–å‚™ RAGAS è©•æ¸¬è³‡æ–™é›†
    ds = Dataset.from_list([
        {
            "question": r["question"],
            "answer": r["answer"], # è©•åˆ†æ™‚åªçµ¦ä¹¾æ·¨çš„å›ç­”
            "contexts": r["contexts"],
            "ground_truth": r["ground_truth"],
        }
        for r in rows
    ])

    # 5. è¨­å®šè©•å¯©æ¨¡å‹
    judge_llm = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0,
    )

    # 6. è¨­å®šè©•å¯© Embeddings
    ragas_embeddings = HuggingFaceEmbeddings(
        model_name="BAAI/bge-m3",
        model_kwargs={"device": "cuda"},
        encode_kwargs={"normalize_embeddings": True},
    )

    print("=== é–‹å§‹é€²è¡Œ Ragas è©•åˆ† (é€™éœ€è¦ä¸€é»æ™‚é–“) ===")
    
    metric_cols = ["faithfulness", "answer_relevancy", "context_recall", "context_precision", "answer_correctness"]
    
    result = evaluate(
        dataset=ds,
        metrics=[faithfulness, answer_relevancy, context_recall, context_precision, answer_correctness],
        llm=judge_llm,
        embeddings=ragas_embeddings,
        run_config=run_config,
    )

    # 7. è™•ç†çµæœèˆ‡æª”æ¡ˆè·¯å¾‘å»ºç«‹
    df = result.to_pandas()
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    
    # å»ºç«‹è³‡æ–™å¤¾çµæ§‹ï¼š eval_result/rag_eval_{timestamp}/
    base_dir = Path("eval_result")
    output_dir = base_dir / f"rag_eval_{timestamp}"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\n[ç³»çµ±] è¼¸å‡ºç›®éŒ„å·²å»ºç«‹: {output_dir}")

    # åˆä½µåˆ†æ•¸
    final_data = []
    for i, row in df.iterrows():
        scores = {m: row.get(m, None) for m in metric_cols}
        item_data = rows[i].copy()
        item_data.update(scores)
        final_data.append(item_data)
    
    final_df = pd.DataFrame(final_data)

    print("\n=== RAG è©•ä¼°å®Œæˆ ===")

    # --- å­˜æª” 1: CSV (Excel ç”¨) ---
    csv_filename = output_dir / f"rag_eval_result_{timestamp}.csv"
    final_df.to_csv(csv_filename, index=False, encoding="utf-8-sig")
    print(f"[å·²å„²å­˜] CSV å ±è¡¨: {csv_filename}")

    # --- å­˜æª” 2: JSON (è©³ç´°è³‡æ–™ï¼Œå·²åˆ†é›¢ thinking_process) ---
    json_filename = output_dir / f"rag_eval_detail_{timestamp}.json"
    with open(json_filename, "w", encoding="utf-8") as f:
        json.dump(final_data, f, ensure_ascii=False, indent=2)
    print(f"[å·²å„²å­˜] JSON è©³ç´°è³‡æ–™: {json_filename}")

    # å…±é€šé–€æª»è¨­å®š
    threshold = 0.6 

    # --- å­˜æª” 3: TXT (ä½åˆ†æª¢è¨å ±å‘Š) ---
    bad_case_filename = output_dir / f"rag_bad_cases_report_{timestamp}.txt"
    
    with open(bad_case_filename, "w", encoding="utf-8") as f:
        f.write(f"=== RAG è©•æ¸¬æª¢è¨å ±å‘Š ({timestamp}) ===\n\n")
        
        f.write("=== å¹³å‡åˆ†æ•¸ ===\n")
        for m in metric_cols:
            if m in final_df.columns:
                mean_score = final_df[m].mean()
                f.write(f"{m}: {mean_score:.3f}\n")
                print(f"{m}: {mean_score:.3f}")
        
        f.write("\n" + "="*50 + "\n")
        
        # ç¯©é¸ä»»ä¸€æŒ‡æ¨™ä½æ–¼é–€æª»çš„
        bad_cases = final_df[
            (final_df['answer_relevancy'] < threshold) | 
            (final_df['faithfulness'] < threshold) |
            (final_df['answer_correctness'] < threshold) |
            (final_df['context_recall'] < threshold) 
        ]

        if not bad_cases.empty:
            f.write(f"\n=== ç•°å¸¸æ¡ˆä¾‹åˆ†æ (ä»»ä¸€æŒ‡æ¨™ < {threshold}) ===\n")
            for idx, row in bad_cases.iterrows():
                f.write(f"\n[ç´¢å¼• Q{idx}]\n")
                f.write(f"å•é¡Œ: {row.get('question', '')}\n")
                # ğŸ”¥ ä¿®æ”¹é€™ä¸€è¡Œï¼Œè£œä¸Š Precision
                f.write(f"åˆ†æ•¸: Faith={row.get('faithfulness', 0):.2f} | Relevancy={row.get('answer_relevancy', 0):.2f} | Recall={row.get('context_recall', 0):.2f} | Precision={row.get('context_precision', 0):.2f} | Correctness={row.get('answer_correctness', 0):.2f}\n")
                f.write(f"æ¨¡å‹å›ç­”: {row.get('answer', '')}\n")
                f.write(f"æ¨™æº–ç­”æ¡ˆ: {row.get('ground_truth', '')}\n")
                f.write("-" * 30 + "\n")
        else:
            f.write("\næ­å–œï¼æ²’æœ‰ç™¼ç¾åˆ†æ•¸éä½çš„ç•°å¸¸æ¡ˆä¾‹ã€‚\n")

    print(f"[å·²å„²å­˜] ä½åˆ†æª¢è¨å ±å‘Š: {bad_case_filename}")

    # --- å­˜æª” 4: TXT (å„ªè‰¯æ¡ˆä¾‹å ±å‘Š - æ‰€æœ‰æŒ‡æ¨™çš†é”æ¨™) ---
    high_quality_filename = output_dir / f"rag_high_quality_report_{timestamp}.txt"
    
    with open(high_quality_filename, "w", encoding="utf-8") as f:
        f.write(f"=== RAG å„ªè‰¯æ¡ˆä¾‹å ±å‘Š ({timestamp}) ===\n")
        f.write(f"ç¯©é¸æ¨™æº–: Faithfulness, Relevancy, Recall, Correctness çš† >= {threshold}\n\n")
        
        # ğŸ”¥ ä¿®æ”¹é‚è¼¯ï¼šåŒæ™‚æ»¿è¶³å››å€‹æŒ‡æ¨™ >= 0.6
        good_cases = final_df[
            (final_df['context_recall'].fillna(0) >= threshold) &
            (final_df['faithfulness'].fillna(0) >= threshold) &
            (final_df['answer_relevancy'].fillna(0) >= threshold) &
            (final_df['answer_correctness'].fillna(0) >= threshold)
        ]

        if not good_cases.empty:
            f.write(f"å…±ç™¼ç¾ {len(good_cases)} é¡Œå…¨æŒ‡æ¨™åˆæ ¼æ¡ˆä¾‹ã€‚\n")
            for idx, row in good_cases.iterrows():
                f.write(f"\n[ç´¢å¼• Q{idx}]\n")
                f.write(f"å•é¡Œ: {row.get('question', '')}\n")
                # ğŸ”¥ ä¿®æ”¹é€™ä¸€è¡Œï¼Œè£œä¸Š Precision
                f.write(f"åˆ†æ•¸: Faith={row.get('faithfulness', 0):.2f} | Relevancy={row.get('answer_relevancy', 0):.2f} | Recall={row.get('context_recall', 0):.2f} | Precision={row.get('context_precision', 0):.2f} | Correctness={row.get('answer_correctness', 0):.2f}\n")
                f.write(f"æ¨¡å‹å›ç­”: {row.get('answer', '')}\n")
                f.write(f"æ¨™æº–ç­”æ¡ˆ: {row.get('ground_truth', '')}\n")
                f.write("-" * 30 + "\n")
        else:
            f.write(f"\nç„¡é¡Œç›®åŒæ™‚æ»¿è¶³å››é …æŒ‡æ¨™ >= {threshold}ã€‚\n")

    print(f"[å·²å„²å­˜] å„ªè‰¯æ¡ˆä¾‹å ±å‘Š: {high_quality_filename}")

if __name__ == "__main__":
    main()