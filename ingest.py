# ingest.py
import os
import math
import json
import re
import hashlib
from pathlib import Path
from typing import List, Dict, Any

# å¼·åˆ¶ä½¿ç”¨ safetensors æ ¼å¼ä»¥é¿å… PyTorch 2.5.1 çš„å®‰å…¨é™åˆ¶
os.environ["TRANSFORMERS_PREFER_SAFETENSORS"] = "1"
os.environ["SENTENCE_TRANSFORMERS_USE_SAFETENSORS"] = "1"

from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader
from langchain_community.document_loaders import CSVLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain.schema import Document

DATA_DIR = Path("data")
DB_DIR = "storage/chroma"
COLL_NAME = "campus_rag"

# =========================
# JSON schema è‡ªå‹•åµæ¸¬
# =========================
def detect_schema(obj: Any) -> str:
    """
    å›å‚³ "people" / "news" / "school" / "unknown"
    - people: æœ‰ã€Œäººç‰©ã€ã€Œé›»è©±ã€ã€Œä¿¡ç®±ã€ç­‰éµ
    - news:   æœ‰ "url","title","published_at","content"
    - school: æœ‰ã€Œåç¨±ã€ã€Œè‹±æ–‡åç¨±ã€ã€Œæ ¡è¨“ã€ç­‰éµï¼ˆä¾‹å¦‚ about_schoo.jsonï¼‰
    """
    sample = None
    if isinstance(obj, list) and obj:
        sample = obj[0]
    elif isinstance(obj, dict):
        sample = obj
    else:
        return "unknown"

    keys = set(sample.keys())
    # è€å¸«åéŒ„ (æ”¯æ´å…©ç¨®æ ¼å¼: èˆŠæ ¼å¼ç”¨ã€Œäººç‰©ã€, æ–°æ ¼å¼ç”¨ã€Œå§“åã€+ã€Œè·ç¨±ã€+ã€Œç³»æ‰€ã€)
    if {"äººç‰©", "é›»è©±", "ä¿¡ç®±"} & keys or {"å§“å", "è·ç¨±", "ä¿¡ç®±"} <= keys:
        return "people"
    # ç³»ç¶²æ–°è
    if {"url", "title", "published_at", "content"} <= keys:
        return "news"
    # å­¸æ ¡åŸºæœ¬è³‡æ–™ï¼ˆabout_schoo.jsonï¼‰
    if {"åç¨±", "è‹±æ–‡åç¨±", "æ ¡è¨“"} <= keys:
        return "school"
    if "é¡åˆ¥" in keys and ("å…§å®¹" in keys or "èªªæ˜" in keys):
        return "academic_rules"
    if ("è¾¦ç†é …ç›®" in keys and "æ‰¿è¾¦äºº" in keys) or ("å­¸ç³»" in keys and "è¯çµ¡äººå“¡" in keys):
        return "contacts"
    if {"å­¸å¹´å­¸æœŸ", "èª²è™Ÿ", "èª²ç¨‹åç¨±", "æ•™å¸«"} <= keys:
        return "course_history"
    if {"é¸åˆ¥", "å­¸å¹´å­¸æœŸ", "æ‰€å±¬å¹´ç´š", "èª²ç¨‹åç¨±"} <= keys and not ({"èª²è™Ÿ", "æ•™å¸«"} & keys):
        return "course_overview"
    return "unknown"

# =========================
# course_overview.jsonï¼ˆèª²ç¨‹ç¸½è¦½ï¼‰ adapter
# =========================

def course_overview_to_documents(
    data: List[Dict[str, Any]], source_path: str
) -> List[Document]:
    docs: List[Document] = []

    def parse_year_term(s: str) -> tuple[int | None, str]:
        """ç°¡å–®æŠŠ '113ä¸Š' æ‹†æˆ (113, 'ä¸Š')ï¼Œå¤±æ•—å°± (None, '')ã€‚"""
        s = (s or "").strip()
        if not s:
            return None, ""
        year = None
        for i, ch in enumerate(s):
            if not ch.isdigit():
                try:
                    year = int(s[:i])
                except Exception:
                    year = None
                term = s[i:]
                return year, term
        try:
            return int(s), ""
        except Exception:
            return None, ""

    for i, rec in enumerate(data, 1):
        year_term_raw = str(rec.get("å­¸å¹´å­¸æœŸ", "")).strip()
        year, term = parse_year_term(year_term_raw)

        select_type = str(rec.get("é¸åˆ¥", "")).strip()   # å¿…ä¿® / é¸ä¿®
        grade = str(rec.get("æ‰€å±¬å¹´ç´š", "")).strip()     # ä¸€å¹´ç´š / äºŒå¹´ç´š / ä¸‰å¹´ç´š / å››å¹´ç´š
        data_source = str(rec.get("è³‡æ–™ä¾†æº", "")).strip()  # URL

        names = rec.get("èª²ç¨‹åç¨±") or []
        if not isinstance(names, list):
            names = [names]
        # æ¸…æ‰ç©ºå­—ä¸²ï¼Œå…¨éƒ¨è½‰æˆ str
        names = [str(n).strip() for n in names if str(n).strip()]
        course_count = len(names)
        courses_str = "ã€".join(names)

        # çµ¦ LLM çš„æ–‡å­—å…§å®¹
        lines = [
            f"å­¸å¹´å­¸æœŸ:{year_term_raw}",
            f"æ‰€å±¬å¹´ç´šï¼š{grade}",
            f"é¸åˆ¥ï¼š{select_type}",
            "èª²ç¨‹åç¨±åˆ—è¡¨ï¼š",
        ]
        lines.extend([f"- {n}" for n in names])
        if data_source:
            lines.append(f"è³‡æ–™ä¾†æºï¼š{data_source}")
        text = "\n".join(lines)

        meta = {
            "source": source_path,
            "file_type": "json",
            "type": "course_overview",        # çµ¦çµ±è¨ˆ/é™¤éŒ¯ç”¨
            "content_type": "course_overview",# ä¹‹å¾Œ filter ç”¨é€™å€‹

            "year_term": year_term_raw,
            "year": year,                     # int æˆ– None
            "term": term,                     # "ä¸Š" / "ä¸‹" / ""
            "grade": grade,                   # ä¸€å¹´ç´š / äºŒå¹´ç´š / ä¸‰å¹´ç´š / å››å¹´ç´š

            "select_type": select_type,       # å¿…ä¿® / é¸ä¿®
            "course_count": course_count,     # int
            "courses": courses_str,           # âœ… å­—ä¸²ï¼Œä¸æ˜¯ list
            "data_source": data_source,       # è³‡æ–™ä¾†æº URL

            "idx": i,
            "needs_split": False,             # ä¸å†åˆ‡å¡Š
        }

        docs.append(Document(page_content=text, metadata=meta))

    return docs

# =========================
# course_history.jsonï¼ˆæ­·å¹´èª²ç¨‹è³‡æ–™ï¼‰ adapter
# =========================

def course_records_to_documents(
    data: List[Dict[str, Any]], source_path: str
) -> List[Document]:
    docs: List[Document] = []

    def parse_year_term(s: str) -> tuple[int | None, str]:
        # e.g. "113ä¸Š" / "113ä¸‹"
        s = (s or "").strip()
        if not s:
            return None, ""
        year = None
        for i, ch in enumerate(s):
            if not ch.isdigit():
                try:
                    year = int(s[:i])
                except Exception:
                    year = None
                term = s[i:]
                return year, term
        try:
            return int(s), ""
        except Exception:
            return None, ""

    for i, rec in enumerate(data, 1):
        yt = rec.get("å­¸å¹´å­¸æœŸ", "")
        year, term = parse_year_term(yt)

        code = rec.get("èª²è™Ÿ", "") or ""
        name = rec.get("èª²ç¨‹åç¨±", "") or ""
        teacher = rec.get("æ•™å¸«", "") or ""
        category = rec.get("é¸åˆ¥", "") or ""
        dept = rec.get("æ‰€å±¬ç³»æ‰€", "") or ""
        grade = rec.get("æ‰€å±¬å¹´ç´š", "") or ""
        data_source = rec.get("è³‡æ–™ä¾†æº", "") or ""

        credit_raw = rec.get("å­¸åˆ†", None)
        try:
            credits = float(credit_raw) if credit_raw not in (None, "", " ") else None
        except Exception:
            credits = None

        # çµ¦ LLM çš„æ–‡å­—
        lines = [
            f"å­¸å¹´å­¸æœŸï¼š{yt}",
            f"æ‰€å±¬å¹´ç´šï¼š{grade}",
            f"èª²è™Ÿï¼š{code}",
            f"èª²ç¨‹åç¨±ï¼š{name}",
            f"æ•™å¸«ï¼š{teacher}",
            f"é¸åˆ¥ï¼š{category}",
            f"å­¸åˆ†ï¼š{credits if credits is not None else ''}",
            f"æ‰€å±¬ç³»æ‰€ï¼š{dept}",
        ]
        if data_source:
            lines.append(f"è³‡æ–™ä¾†æºï¼š{data_source}")
        text = "\n".join(lines)

        meta = {
            "source": source_path,
            "file_type": "json",
            "type": "course_history",      # çµ±è¨ˆç”¨
            "content_type": "course",      # ä¹‹å¾Œ filter ç”¨é€™å€‹

            "year_term": yt,
            "year": year,                  # int or None
            "term": term,                  # "ä¸Š" / "ä¸‹" / ""
            "grade": grade,                # ä¸€å¹´ç´š / äºŒå¹´ç´š / ä¸‰å¹´ç´š / å››å¹´ç´š

            "course_code": code,
            "course_name": name,
            "teacher": teacher,
            "category": category,          # "å¿…ä¿®" / "é¸ä¿®"
            "required": (category == "å¿…ä¿®"),
            "credits": credits,            # float or None
            "department": dept,
            "data_source": data_source,    # è³‡æ–™ä¾†æº URL

            "idx": i,
            "needs_split": False,
        }

        docs.append(Document(page_content=text, metadata=meta))

    return docs

# =========================
# contact.jsonï¼ˆè¯çµ¡è³‡è¨Šï¼‰ adapter
# =========================
def contact_records_to_documents(
    data: List[Dict[str, Any]], source_path: str
) -> List[Document]:
    docs: List[Document] = []

    for i, rec in enumerate(data, 1):
        data_source = rec.get("è³‡æ–™ä¾†æº", "") or ""
        
        if "è¾¦ç†é …ç›®" in rec:  # è¡Œæ”¿/æ‹›ç”Ÿé¡
            role = "service"
            item = rec.get("è¾¦ç†é …ç›®", "").strip()
            person = rec.get("æ‰¿è¾¦äºº", "").strip()
            ext = rec.get("åˆ†æ©Ÿ", "").strip()

            lines = [
                f"è¾¦ç†é …ç›®ï¼š{item}",
                f"æ‰¿è¾¦äººï¼š{person}",
                f"è¯çµ¡é›»è©±ï¼š{ext}",
            ]
        elif "å­¸ç³»" in rec:   # å„å­¸ç³»è¯çµ¡äºº
            role = "department"
            dept = rec.get("å­¸ç³»", "").strip()
            person = rec.get("è¯çµ¡äººå“¡", "").strip()
            ext = rec.get("åˆ†æ©Ÿ", "").strip()

            lines = [
                f"å­¸ç³»ï¼š{dept}",
                f"è¯çµ¡äººå“¡ï¼š{person}",
                f"è¯çµ¡é›»è©±ï¼š{ext}",
            ]
        else:
            # ä¿éšªï¼šä¸ç¬¦åˆé æœŸæ¬„ä½å°± flatten ä¸€ä¸‹
            role = "unknown"
            lines = [f"{k}ï¼š{v}" for k, v in rec.items() if k != "è³‡æ–™ä¾†æº"]

        if data_source:
            lines.append(f"è³‡æ–™ä¾†æºï¼š{data_source}")
        text = "\n".join(lines)

        meta = {
            "source": source_path,
            "file_type": "json",
            "type": "contact",           # çµ¦ä½ çµ±è¨ˆç”¨
            "content_type": "contact",   # ä¹‹å¾Œ filter ç”¨é€™å€‹
            "role": role,                # "service" or "department"
            "item": rec.get("è¾¦ç†é …ç›®") or "",
            "department": rec.get("å­¸ç³»") or "",
            "person": rec.get("æ‰¿è¾¦äºº") or rec.get("è¯çµ¡äººå“¡") or "",
            "phone": rec.get("åˆ†æ©Ÿ") or "",
            "data_source": data_source,
            "idx": i,
            "needs_split": False,
        }

        docs.append(Document(page_content=text, metadata=meta))

    return docs

# =========================
# academic_requirements.jsonï¼ˆå­¸å‰‡/ç•¢æ¥­è¦å®šï¼‰ adapter
# =========================
def academic_records_to_documents(
    data: List[Dict[str, Any]], source_path: str
) -> List[Document]:
    docs: List[Document] = []

    def infer_topic(category: str) -> str:
        if "ä¿®æ¥­è¦å®š" in category:
            return "graduation"
        if "å°ˆé¡Œ" in category:
            return "capstone"
        if "è¼”ç³»" in category:
            return "minor"
        if "è½‰ç³»" in category:
            return "transfer"
        if "å¯¦ç¿’" in category:
            return "internship"
        if "å£è©¦" in category:
            return "thesis_oral"
        return "general"

    for i, rec in enumerate(data, 1):
        category = rec.get("é¡åˆ¥", "").strip()
        topic = infer_topic(category)

        lines = [f"é¡åˆ¥ï¼š{category}"]
        for k, v in rec.items():
            if k == "é¡åˆ¥":
                continue
            # çµ±ä¸€æˆã€Œæ¬„åï¼šå…§å®¹ã€çš„æ ¼å¼
            lines.append(f"{k}ï¼š{v}")
        text = "\n".join(lines)

        meta = {
            "source": source_path,
            "file_type": "json",
            "type": "academic_rules",
            "content_type": "academic_rule",
            "category": category,
            "topic": topic,   # å–®ä¸€å­—ä¸²ï¼Œæ–¹ä¾¿ filter
            "idx": i,
            "needs_split": False,
        }

        docs.append(Document(page_content=text, metadata=meta))

    return docs

# =========================
# schoolï¼ˆå­¸æ ¡è³‡è¨Šï¼‰ adapter
# =========================
def school_info_to_documents(obj: Any, source_path: str) -> List[Document]:
    """
    å°‡ about_schoo.json é€™é¡ã€Œå­¸æ ¡è³‡è¨Šã€æ•´ç†æˆä¸€ä»½ Documentï¼Œ
    ä¸¦åœ¨ metadata è£¡è£œå……å¸¸ç”¨æ¬„ä½ï¼ˆæ ¡åã€æ ¡è¨“ã€ç¶²å€ç­‰ï¼‰ã€‚
    """
    # é æœŸæ ¼å¼ï¼šlist[dict]
    if not isinstance(obj, list) or not obj:
        text = str(obj)
        meta = {
            "source": source_path,
            "file_type": "json",
            "content_type": "school",
            "needs_split": False,
        }
        return [Document(page_content=text, metadata=meta)]

    # åšå€‹å®‰å…¨çš„ helperï¼šå¾ä¸åŒ block æŠ“ key
    def find_key(key: str, default: str = "") -> Any:
        for block in obj:
            if isinstance(block, dict) and key in block:
                return block[key]
        return default

    # åŸºæœ¬è³‡è¨Š
    name = find_key("åç¨±", "")
    name_en = find_key("è‹±æ–‡åç¨±", "")
    motto = find_key("æ ¡è¨“", "")
    founded_at = find_key("æˆç«‹æ™‚é–“", "")
    founder = find_key("å‰µè¾¦äºº", "")
    school_type = find_key("é¡å‹", "")

    address = find_key("åœ°å€", "")
    phone = find_key("é›»è©±", "")
    emergency_phone = find_key("ç·Šæ€¥æ ¡å®‰å°ˆç·š", "")
    fax = find_key("å‚³çœŸ", "")
    president_phone = find_key("æ ¡é•·å®¤é›»è©±", "")
    president_fax = find_key("æ ¡é•·å®¤å‚³çœŸ", "")
    president_email = find_key("æ ¡é•·å®¤ email", "")

    school_code = find_key("å­¸æ ¡ä»£ç¢¼", "")
    url = find_key("ç¶²å€", "")
    departments = find_key("ç³»æ‰€çµæ§‹", [])
    if isinstance(departments, list):
        departments_str = "ã€".join(map(str, departments))
    else:
        departments_str = str(departments) if departments else ""

    prev_name = find_key("å‰èº«", "")
    reorg_at = find_key("æ”¹åˆ¶æ™‚é–“", "")
    rename_at = find_key("æ›´åæ™‚é–“", "")
    feature = find_key("ç‰¹è‰²", "")

    # çµ¦ LLM çœ‹çš„æ–‡å­—å…§å®¹ï¼ˆä½ å¯ä»¥ä¹‹å¾Œå†å¾®èª¿æ ¼å¼ï¼‰
    lines = [
        f"åç¨±ï¼š{name}",
        f"è‹±æ–‡åç¨±ï¼š{name_en}",
        f"æ ¡è¨“ï¼š{motto}",
        f"æˆç«‹æ™‚é–“ï¼š{founded_at}",
        f"å‰µè¾¦äººï¼š{founder}",
        f"é¡å‹ï¼š{school_type}",
        "",
        f"åœ°å€ï¼š{address}",
        f"é›»è©±ï¼š{phone}",
        f"ç·Šæ€¥æ ¡å®‰å°ˆç·šï¼š{emergency_phone}",
        f"å‚³çœŸï¼š{fax}",
        f"æ ¡é•·å®¤é›»è©±ï¼š{president_phone}",
        f"æ ¡é•·å®¤å‚³çœŸï¼š{president_fax}",
        f"æ ¡é•·å®¤ emailï¼š{president_email}",
        "",
        f"å­¸æ ¡ä»£ç¢¼ï¼š{school_code}",
        f"ç¶²å€ï¼š{url}",
        f"ç³»æ‰€çµæ§‹ï¼š{departments_str}",
        "",
        f"å‰èº«ï¼š{prev_name}",
        f"æ”¹åˆ¶æ™‚é–“ï¼š{reorg_at}",
        f"æ›´åæ™‚é–“ï¼š{rename_at}",
        f"ç‰¹è‰²ï¼š{feature}",
    ]
    text = "\n".join(lines)

    meta = {
        "source": source_path,
        "file_type": "json",
        "content_type": "school",
        "name": name,
        "name_en": name_en,
        "motto": motto,
        "founded_at": founded_at,
        "founder": founder,
        "school_type": school_type,
        "address": address,
        "phone": phone,
        "emergency_phone": emergency_phone,
        "fax": fax,
        "president_phone": president_phone,
        "president_fax": president_fax,
        "president_email": president_email,
        "school_code": school_code,
        "url": url,
        "departments": departments_str,   # âœ… å­˜æˆç´”å­—ä¸²å°±æ²’å•é¡Œ
        "prev_name": prev_name,
        "reorg_at": reorg_at,
        "rename_at": rename_at,
        "feature": feature,
        "needs_split": False,  # é€™ä»½æœ¬ä¾†å°±ä¸é•·ï¼Œä¸å†äºŒæ¬¡åˆ‡å¡Š
        "idx": 1,
    }

    return [Document(page_content=text, metadata=meta)]

# =========================
# peopleï¼ˆè€å¸«åéŒ„ï¼‰ adapter
# =========================
_name_title_pat = re.compile(
    r"^\s*(?P<name>[\u4e00-\u9fa5A-Za-z0-9ï¼ãƒ»]+)\s*(?P<title>.+)?$"
)

def _parse_name_title(s: str) -> Dict[str, str]:
    m = _name_title_pat.match(s or "")
    if not m:
        return {"name": s or "", "title": ""}

    name = (m.group("name") or "").strip().replace("\u00a0", " ")
    title = (m.group("title") or "").strip(" ,ï¼Œ").replace("\u00a0", " ")

    # æœ‰äº›ä¾†æºæœƒæŠŠã€Œè·ç¨±/è·å‹™ï¼šã€é€™æ®µä¹Ÿå¡é€²ä¾†ï¼Œé€™é‚Šé †ä¾¿æ¸…æ‰é–‹é ­çš„æ¨™ç±¤
    if title.startswith("è·ç¨±/è·å‹™"):
        title = re.sub(r"^è·ç¨±/è·å‹™[:ï¼š]?\s*", "", title)

    return {"name": name, "title": title}


def _split_meta(raw: str) -> Dict[str, str]:
    """
    æŠŠåŸæœ¬å¡åœ¨ metadata çš„å­—ä¸²æ‹†æˆä¸‰å¡Šï¼š
    - education: å­¸æ­·
    - experience: ç¶“æ­·
    - expertise: æ•™å­¸èˆ‡ç ”ç©¶é ˜åŸŸ
    é€™æ¨£å°±ä¸æœƒåœ¨ã€Œç ”ç©¶é ˜åŸŸã€è£¡å†æŠŠå­¸æ­·ã€ç¶“æ­·é‡è¤‡å°ä¸€æ¬¡ã€‚
    """
    raw = (raw or "").strip()
    if not raw:
        return {"education": "", "experience": "", "expertise": ""}

    education = ""
    experience = ""
    expertise = ""

    txt = raw

    # å…ˆåˆ‡æ‰ã€Œæ•™å­¸èˆ‡ç ”ç©¶é ˜åŸŸã€é‚£ä¸€æ®µï¼Œå‰©ä¸‹å‰é¢çµ¦å­¸æ­·/ç¶“æ­·ç”¨
    head, sep, tail = txt.partition("æ•™å­¸èˆ‡ç ”ç©¶é ˜åŸŸ")
    if sep:  # æœ‰æ‰¾åˆ°æ•™å­¸èˆ‡ç ”ç©¶é ˜åŸŸ
        txt = head.strip()
        expertise = tail.lstrip(" ï¼š:").strip()
    else:
        txt = raw

    # è™•ç†å­¸æ­· / ç¶“æ­·
    if "å­¸æ­·" in txt or "ç¶“æ­·" in txt:
        if "å­¸æ­·" in txt:
            after_degree = txt.split("å­¸æ­·", 1)[1]
            after_degree = after_degree.lstrip(" ï¼š:").strip()
        else:
            after_degree = txt

        if "ç¶“æ­·" in after_degree:
            part_deg, part_exp = after_degree.split("ç¶“æ­·", 1)
            education = part_deg.strip(" ã€‚\n\r\t")
            experience = part_exp.lstrip(" ï¼š:").strip()
        else:
            education = after_degree.strip(" ã€‚\n\r\t")
    else:
        # æ²’æœ‰ç‰¹åˆ¥æ¨™å­¸æ­·/ç¶“æ­·ï¼Œå°±å…¨éƒ¨ç•¶æˆç ”ç©¶/æ•™å­¸èªªæ˜
        if not expertise:
            expertise = raw

    return {
        "education": education,
        "experience": experience,
        "expertise": expertise,
    }


def _fmt_people_page_content(meta: Dict[str, Any]) -> str:
    lines = [
        f"å§“åï¼š{meta.get('name','')}",
        f"è·ç¨±/è·å‹™ï¼š{meta.get('title','')}",
    ]
    if meta.get("department"):
        lines.append(f"ç³»æ‰€ï¼š{meta['department']}")
    lines.extend([
        f"è¾¦å…¬å®¤ï¼š{meta.get('office','')}",
        f"åˆ†æ©Ÿ/é›»è©±ï¼š{meta.get('phone','')}",
        f"Emailï¼š{meta.get('email','')}",
    ])
    if meta.get("education"):
        lines.append(f"å­¸æ­·ï¼š{meta['education']}")
    if meta.get("experience"):
        lines.append(f"ç¶“æ­·ï¼š{meta['experience']}")
    if meta.get("expertise"):
        lines.append(f"ç ”ç©¶é ˜åŸŸï¼š{meta['expertise']}")
    if meta.get("data_source"):
        lines.append(f"è³‡æ–™ä¾†æºï¼š{meta['data_source']}")
    return "\n".join(lines)


def people_records_to_documents(
    data: List[Dict[str, Any]], source_path: str
) -> List[Document]:
    docs: List[Document] = []
    for i, rec in enumerate(data, 1):
        # æ”¯æ´å…©ç¨®æ ¼å¼:
        # 1. èˆŠæ ¼å¼: ã€Œäººç‰©ã€æ¬„ä½åŒ…å«å§“åå’Œè·ç¨±
        # 2. æ–°æ ¼å¼: ã€Œå§“åã€å’Œã€Œè·ç¨±ã€åˆ†é–‹
        if "å§“å" in rec:
            # æ–°æ ¼å¼ (department_members.json)
            who = {"name": rec.get("å§“å", "").strip(), "title": rec.get("è·ç¨±", "").strip()}
        else:
            # èˆŠæ ¼å¼
            who = _parse_name_title(rec.get("äººç‰©", ""))

        # å–å¾—ç³»æ‰€å’Œè³‡æ–™ä¾†æº
        department = rec.get("ç³»æ‰€", "") or ""
        data_source = rec.get("è³‡æ–™ä¾†æº", "") or ""

        raw_meta = rec.get("metadata") or ""
        meta_parsed = _split_meta(raw_meta)

        meta = {
            "source": source_path,
            "file_type": "json",
            "content_type": "people",
            "name": who["name"],
            "title": who["title"],
            "phone": rec.get("é›»è©±"),
            "email": rec.get("ä¿¡ç®±"),
            "office": rec.get("è¾¦å…¬å®¤"),
            "department": department,
            "data_source": data_source,
            "education": meta_parsed["education"],
            "experience": meta_parsed["experience"],
            "expertise": meta_parsed["expertise"],
            "idx": i,
            "needs_split": False,
        }
        docs.append(
            Document(
                page_content=_fmt_people_page_content(meta),
                metadata=meta,
            )
        )
    return docs

# =========================
# newsï¼ˆç³»ç¶²æ–°èï¼‰ adapter
# =========================
def _fmt_news_page_content(meta: Dict[str, Any], content: str) -> str:
    return "\n".join([
        f"æ¨™é¡Œï¼š{meta.get('title','')}",
        f"æ—¥æœŸï¼š{meta.get('published_at','')}",
        "å…§æ–‡ï¼š",
        content or "",
    ])

def news_records_to_documents(data: List[Dict[str, Any]], source_path: str) -> List[Document]:
    docs: List[Document] = []

    # ç›®æ¨™ï¼šæ§åˆ¶æ¯å¡Šé•·åº¦ï¼Œé¿å…åµŒå…¥æ¨¡å‹æˆªæ–·ï¼ˆä¸­æ–‡å­—æ•¸â‰ˆtoken æ•¸é‡çš„å¥½è¿‘ä¼¼ï¼‰
    TARGET_CHARS = 1000      # å¤§è‡´å°æ‡‰ 256â€“384 tokens
    OVERLAP_CHARS = 80

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=TARGET_CHARS,
        chunk_overlap=OVERLAP_CHARS,
        # å¼·åŒ–ä¸­æ–‡æ¨™é»åˆ†å‰²ï¼›æœ€å¾Œå†ç”¨ç©ºç™½ã€è‹±æ–‡æ¨™é»è£œåˆ€
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ã€", "ï¼š", "â€”â€”", " ", ",", ".", "ï¼Œ", ":"]
    )

    from datetime import datetime
    def to_ts(s: str | None) -> int | None:
        if not s: return None
        for fmt in ("%Y-%m-%d", "%Y/%m/%d", "%Y-%m-%d %H:%M", "%Y/%m/%d %H:%M"):
            try:
                return int(datetime.strptime(s, fmt).timestamp())
            except Exception:
                pass
        return None

    for i, rec in enumerate(data, 1):
        title = rec.get("title") or ""
        content = rec.get("content") or ""
        published_at = rec.get("published_at")
        published_ts = to_ts(published_at)
        url = rec.get("url")

        # ç‚ºæ¯ç¯‡æ–°èç”¢ç”Ÿç©©å®š article_idï¼ˆåˆ©æ–¼é‡çµ„èˆ‡å»é‡ï¼‰
        article_key = f"{source_path}|{url or title}|{published_at or ''}|{i}"
        article_id = hashlib.sha1(article_key.encode("utf-8")).hexdigest()[:12]

        base_meta = {
            "source": source_path,
            "file_type": "json",
            "content_type": "news",
            "url": url,
            "title": title,
            "published_at": published_at,
            "published_at_ts": published_ts,  # ä¹‹å¾Œå¥½ç”¨æ–¼æ’åº/éæ¿¾
            "idx": i,
            "article_id": article_id,
            # æˆ‘å€‘æœƒåœ¨æ­¤å‡½å¼å®Œæˆåˆ‡å¡Šï¼Œé¿å…ä¸»æµç¨‹å†æ¬¡åˆ‡
            "needs_split": False,
        }

        # çŸ­æ–‡ç›´æ¥ä¸€å¡Šï¼ˆé¿å…ä¸å¿…è¦åˆ‡å‰²ï¼‰
        if len(content) <= TARGET_CHARS:
            docs.append(Document(
                page_content=_fmt_news_page_content(base_meta, content),
                metadata=base_meta
            ))
            continue

        # é•·æ–‡ï¼šåªå°ã€Œå…§æ–‡ã€åšåˆ‡å¡Šï¼Œå†æŠŠæ¨™é¡Œ/æ—¥æœŸç•¶å‰ç¶´è£œå›æ¯å¡Š
        parts = splitter.split_text(content)

        # è‹¥æœ€å¾Œä¸€å¡Šå¤ªçŸ­ï¼Œä½µå›å‰ä¸€å¡Šï¼Œé¿å…ç”¢ç”Ÿã€Œç¢å°¾ã€
        if len(parts) >= 2 and len(parts[-1]) < TARGET_CHARS // 3:
            parts[-2] = parts[-2] + ("\n" if not parts[-2].endswith("\n") else "") + parts[-1]
            parts.pop()

        for j, part in enumerate(parts):
            meta = dict(base_meta)
            meta.update({"chunk": j})
            docs.append(Document(
                page_content=_fmt_news_page_content(meta, part),
                metadata=meta
            ))

    return docs



# =========================
# JSON è¼‰å…¥èˆ‡åˆ†æ´¾
# =========================
def load_json_as_documents(path: Path) -> List[Document]:
    with open(path, "r", encoding="utf-8") as f:
        obj = json.load(f)

    schema = detect_schema(obj)
    if schema == "people":
        data = obj if isinstance(obj, list) else [obj]
        return people_records_to_documents(data, str(path))
    elif schema == "news":
        data = obj if isinstance(obj, list) else [obj]
        return news_records_to_documents(data, str(path))
    elif schema == "school":
        # ç›´æ¥äº¤çµ¦ school adapter è™•ç†ï¼ˆä¸å† flattenï¼‰
        return school_info_to_documents(obj, str(path))
    elif schema == "academic_rules":
        data = obj if isinstance(obj, list) else [obj]
        return academic_records_to_documents(data, str(path))
    elif schema == "contacts":
        data = obj if isinstance(obj, list) else [obj]
        return contact_records_to_documents(data, str(path))
    elif schema == "course_history":
        data = obj if isinstance(obj, list) else [obj]
        return course_records_to_documents(data, str(path))
    elif schema == "course_overview":
        data = obj if isinstance(obj, list) else [obj]
        return course_overview_to_documents(data, str(path))
    else:
        # å¾Œå‚™ï¼šä¸èªå¾—çš„ JSON â†’ æ‰å¹³åŒ–æˆä¸€ä»½ Documentï¼ˆä»ä¿ç•™ metadataï¼‰
        def flatten(o):
            if isinstance(o, dict):
                for k, v in o.items():
                    yield str(k); yield from flatten(v)
            elif isinstance(o, list):
                for it in o:
                    yield from flatten(it)
            else:
                yield str(o)
        text = "\n".join(x for x in flatten(obj) if x)
        return [Document(page_content=text, metadata={"source": str(path), "type": "unknown", "needs_split": True})]


# =========================
# å…¶ä»–æ ¼å¼è¼‰å…¥
# =========================
def load_documents(data_dir: Path) -> List[Document]:
    docs: List[Document] = []
    for path in data_dir.rglob("*"):
        suf = path.suffix.lower()
        if suf == ".pdf":
            loader = PyPDFLoader(str(path))
            # PDF loader å·²ç¶“ä¸€é ä¸€ä»½ï¼Œå¾Œé¢ä»å»ºè­°åˆ‡ä¸€ä¸‹é•·é ï¼ˆäº¤ç”±ä¸»æµç¨‹ï¼‰
            for i, d in enumerate(loader.load(), 1):
                d.metadata.update({"source": str(path), "type": "pdf", "needs_split": True, "idx": i})
                docs.append(d)
        elif suf == ".docx":
            loader = Docx2txtLoader(str(path))
            for i, d in enumerate(loader.load(), 1):
                d.metadata.update({"source": str(path), "type": "docx", "needs_split": True, "idx": i})
                docs.append(d)
        elif suf == ".txt":
            with open(path, "r", encoding="utf-8") as f:
                text = f.read()
            docs.append(Document(page_content=text, metadata={"source": str(path), "type": "txt", "needs_split": True,"idx": 1}))
        elif suf == ".csv":
            loader = CSVLoader(file_path=str(path), encoding="utf-8")
            for i, d in enumerate(loader.load(), 1):
                d.metadata.update({"source": str(path), "type": "csv", "needs_split": True, "idx": i})
                docs.append(d)
        elif suf in {".json", ".jsonl"}:
            # ä½ çš„å…©å€‹ JSONï¼ˆdepartment_members.json, ttu_cse_news.sorted.jsonï¼‰æœƒèµ°é€™æ¢
            docs.extend(load_json_as_documents(path))
        else:
            # å¿½ç•¥å…¶ä»–æ ¼å¼
            continue
    return docs


# =========================
# ä¸»æµç¨‹ï¼ˆå»ºç«‹/æ›´æ–°ç´¢å¼•ï¼‰
# =========================
def main():
    assert DATA_DIR.exists(), "è«‹å…ˆæŠŠæª”æ¡ˆæ”¾é€² data/ ç›®éŒ„ï¼ˆæ”¯æ´ JSON/PDF/DOCX/CSV/TXTï¼‰"

    print("â–¶ è®€å–æª”æ¡ˆâ€¦")
    docs = load_documents(DATA_DIR)
    print(f"â–¶ è®€åˆ° {len(docs)} ä»½åŸå§‹æ–‡ä»¶/ç‰‡æ®µï¼ˆå«å·²åˆ‡å¥½çš„ JSON æ–‡ä»¶ï¼‰")

    # å°‡éœ€è¦å†åˆ‡å¡Šçš„æ–‡ä»¶æŒ‘å‡ºä¾†åˆ‡ï¼Œå…¶ä»–ç›´æ¥ä¿ç•™
    need_split: List[Document] = [d for d in docs if d.metadata.get("needs_split", False)]
    keep: List[Document] = [d for d in docs if not d.metadata.get("needs_split", False)]

    if need_split:
        print(f"â–¶ å° {len(need_split)} ä»½æ–‡ä»¶é€²ä¸€æ­¥åˆ‡å¡Šâ€¦")
        splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=200)
        
        split_more: List[Document] = []
        for doc in need_split:
            parts = splitter.split_documents([doc])  # å°å–®ä¸€æ–‡ä»¶åˆ‡ï¼Œä¿æŒåˆ†çµ„
            for j, part in enumerate(parts):
                part.metadata["needs_split"] = False
                part.metadata["chunk"] = j          # æ¯ä»½åŸå§‹æ–‡ä»¶çš„å¡Šåº
            split_more.extend(parts)

        final_docs = keep + split_more
        print(f"â–¶ ç”¢ç”Ÿ {len(split_more)} å€‹åˆ‡å¡Šï¼›åˆè¨ˆ {len(final_docs)} ä»½å¯å…¥åº«æ–‡ä»¶")
    else:
        final_docs = keep
        print(f"â–¶ ç„¡éœ€é¡å¤–åˆ‡å¡Šï¼›åˆè¨ˆ {len(final_docs)} ä»½å¯å…¥åº«æ–‡ä»¶")

    # çµ±è¨ˆå„ type æ–¹ä¾¿ä½ æª¢æŸ¥
    from collections import Counter
    ctype_count = Counter(
        d.metadata.get("content_type", d.metadata.get("type", "unknown"))
        for d in final_docs
    )
    print("â–¶ é¡å‹çµ±è¨ˆï¼š", dict(ctype_count))

    print("â–¶ æº–å‚™åµŒå…¥æ¨¡å‹(å¤šèª)â€¦")
    embeddings = HuggingFaceEmbeddings(
        model_name="BAAI/bge-m3",
        # model_kwargs={"device": "cpu"},
        model_kwargs={"device": "cuda"},
        encode_kwargs={"normalize_embeddings": True},  # ğŸ‘ˆ è·Ÿ query.py ä¸€æ¨£
    )

    print("â–¶ å»ºç«‹/æ›´æ–° Chroma å‘é‡åº«â€¦")
    vectordb = Chroma(
        collection_name=COLL_NAME,
        embedding_function=embeddings,
        persist_directory=DB_DIR,
        collection_metadata={"hnsw:space": "cosine"},   # è·é›¢åº¦é‡ï¼šcosine / l2 / ip
    )

    # ç©©å®š IDï¼šé¿å…é‡è¤‡å¯«å…¥ï¼›åŒå…§å®¹+åŒä¾†æºæœƒå¾—åˆ°ç›¸åŒ id
    def stable_id(doc: Document) -> str:
        src = str(doc.metadata.get("source", ""))
        typ = str(doc.metadata.get("type", ""))
        file_type = str(doc.metadata.get("file_type", ""))
        content_type = str(doc.metadata.get("content_type", doc.metadata.get("type", "")))  # èˆ‡èˆŠ 'type' ç›¸å®¹
        aid = str(doc.metadata.get("article_id", ""))  # people/å…¶ä»–å‹åˆ¥æ²’æœ‰å°±ç•™ç©º
        idx = str(doc.metadata.get("idx", ""))
        chk = str(doc.metadata.get("chunk", 0))
        raw = f"{src}|{file_type}|{content_type}|{aid}|{idx}|{chk}".encode("utf-8")
        return hashlib.sha1(raw).hexdigest()

    ids = [stable_id(d) for d in final_docs]
    # é™¤éŒ¯
    from collections import Counter
    dups = [k for k, v in Counter(ids).items() if v > 1]
    if dups:
        raise RuntimeError(f"stable_id é‡è¦† {len(dups)} ç­†ï¼Œä¾‹ï¼š{dups[:3]}")

    vectordb.add_documents(final_docs, ids=ids)

    print("âœ… å®Œæˆç´¢å¼•å»ºç«‹/æ›´æ–°ï¼è³‡æ–™åº«ä½ç½®ï¼š", DB_DIR)
    print(f"âœ… collection = {COLL_NAME}ï¼Œå…±æ–°å¢/å»é‡å¾Œå¯«å…¥ {len(final_docs)} ä»½æ–‡ä»¶")


if __name__ == "__main__":
    main()
