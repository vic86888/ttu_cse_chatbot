# ingest.py
import os
import json
import re
import hashlib
import sys
from pathlib import Path
from typing import List, Dict, Any

# å¼·åˆ¶ä½¿ç”¨ safetensors æ ¼å¼ä»¥é¿å… PyTorch 2.5.1 çš„å®‰å…¨é™åˆ¶
os.environ["TRANSFORMERS_PREFER_SAFETENSORS"] = "1"
os.environ["SENTENCE_TRANSFORMERS_USE_SAFETENSORS"] = "1"

from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader
from langchain_community.document_loaders import CSVLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain.schema import Document

from datetime import datetime
from zoneinfo import ZoneInfo

# ingest.py é–‹é ­
from json_rewriter import rewrite_json_record

DATA_DIR = Path("data_qwen")
DB_DIR = "storage/chroma"
COLL_NAME = "campus_rag"

# =========================
# JSON schema è‡ªå‹•åµæ¸¬
# =========================
def detect_schema(obj: Any) -> str:
    """
    å›å‚³ "people" / "news" / "school" / "unknown"
    - people: æœ‰ã€Œäººç‰©ã€ã€Œé›»è©±ã€ã€Œä¿¡ç®±ã€ç­‰éµ
    - news:   æœ‰ "url","title","published_at","content"ï¼ˆå¯é¡å¤–å« "category"ï¼‰
    - school: æœ‰ã€Œåç¨±ã€ã€Œè‹±æ–‡åç¨±ã€ã€Œæ ¡è¨“ã€ç­‰éµï¼ˆä¾‹å¦‚ about_schoo.jsonï¼‰
    """
    sample = None
    if isinstance(obj, list) and obj:
        sample = obj[0]
    elif isinstance(obj, dict):
        # æª¢æŸ¥æ˜¯å¦æ˜¯æ–°æ ¼å¼çš„æ•™è·å“¡è³‡æ–™ï¼ˆæœ‰ "ç¸½è¦½" å’Œ "æˆå“¡åˆ—è¡¨" éµï¼‰
        if "ç¸½è¦½" in obj and isinstance(obj.get("ç¸½è¦½"), dict):
            overview = obj["ç¸½è¦½"]
            if "æˆå“¡åˆ—è¡¨" in overview:
                return "people"
        # æª¢æŸ¥æ˜¯å¦æ˜¯èˆŠæ ¼å¼çš„æ•™è·å“¡è³‡æ–™ï¼ˆæœ‰ "æˆå“¡åˆ—è¡¨" éµï¼‰
        if "æˆå“¡åˆ—è¡¨" in obj and isinstance(obj["æˆå“¡åˆ—è¡¨"], list):
            return "people"
        elif isinstance(obj, dict):
            # --- âœ… æ–°æ ¼å¼ï¼šèª²ç¨‹æ­·å²ï¼ˆå·¢ç‹€ï¼š113ä¸Š/113ä¸‹ â†’ å¹´ç´š â†’ èª²ç¨‹åˆ—è¡¨ï¼‰ ---
            term_keys = [
                k for k in obj.keys()
                if re.match(r"^\d{2,3}[ä¸Šä¸‹]$", str(k).strip())
            ]
            if term_keys:
                v0 = obj.get(term_keys[0])
                if isinstance(v0, dict):
                    # ä»»ä¸€ grade block å…§å«ã€Œèª²ç¨‹åˆ—è¡¨ã€å°±è¦–ç‚ºæ–°æ ¼å¼
                    if any(isinstance(gv, dict) and "èª²ç¨‹åˆ—è¡¨" in gv for gv in v0.values()):
                        return "course_history_nested"
        sample = obj
    else:
        return "unknown"

    keys = set(sample.keys())
    # è€å¸«åéŒ„ (æ”¯æ´å…©ç¨®æ ¼å¼: èˆŠæ ¼å¼ç”¨ã€Œäººç‰©ã€, æ–°æ ¼å¼ç”¨ã€Œå§“åã€+ã€Œè·ç¨±ã€+ã€Œç³»æ‰€ã€)
    if {"äººç‰©", "é›»è©±", "ä¿¡ç®±"} & keys or {"å§“å", "è·ç¨±", "ä¿¡ç®±"} <= keys:
        return "people"
    # ç³»ç¶²æ–°è
    if {"url", "title", "published_at", "content"} <= keys:
        return "news"
    # å­¸æ ¡åŸºæœ¬è³‡æ–™ï¼ˆabout_schoo.jsonï¼‰
    if {"åç¨±", "è‹±æ–‡åç¨±", "æ ¡è¨“"} <= keys:
        return "school"
    if "é¡åˆ¥" in keys and ("å…§å®¹" in keys or "èªªæ˜" in keys):
        return "academic_rules"
    if ("è¾¦ç†é …ç›®" in keys and "æ‰¿è¾¦äºº" in keys) or ("å­¸ç³»" in keys and "è¯çµ¡äººå“¡" in keys):
        return "contacts"
    if {"å­¸å¹´å­¸æœŸ", "èª²è™Ÿ", "èª²ç¨‹åç¨±", "æ•™å¸«"} <= keys:
        return "course_history"
    if {"é¸åˆ¥", "å­¸å¹´å­¸æœŸ", "æ‰€å±¬å¹´ç´š", "èª²ç¨‹åç¨±"} <= keys and not ({"èª²è™Ÿ", "æ•™å¸«"} & keys):
        return "course_overview"
    # äººå·¥æ™ºæ…§å­¸åˆ†å­¸ç¨‹ï¼ˆæˆ–å…¶ä»–å­¸åˆ†å­¸ç¨‹ï¼‰èª²ç¨‹æ¸…å–®
    # ç‰¹è‰²éµï¼šæœ‰ã€Œè¨­ç½®å®—æ—¨/é©ç”¨å°è±¡/èª²ç¨‹ä»£ç¢¼/èª²ç¨‹åç¨±/å­¸åˆ†æ•¸ã€
    if {"è¨­ç½®å®—æ—¨", "é©ç”¨å°è±¡", "èª²ç¨‹ä»£ç¢¼", "èª²ç¨‹åç¨±", "å­¸åˆ†æ•¸"} <= keys:
        return "program_courses"
        # è¡Œäº‹æ›† / æ ¡å‹™æ—¥ç¨‹
    # ç‰¹è‰²éµï¼šæœ‰ã€Œå¹´/æœˆ/æ—¥/æ´»å‹•äº‹é …ã€ï¼ˆé€šå¸¸é‚„æœ‰ æ˜ŸæœŸã€è³‡æ–™ä¾†æºï¼‰
    if {"å¹´", "æœˆ", "æ—¥", "æ´»å‹•äº‹é …"} <= keys:
        return "calendar"
    return "unknown"

# =========================
# æ–°æ ¼å¼ course_historyï¼ˆå·¢ç‹€ï¼šå­¸æœŸâ†’å¹´ç´šâ†’èª²ç¨‹åˆ—è¡¨ï¼‰ adapter
#  - overview çš„ idx ä½¿ç”¨å…¨åŸŸéå¢ intï¼ˆé¿å… stable_id é‡è¤‡ï¼‰
#  - overview å…§å®¹å«ã€Œæ•™å¸« / é¸åˆ¥ / å­¸åˆ†ã€
#  - overview ä¾ token â‰¤ 500 å‹•æ…‹åˆ†æ‰¹ï¼Œä¸”ä¸æ‹†å–®ä¸€èª²ç¨‹æ¢ç›®
#  - æ¯å€‹ overview chunk æœ€å¾Œé™„è³‡æ–™ä¾†æº URL
#  - term/grade å…ˆæ’åºï¼Œç¢ºä¿ idx ç©©å®š
# =========================

def course_history_nested_to_documents(
    obj: Dict[str, Any], source_path: str
) -> List[Document]:
    docs: List[Document] = []

    overview_global_idx = 0   # overview å…¨åŸŸ idxï¼ˆint, æª”å…§å”¯ä¸€ï¼‰
    global_course_idx = 0     # æ¯é–€èª²å…¨åŸŸ idxï¼ˆç›®å‰æœªç”¨ï¼Œå…ˆä¿ç•™ä»¥å¾Œå¯èƒ½æœƒç”¨ï¼‰

    def as_int(x, default=None):
        try:
            s = str(x).strip()
            if not s:
                return default
            return int(s)
        except Exception:
            return default

    def parse_year_term(s: str) -> tuple[int | None, str]:
        """æŠŠ '113ä¸Š' / '113ä¸‹' æ‹†æˆ (113, 'ä¸Š'/'ä¸‹')"""
        s = (s or "").strip()
        if not s:
            return None, ""
        for i, ch in enumerate(s):
            if not ch.isdigit():
                year = as_int(s[:i], None)
                term = s[i:]
                return year, term
        return as_int(s, None), ""

    def term_sort_key(t: str):
        """å­¸æœŸæ’åºï¼šå¹´å°â†’å¤§ï¼›åŒå¹´ ä¸Šâ†’ä¸‹"""
        year, term = parse_year_term(t)
        term_order = 0 if term == "ä¸Š" else 1 if term == "ä¸‹" else 9
        return (year or 0, term_order, t)

    GRADE_ORDER = {
        "ä¸€å¹´ç´š": 1, "äºŒå¹´ç´š": 2, "ä¸‰å¹´ç´š": 3, "å››å¹´ç´š": 4, "ç ”ç©¶æ‰€": 10,
    }

    def grade_sort_key(g: str):
        return (GRADE_ORDER.get(g, 99), g)

    def parse_credits(x: Any) -> float | None:
        try:
            s = str(x).strip()
            if not s or s.lower() == "nan":
                return None
            return float(s)
        except Exception:
            return None

    # ===== token è¨ˆæ•¸èˆ‡ä¸æ‹†èª²ç¨‹çš„ batching =====
    def count_chars(text: str) -> int:
        # ç›´æ¥ç”¨ Python å­—ä¸²é•·åº¦ï¼ˆä»¥ Unicode å­—å…ƒè¨ˆï¼‰
        return len(text or "")

    def batch_courses_by_chars(
        header_lines: List[str],
        course_entries: List[Dict[str, Any]],
        tail_lines: List[str],
        max_chars: int = 500,
    ) -> List[List[Dict[str, Any]]]:
        """
        course_entries æ¯ä¸€ç­†æ˜¯ä¸€é–€èª²ï¼ˆä¸å¯æ‹†ï¼‰ã€‚
        ä¾å­—å…ƒæ•¸ä¸Šé™åˆ†æ‰¹ï¼ˆå« header+tailï¼‰ï¼Œå›å‚³ã€Œèª²ç¨‹ entry çš„åˆ—è¡¨åˆ—è¡¨ã€ï¼Œ
        æ¯ä¸€å€‹ batch ä¹‹å¾Œæœƒä¸Ÿçµ¦é‡å¯«å™¨ã€‚
        """
        batches: List[List[Dict[str, Any]]] = []

        fixed_text = "\n".join(header_lines + tail_lines)
        fixed_chars = count_chars(fixed_text)

        # header + tail å·²ç¶“è¶…éä¸Šé™ï¼Œå°±å…¨éƒ¨å¡åŒä¸€æ‰¹ï¼ˆäº¤çµ¦ LLM è‡ªå·±æ§åˆ¶å­—æ•¸ï¼‰
        if fixed_chars >= max_chars:
            batches.append(course_entries)
            return batches

        cur_chars = fixed_chars
        current_batch: List[Dict[str, Any]] = []

        for entry in course_entries:
            line = f"- {entry.get('æ¦‚è¦', '')}"
            line_chars = count_chars(line)

            if current_batch and (cur_chars + line_chars) > max_chars:
                batches.append(current_batch)
                current_batch = []
                cur_chars = fixed_chars

            # å–®ä¸€èª²ç¨‹è‡ªå·±å°±è¶…é max_charsï¼šä»è¦æ”¾ï¼ˆä¸æ‹†èª²ï¼‰
            current_batch.append(entry)
            cur_chars += line_chars

        if current_batch:
            batches.append(current_batch)

        return batches

    # ===== term å…ˆæ’åºï¼ˆ113ä¸Š â†’ 113ä¸‹ï¼‰=====
    for year_term in sorted(obj.keys(), key=term_sort_key):
        grades_block = obj.get(year_term)
        if not isinstance(grades_block, dict):
            continue

        year, term = parse_year_term(str(year_term))

        # ===== grade å…ˆæ’åºï¼ˆä¸€ â†’ äºŒ â†’ ä¸‰ â†’ å››ï¼‰=====
        for grade_name in sorted(grades_block.keys(), key=grade_sort_key):
            grade_data = grades_block.get(grade_name)
            if not isinstance(grade_data, dict):
                continue

            course_list = grade_data.get("èª²ç¨‹åˆ—è¡¨", []) or []
            if not isinstance(course_list, list):
                course_list = [course_list]

            course_count = grade_data.get("èª²ç¨‹æ•¸")
            try:
                course_count = int(course_count)
            except Exception:
                course_count = len(course_list)

            # ========= é å…ˆæ•´ç† overview çš„èª²ç¨‹ entryï¼ˆä¸å¯æ‹†åŸå­ï¼‰ =========
            course_entries: List[Dict[str, Any]] = []
            data_sources_all: List[str] = []

            for c in course_list:
                if not isinstance(c, dict):
                    continue

                name     = str(c.get("èª²ç¨‹åç¨±", "")).strip()
                code     = str(c.get("èª²è™Ÿ", "")).strip()
                teacher  = str(c.get("æ•™å¸«", "")).strip()
                category = str(c.get("é¸åˆ¥", "")).strip()
                credits  = parse_credits(c.get("å­¸åˆ†"))
                ds       = str(c.get("è³‡æ–™ä¾†æº", "")).strip()

                if ds:
                    data_sources_all.append(ds)

                if not name:
                    continue

                summary = f"{name}"
                if code:
                    summary += f"({code})"
                if teacher:
                    summary += f" / {teacher}"
                if category:
                    summary += f" / {category}"
                if credits is not None:
                    summary += f" / {credits}å­¸åˆ†"

                entry: Dict[str, Any] = {
                    "èª²ç¨‹åç¨±": name,
                    "èª²è™Ÿ": code,
                    "æ•™å¸«": teacher,
                    "é¸åˆ¥": category,
                    "å­¸åˆ†": credits,
                    "è³‡æ–™ä¾†æº": ds,
                    "æ¦‚è¦": summary,
                }
                course_entries.append(entry)

            if not course_entries:
                continue

            data_source_str = "ï¼›".join(sorted(set(data_sources_all)))

            # ========== (A) overview docsï¼ˆâ‰¤500 å­—å…ƒï¼Œä¸æ‹†èª²ï¼‰ ==========
            header_lines = [
                f"å­¸å¹´å­¸æœŸï¼š{year_term}",
                f"æ‰€å±¬å¹´ç´šï¼š{grade_name}",
                f"èª²ç¨‹æ•¸ï¼š{course_count}",
                "",
                "èª²ç¨‹åå–®ï¼š",
            ]
            tail_lines: List[str] = []
            if data_source_str:
                tail_lines = ["", f"è³‡æ–™ä¾†æºï¼š{data_source_str}"]

            # ä¾å­—æ•¸æ‹†æˆå¤šå€‹ã€Œèª²ç¨‹ç¸½è¦½ chunkã€
            batches = batch_courses_by_chars(
                header_lines=header_lines,
                course_entries=course_entries,
                tail_lines=tail_lines,
                max_chars=500,
            )

            num_chunks = len(batches)

            for chunk_idx, course_batch in enumerate(batches):
                overview_global_idx += 1

                # æº–å‚™çµ¦é‡å¯«å™¨çš„ recordï¼šä¸€å€‹ chunk = æŸå­¸æœŸæŸå¹´ç´šçš„ä¸€éƒ¨åˆ†èª²ç¨‹ç¸½è¦½
                record: Dict[str, Any] = {
                    "å­¸å¹´å­¸æœŸ": str(year_term),
                    "å¹´ç´š": str(grade_name),
                    "èª²ç¨‹ç¸½æ•¸": course_count,
                    "æœ¬æ‰¹èª²ç¨‹æ•¸": len(course_batch),
                    "èª²ç¨‹åˆ—è¡¨": course_batch,          # list[dict]ï¼Œæ¯ç­†æ˜¯ä¸€é–€èª²
                    "è³‡æ–™ä¾†æº": data_source_str,
                    "ä¾†æºæª”æ¡ˆ": source_path,
                }

                if overview_global_idx == 9:  # åªå°å‰å…©å€‹ chunkï¼Œé¿å…çˆ† log
                    print("[DEBUG course_history record]")
                    print(json.dumps(record, ensure_ascii=False, indent=2))
                    # ç„¶å¾Œå†å‘¼å« rewrite_json_record(...)

                try:
                    overview_text = rewrite_json_record(
                        record=record,
                        schema_hint="course_history_overview",
                        max_chars=500,
                    )
                except Exception as e:
                    print(f"[course_history_nested_to_documents] rewrite_json_record ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}")
                    sys.exit(1)

                # metadataï¼šé€™å€‹ chunk å…§çš„èª²ç¨‹æ¦‚è¦å­—ä¸²ï¼ˆåŸæœ¬å« course_namesï¼‰
                course_summaries_in_chunk = [
                    entry.get("æ¦‚è¦", "") for entry in course_batch if entry.get("æ¦‚è¦")
                ]

                docs.append(Document(
                    page_content=overview_text.strip(),
                    metadata={
                        "source": source_path,
                        "file_type": "json",
                        "type": "course_history_overview",
                        "content_type": "course_history_overview",

                        "year_term": str(year_term),
                        "year": year,
                        "term": term,
                        "grade": str(grade_name),

                        "course_count": course_count,
                        "course_names": "ã€".join(course_summaries_in_chunk),
                        "data_source": data_source_str,

                        "idx": overview_global_idx,  # int
                        "chunk": chunk_idx,
                        "total_chunks": num_chunks,
                        "needs_split": False,
                    }
                ))

    return docs


# =========================
# calendar.jsonï¼ˆè¡Œäº‹æ›†ï¼šä¾æœˆåˆ†åˆ‡å¡Šï¼‰ adapter
# =========================

def calendar_months_to_documents(
    data: List[Dict[str, Any]], source_path: str
) -> List[Document]:
    docs: List[Document] = []
    if not data:
        return docs

    def to_int(x) -> int | None:
        try:
            s = str(x).strip()
            return int(s) if s else None
        except Exception:
            return None

    def parse_day_start(day_raw: str) -> int | None:
        """
        æŠ“ã€Œèµ·å§‹æ—¥ã€æ’åºç”¨ï¼š
        - "1" -> 1
        - "8~12" -> 8
        - "10/13~11/3" -> 13 (å–èµ·å§‹æ—¥)
        è§£æå¤±æ•—å°± None
        """
        s = (day_raw or "").strip()
        if not s:
            return None
        # å–ç¬¬ä¸€æ®µå¯èƒ½çš„æ•¸å­—
        m = re.search(r"(\d+)", s)
        if not m:
            return None
        try:
            return int(m.group(1))
        except Exception:
            return None

    # 1) ä¾ (å¹´, æœˆ) åˆ†çµ„
    grouped: Dict[tuple[int | None, int | None], List[Dict[str, Any]]] = {}
    for rec in data:
        y = to_int(rec.get("å¹´"))
        m = to_int(rec.get("æœˆ"))
        grouped.setdefault((y, m), []).append(rec)

    # 2) ä¾å¹´/æœˆæ’åºè¼¸å‡º
    month_items = sorted(grouped.items(), key=lambda kv: (kv[0][0] or 0, kv[0][1] or 0))

    idx = 0
    for (year_roc, month), items in month_items:
        idx += 1
        year_ad = year_roc + 1911 if year_roc is not None else None

        # å…ˆæŒ‰ã€Œèµ·å§‹æ—¥ã€ç²—æ’åºï¼ˆNone çš„ä¿æŒåŸé †åºï¼‰
        items_sorted = sorted(
            items,
            key=lambda r: (
                parse_day_start(str(r.get("æ—¥", ""))) is None,
                parse_day_start(str(r.get("æ—¥", ""))) or 0,
            ),
        )

        # 3) æ•´ç†æˆæ´»å‹•åˆ—è¡¨ï¼ˆçµ¦é‡å¯«å™¨ & metadata ç”¨ï¼‰
        events_entries: List[Dict[str, Any]] = []
        events_for_meta: List[str] = []
        data_sources: List[str] = []

        for r in items_sorted:
            day_raw = str(r.get("æ—¥", "")).strip()
            weekday = str(r.get("æ˜ŸæœŸ", "")).strip()
            event = str(r.get("æ´»å‹•äº‹é …", "")).strip()
            ds = str(r.get("è³‡æ–™ä¾†æº", "")).strip()

            if ds:
                data_sources.append(ds)

            # çµ¦ metadata ç”¨çš„ç°¡å–®å­—ä¸²
            if event:
                events_for_meta.append(f"{month}/{day_raw}:{event}")

            # çµ¦é‡å¯«å™¨ç”¨çš„çµæ§‹åŒ–æ´»å‹•è³‡è¨Š
            events_entries.append(
                {
                    "æ—¥": day_raw,
                    "æ˜ŸæœŸ": weekday,
                    "æ´»å‹•äº‹é …": event,
                    "è³‡æ–™ä¾†æº": ds,
                }
            )

        data_source_str = "ï¼›".join(sorted(set(data_sources)))

        # 4) æº–å‚™é€™å€‹ã€Œæœˆä»½ç¸½è¦½ã€çš„ recordï¼Œä¸Ÿçµ¦é‡å¯«å™¨
        header_title = f"{year_roc if year_roc is not None else ''}å¹´{month if month is not None else ''}æœˆè¡Œäº‹æ›†"

        record: Dict[str, Any] = {
            "è¡Œäº‹æ›†æ¨™é¡Œ": header_title,
            "æ°‘åœ‹å¹´": year_roc,
            "è¥¿å…ƒå¹´": year_ad,
            "æœˆä»½": month,
            "æ´»å‹•æ•¸é‡": len(events_entries),
            "æ´»å‹•åˆ—è¡¨": events_entries,
            "è³‡æ–™ä¾†æº": data_source_str,
            "ä¾†æºæª”æ¡ˆ": source_path,
        }

        try:
            text = rewrite_json_record(
                record=record,
                schema_hint="calendar_month",  # å°æ‡‰é€™ç¨®ã€Œæœˆä»½ç¸½è¦½ã€è³‡æ–™
                max_chars=500,
            )
        except Exception as e:
            print(f"[calendar_months_to_documents] rewrite_json_record ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}")
            sys.exit(1)

        # 5) ç‰¹åŒ– metadataï¼ˆç¶­æŒåŸæœ¬æ¬„ä½ï¼‰
        meta = {
            "source": source_path,
            "file_type": "json",
            "type": "calendar_month",
            "content_type": "calendar_month",

            "title": str(items_sorted[0].get("title", "")).strip(),
            "year_roc": year_roc,
            "year_ad": year_ad,
            "month": month,

            "event_count": len(items_sorted),
            "events": "ã€".join(events_for_meta),   # å­˜æˆå­—ä¸²ï¼Œæ–¹ä¾¿ filter / æª¢ç´¢
            "data_source": data_source_str,

            "idx": idx,
            "needs_split": False,  # æœˆ chunk ä¸å†äºŒæ¬¡åˆ‡
        }

        docs.append(Document(page_content=text.strip(), metadata=meta))

    return docs

def calendar_events_to_documents(
    data: List[Dict[str, Any]], source_path: str
) -> List[Document]:
    """
    å°‡è¡Œäº‹æ›†æ¯ä¸€ç­†æ´»å‹•ç¨ç«‹æˆä¸€ä»½ Documentï¼Œ
    ä¸¦è£œ event_date/event_date_ts è®“ retriever èƒ½ç”¨æ™‚é–“ filterã€‚
    å…§å®¹æœ¬é«”æ”¹ç”¨ rewrite_json_record åšè‡ªç„¶èªå¥é‡å¯«ã€‚
    """
    docs: List[Document] = []
    if not data:
        return docs

    tz = ZoneInfo("Asia/Taipei")

    def to_int(x) -> int | None:
        try:
            s = str(x).strip()
            return int(s) if s else None
        except Exception:
            return None

    def parse_range_start(day_raw: str) -> tuple[int | None, int | None]:
        """
        å¾ã€Œæ—¥ã€æ¬„æŠ“èµ·å§‹(æœˆ,æ—¥)ï¼š
        - "1" -> (None, 1)
        - "8~12" -> (None, 8)
        - "10/13~11/3" -> (10, 13)
        """
        s = (day_raw or "").strip()
        if not s:
            return None, None

        m = re.match(r"(\d+)\s*/\s*(\d+)\s*~\s*(\d+)\s*/\s*(\d+)", s)
        if m:
            return int(m.group(1)), int(m.group(2))

        m = re.match(r"(\d+)\s*~\s*(\d+)", s)
        if m:
            return None, int(m.group(1))

        m = re.search(r"(\d+)", s)
        if m:
            return None, int(m.group(1))

        return None, None

    idx = 0
    for rec in data:
        year_roc = to_int(rec.get("å¹´"))
        month = to_int(rec.get("æœˆ"))
        day_raw = str(rec.get("æ—¥", "")).strip()

        start_m_raw, start_d = parse_range_start(day_raw)
        start_m = start_m_raw if start_m_raw is not None else month

        year_ad = year_roc + 1911 if year_roc is not None else None

        event_date_iso = None
        event_date_ts = None
        if year_ad and start_m and start_d:
            event_date_iso = f"{year_ad:04d}-{start_m:02d}-{start_d:02d}"
            event_date_ts = int(datetime(year_ad, start_m, start_d, tzinfo=tz).timestamp())

        weekday = str(rec.get("æ˜ŸæœŸ", "")).strip()
        activity = str(rec.get("æ´»å‹•äº‹é …", "")).strip()
        url = str(rec.get("è³‡æ–™ä¾†æº", "")).strip()
        title = str(rec.get("title", "è¡Œäº‹æ›†")).strip()

        idx += 1

        # === æº–å‚™çµ¦é‡å¯«å™¨çš„ record ===
        record: Dict[str, Any] = {
            "æ¨™é¡Œ": title,
            "æ°‘åœ‹å¹´": year_roc,
            "è¥¿å…ƒå¹´": year_ad,
            "æœˆä»½": month,
            "åŸå§‹æ—¥æ¬„ä½": day_raw,
            "è§£æèµ·å§‹æœˆä»½": start_m,
            "è§£æèµ·å§‹æ—¥": start_d,
            "æ˜ŸæœŸ": weekday,
            "æ´»å‹•äº‹é …": activity,
            "æ´»å‹•æ—¥æœŸ": event_date_iso,
            "æ´»å‹•æ—¥æœŸ_timestamp": event_date_ts,
            "è³‡æ–™ä¾†æº": url,
            "ä¾†æºæª”æ¡ˆ": source_path,
            "åŸå§‹ç´€éŒ„": rec,   # ä¿éšªï¼šæŠŠåŸå§‹ JSON ä¹Ÿå¡é€²å»ï¼Œè®“ LLM å¯ä»¥çœ‹åˆ°å…¨éƒ¨æ¬„ä½
        }

        try:
            text = rewrite_json_record(
                record=record,
                schema_hint="calendar_event",  # å–®ç­†è¡Œäº‹æ›†æ´»å‹•
                max_chars=400,
            )
        except Exception as e:
            print(f"[calendar_events_to_documents] rewrite_json_record ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}")
            sys.exit(1)

        meta = {
            "source": source_path,
            "file_type": "json",

            # âœ… çµ¦ retriever/filter ç”¨
            "type": "calendar_event",
            "content_type": "calendar_event",

            "title": title,
            "year_roc": year_roc,
            "year_ad": year_ad,
            "month": start_m,
            "day_raw": day_raw,
            "weekday": weekday,

            "event_date": event_date_iso,
            "event_date_ts": event_date_ts,   # âœ… é—œéµï¼šepoch int

            "activity": activity,
            "url": url,
            "idx": idx,
            "needs_split": False,
        }

        docs.append(Document(page_content=text.strip(), metadata=meta))

    return docs
    
# =========================
# program_courses.jsonï¼ˆä»¥èª²ç¨‹é¡åˆ¥åˆ†çµ„åˆ‡å¡Šï¼‰ adapter
# =========================

def program_courses_to_documents(
    data: List[Dict[str, Any]], source_path: str
) -> List[Document]:
    docs: List[Document] = []

    if not data:
        return []

    # å–å­¸ç¨‹å±¤ç´šè³‡è¨Šï¼ˆæ¯ç­†éƒ½ä¸€æ¨£ï¼Œæ‹¿ç¬¬ä¸€ç­†å³å¯ï¼‰
    program_title = str(data[0].get("title", "")).strip()
    program_purpose = str(data[0].get("è¨­ç½®å®—æ—¨", "")).strip()
    program_target = str(data[0].get("é©ç”¨å°è±¡", "")).strip()

    def parse_credits(x: Any) -> float | None:
        try:
            s = str(x).strip()
            if not s or s.lower() == "nan":
                return None
            return float(s)
        except Exception:
            return None

    def extract_substitutes(note: str) -> str:
        note = note or ""
        alts = re.findall(r"ã€([^ã€‘]+)ã€‘", note)
        alts = [a.strip() for a in alts if a.strip()]
        return "ã€".join(alts)

    # 1) ä¾èª²ç¨‹é¡åˆ¥åˆ†çµ„
    grouped: Dict[str, List[Dict[str, Any]]] = {}
    for rec in data:
        cat = str(rec.get("èª²ç¨‹é¡åˆ¥", "")).strip() or "æœªåˆ†é¡"
        grouped.setdefault(cat, []).append(rec)

    # 2) æ¯å€‹é¡åˆ¥ â†’ ä¸€ä»½ Documentï¼ˆä¸Ÿçµ¦é‡å¯«å™¨ï¼‰
    for idx, (cat, items) in enumerate(grouped.items(), 1):
        course_names: List[str] = []
        required_count = 0
        credits_sum = 0.0

        # æº–å‚™çµ¦é‡å¯«å™¨ç”¨çš„ã€Œèª²ç¨‹åˆ—è¡¨ã€
        course_entries: List[Dict[str, Any]] = []

        for rec in items:
            code = str(rec.get("èª²ç¨‹ä»£ç¢¼", "")).strip()
            name = str(rec.get("èª²ç¨‹åç¨±", "")).strip()
            credits = parse_credits(rec.get("å­¸åˆ†æ•¸"))
            note = str(rec.get("å‚™è¨»", "")).strip()
            note = "" if note.lower() == "nan" else note

            required = ("å¿…ä¿®" in note) or ("å¿…é¸" in note)
            if required:
                required_count += 1

            if credits is not None:
                credits_sum += credits

            substitutes = extract_substitutes(note)

            # çµ¦é‡å¯«å™¨çœ‹çš„å–®ç­†èª²ç¨‹è³‡è¨Š
            entry: Dict[str, Any] = {
                "èª²ç¨‹ä»£ç¢¼": code,
                "èª²ç¨‹åç¨±": name,
                "å­¸åˆ†æ•¸": credits,
                "å‚™è¨»": note,
                "æ˜¯å¦å¿…ä¿®": required,
                "å¯æ›¿ä»£èª²ç¨‹": substitutes,  # å¾å‚™è¨»ä¸­æŠ½å‡ºçš„æ›¿ä»£èª²ç¨‹è³‡è¨Š
            }
            course_entries.append(entry)

            # çµ¦ metadata ç”¨çš„ç°¡å–®åç¨±å­—ä¸²
            if name and code:
                course_names.append(f"{name}({code})")
            elif name:
                course_names.append(name)

        # æº–å‚™æ•´å€‹ã€Œå­¸ç¨‹ï¼‹é¡åˆ¥ã€çš„ recordï¼Œä¸Ÿçµ¦é‡å¯«å™¨
        record: Dict[str, Any] = {
            "å­¸ç¨‹åç¨±": program_title,
            "å­¸ç¨‹è¨­ç½®å®—æ—¨": program_purpose,
            "å­¸ç¨‹é©ç”¨å°è±¡": program_target,
            "èª²ç¨‹é¡åˆ¥": cat,
            "èª²ç¨‹åˆ—è¡¨": course_entries,
            "èª²ç¨‹ç¸½æ•¸": len(items),
            "å¿…ä¿®èª²ç¨‹æ•¸": required_count,
            "ç¸½å­¸åˆ†æ•¸": credits_sum,
            "ä¾†æºæª”æ¡ˆ": source_path,
        }

        try:
            text = rewrite_json_record(
                record=record,
                schema_hint="program_courses",   # å°æ‡‰é€™ç¨®ã€Œå­¸ç¨‹èª²ç¨‹ã€è³‡æ–™
                max_chars=500,
            )
            # ğŸ” åŠ é€™å…©è¡Œçœ‹çœ‹å¯¦éš›è¼¸å‡ºé•·æ€æ¨£
            if idx == 1:
                print("\n[DEBUG program_courses_to_documents] sample output:")
                print(text[:200])
        except Exception as e:
            print(f"[program_courses_to_documents] rewrite_json_record ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}")
            sys.exit(1)

        meta = {
            "source": source_path,
            "file_type": "json",
            "type": "program_course_category",
            "content_type": "program_course_category",

            # å­¸ç¨‹å±¤ç´š
            "program_title": program_title,
            "program_purpose": program_purpose,
            "program_target": program_target,

            # é¡åˆ¥å±¤ç´šï¼ˆåˆ‡å¡Š keyï¼‰
            "course_category": cat,
            "course_count": len(items),
            "required_count": required_count,
            "credits_sum": credits_sum,

            # ç‚ºäº† metadata å¯ç´¢å¼•ã€ä¸èƒ½æ”¾ list â†’ è½‰å­—ä¸²
            "courses": "ã€".join(course_names),

            "idx": idx,
            "needs_split": False,
        }

        docs.append(Document(page_content=text.strip(), metadata=meta))

    return docs

# =========================
# course_overview.jsonï¼ˆèª²ç¨‹ç¸½è¦½ï¼‰ adapter
# =========================

def course_overview_to_documents(
    data: List[Dict[str, Any]], source_path: str
) -> List[Document]:
    docs: List[Document] = []

    def parse_year_term(s: str) -> tuple[int | None, str]:
        """ç°¡å–®æŠŠ '113ä¸Š' æ‹†æˆ (113, 'ä¸Š')ï¼Œå¤±æ•—å°± (None, '')ã€‚"""
        s = (s or "").strip()
        if not s:
            return None, ""
        year = None
        for i, ch in enumerate(s):
            if not ch.isdigit():
                try:
                    year = int(s[:i])
                except Exception:
                    year = None
                term = s[i:]
                return year, term
        try:
            return int(s), ""
        except Exception:
            return None, ""

    for i, rec in enumerate(data, 1):
        if not isinstance(rec, dict):
            # ä¿éšªè™•ç†ï¼šé dict å°±åŒ…æˆä¸€å€‹æ¬„ä½
            rec = {"value": rec}

        year_term_raw = str(rec.get("å­¸å¹´å­¸æœŸ", "")).strip()
        year, term = parse_year_term(year_term_raw)

        select_type = str(rec.get("é¸åˆ¥", "")).strip()        # å¿…ä¿® / é¸ä¿®
        grade = str(rec.get("æ‰€å±¬å¹´ç´š", "")).strip()          # ä¸€å¹´ç´š / äºŒå¹´ç´š / ä¸‰å¹´ç´š / å››å¹´ç´š
        data_source = str(rec.get("è³‡æ–™ä¾†æº", "")).strip()    # URL

        names = rec.get("èª²ç¨‹åç¨±") or []
        if not isinstance(names, list):
            names = [names]
        # æ¸…æ‰ç©ºå­—ä¸²ï¼Œå…¨éƒ¨è½‰æˆ str
        names = [str(n).strip() for n in names if str(n).strip()]
        course_count = len(names)
        courses_str = "ã€".join(names)

        # æº–å‚™çµ¦é‡å¯«å™¨çš„ record
        record: Dict[str, Any] = dict(rec)  # è¤‡è£½ä¸€ä»½ï¼Œé¿å…å‹•åˆ°åŸè³‡æ–™

        # è£œå……çµæ§‹åŒ–è³‡è¨Šçµ¦ LLM åƒè€ƒ
        record.setdefault("å­¸å¹´å­¸æœŸ", year_term_raw)
        record.setdefault("è§£æå­¸å¹´åº¦", year)           # int æˆ– None
        record.setdefault("è§£æå­¸æœŸ", term)             # "ä¸Š" / "ä¸‹" / ""
        record.setdefault("æ‰€å±¬å¹´ç´š", grade)
        record.setdefault("é¸åˆ¥", select_type)
        record.setdefault("èª²ç¨‹åç¨±åˆ—è¡¨", names)
        record.setdefault("èª²ç¨‹æ•¸", course_count)
        record.setdefault("è³‡æ–™ä¾†æº", data_source)
        record.setdefault("ä¾†æºæª”æ¡ˆ", source_path)

        try:
            rewritten = rewrite_json_record(
                record=record,
                schema_hint="course_overview",   # å°æ‡‰ detect_schema ä¸­çš„é¡å‹
                max_chars=400,
            )
        except Exception as e:
            print(f"[course_overview_to_documents] rewrite_json_record ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}")
            sys.exit(1)

        text = rewritten.strip()

        meta = {
            "source": source_path,
            "file_type": "json",
            "type": "course_overview",         # çµ¦çµ±è¨ˆ/é™¤éŒ¯ç”¨
            "content_type": "course_overview", # ä¹‹å¾Œ filter ç”¨é€™å€‹

            "year_term": year_term_raw,
            "year": year,                      # int æˆ– None
            "term": term,                      # "ä¸Š" / "ä¸‹" / ""
            "grade": grade,                    # ä¸€å¹´ç´š / äºŒå¹´ç´š / ä¸‰å¹´ç´š / å››å¹´ç´š

            "select_type": select_type,        # å¿…ä¿® / é¸ä¿®
            "course_count": course_count,      # int
            "courses": courses_str,            # âœ… å­—ä¸²ï¼Œä¸æ˜¯ list
            "data_source": data_source,        # è³‡æ–™ä¾†æº URL

            "idx": i,
            "needs_split": False,              # ä¸å†åˆ‡å¡Š
        }

        docs.append(Document(page_content=text, metadata=meta))

    return docs

# =========================
# course_history.jsonï¼ˆæ­·å¹´èª²ç¨‹è³‡æ–™ï¼‰ adapter
# =========================

def course_records_to_documents(
    data: List[Dict[str, Any]], source_path: str
) -> List[Document]:
    docs: List[Document] = []

    def parse_year_term(s: str) -> tuple[int | None, str]:
        # e.g. "113ä¸Š" / "113ä¸‹"
        s = (s or "").strip()
        if not s:
            return None, ""
        year = None
        for i, ch in enumerate(s):
            if not ch.isdigit():
                try:
                    year = int(s[:i])
                except Exception:
                    year = None
                term = s[i:]
                return year, term
        try:
            return int(s), ""
        except Exception:
            return None, ""

    for i, rec in enumerate(data, 1):
        yt = rec.get("å­¸å¹´å­¸æœŸ", "")
        year, term = parse_year_term(yt)

        code = rec.get("èª²è™Ÿ", "") or ""
        name = rec.get("èª²ç¨‹åç¨±", "") or ""
        teacher = rec.get("æ•™å¸«", "") or ""
        category = rec.get("é¸åˆ¥", "") or ""
        dept = rec.get("æ‰€å±¬ç³»æ‰€", "") or ""
        grade = rec.get("æ‰€å±¬å¹´ç´š", "") or ""
        data_source = rec.get("è³‡æ–™ä¾†æº", "") or ""

        credit_raw = rec.get("å­¸åˆ†", None)
        try:
            credits = float(credit_raw) if credit_raw not in (None, "", " ") else None
        except Exception:
            credits = None

        # çµ¦ LLM çš„æ–‡å­—
        lines = [
            f"å­¸å¹´å­¸æœŸï¼š{yt}",
            f"æ‰€å±¬å¹´ç´šï¼š{grade}",
            f"èª²è™Ÿï¼š{code}",
            f"èª²ç¨‹åç¨±ï¼š{name}",
            f"æ•™å¸«ï¼š{teacher}",
            f"é¸åˆ¥ï¼š{category}",
            f"å­¸åˆ†ï¼š{credits if credits is not None else ''}",
            f"æ‰€å±¬ç³»æ‰€ï¼š{dept}",
        ]
        if data_source:
            lines.append(f"è³‡æ–™ä¾†æºï¼š{data_source}")
        text = "\n".join(lines)

        meta = {
            "source": source_path,
            "file_type": "json",
            "type": "course_history",      # çµ±è¨ˆç”¨
            "content_type": "course",      # ä¹‹å¾Œ filter ç”¨é€™å€‹

            "year_term": yt,
            "year": year,                  # int or None
            "term": term,                  # "ä¸Š" / "ä¸‹" / ""
            "grade": grade,                # ä¸€å¹´ç´š / äºŒå¹´ç´š / ä¸‰å¹´ç´š / å››å¹´ç´š

            "course_code": code,
            "course_name": name,
            "teacher": teacher,
            "category": category,          # "å¿…ä¿®" / "é¸ä¿®"
            "required": (category == "å¿…ä¿®"),
            "credits": credits,            # float or None
            "department": dept,
            "data_source": data_source,    # è³‡æ–™ä¾†æº URL

            "idx": i,
            "needs_split": False,
        }

        docs.append(Document(page_content=text, metadata=meta))

    return docs

# =========================
# contact.jsonï¼ˆè¯çµ¡è³‡è¨Šï¼‰ adapter
# =========================

def contact_records_to_documents(
    data: List[Dict[str, Any]], source_path: str
) -> List[Document]:
    docs: List[Document] = []

    for i, rec in enumerate(data, 1):
        data_source = rec.get("è³‡æ–™ä¾†æº", "") or ""

        # === åˆ¤æ–·è¯çµ¡é¡å‹ï¼ˆroleï¼‰ï¼†æŠ½ metadata ç”¨çš„æ¬„ä½ ===
        if "è¾¦ç†é …ç›®" in rec:  # è¡Œæ”¿/æ‹›ç”Ÿé¡
            role = "service"
            item = str(rec.get("è¾¦ç†é …ç›®", "")).strip()
            person = str(rec.get("æ‰¿è¾¦äºº", "")).strip()
            ext = str(rec.get("åˆ†æ©Ÿ", "")).strip()
        elif "å­¸ç³»" in rec:   # å„å­¸ç³»è¯çµ¡äºº
            role = "department"
            item = ""  # é€™é¡æ²’æœ‰ã€Œè¾¦ç†é …ç›®ã€
            dept = str(rec.get("å­¸ç³»", "")).strip()
            person = str(rec.get("è¯çµ¡äººå“¡", "")).strip()
            ext = str(rec.get("åˆ†æ©Ÿ", "")).strip()
        else:
            role = "unknown"
            item = ""
            dept = str(rec.get("å­¸ç³»", "")).strip() if "å­¸ç³»" in rec else ""
            person = str(rec.get("æ‰¿è¾¦äºº") or rec.get("è¯çµ¡äººå“¡") or "").strip()
            ext = str(rec.get("åˆ†æ©Ÿ") or "").strip()

        # === æº–å‚™çµ¦é‡å¯«å™¨çš„ record ===
        record: Dict[str, Any] = dict(rec)  # æ‹·è²ä¸€ä»½ï¼Œé¿å…ç›´æ¥æ”¹åˆ°åŸå§‹è³‡æ–™

        # è£œå……ä¸€äº›èªæ„æç¤ºæ¬„ä½ï¼Œè®“ LLM å¥½å¯«ä¸€é»
        if role == "service":
            record.setdefault("è¯çµ¡é¡å‹", "è¡Œæ”¿æˆ–æ‹›ç”Ÿç›¸é—œæœå‹™è¯çµ¡è³‡è¨Š")
        elif role == "department":
            record.setdefault("è¯çµ¡é¡å‹", "å­¸ç³»è¯çµ¡äººè³‡è¨Š")
        else:
            record.setdefault("è¯çµ¡é¡å‹", "ä¸€èˆ¬è¯çµ¡è³‡è¨Š")

        if data_source:
            record.setdefault("è³‡æ–™ä¾†æº", data_source)
        record.setdefault("ä¾†æºæª”æ¡ˆ", source_path)

        try:
            rewritten = rewrite_json_record(
                record=record,
                schema_hint="contacts",   # å°æ‡‰ä½  detect_schema çš„é¡å‹
                max_chars=400,
            )
        except Exception as e:
            print(f"[contact_records_to_documents] rewrite_json_record ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}")
            sys.exit(1)

        text = rewritten.strip()

        # === metadata ä¿ç•™åŸæœ¬è¨­è¨ˆ ===
        meta = {
            "source": source_path,
            "file_type": "json",
            "type": "contact",           # çµ¦ä½ çµ±è¨ˆç”¨
            "content_type": "contact",   # ä¹‹å¾Œ filter ç”¨é€™å€‹
            "role": role,                # "service" or "department" or "unknown"
            "item": rec.get("è¾¦ç†é …ç›®") or "",
            "department": rec.get("å­¸ç³»") or "",
            "person": rec.get("æ‰¿è¾¦äºº") or rec.get("è¯çµ¡äººå“¡") or "",
            "phone": rec.get("åˆ†æ©Ÿ") or "",
            "data_source": data_source,
            "idx": i,
            "needs_split": False,
        }

        docs.append(Document(page_content=text, metadata=meta))

    return docs

# =========================
# academic_requirements.jsonï¼ˆå­¸å‰‡/ç•¢æ¥­è¦å®šï¼‰ adapter
# =========================

def academic_records_to_documents(
    data: List[Dict[str, Any]], source_path: str
) -> List[Document]:
    docs: List[Document] = []

    def infer_topic(category: str) -> str:
        if "ä¿®æ¥­è¦å®š" in category:
            return "graduation"
        if "å°ˆé¡Œ" in category:
            return "capstone"
        if "è¼”ç³»" in category:
            return "minor"
        if "è½‰ç³»" in category:
            return "transfer"
        if "å¯¦ç¿’" in category:
            return "internship"
        if "å£è©¦" in category:
            return "thesis_oral"
        return "general"

    for i, rec in enumerate(data, 1):
        category = str(rec.get("é¡åˆ¥", "")).strip()
        topic = infer_topic(category)

        # æº–å‚™çµ¦é‡å¯«å™¨çš„ recordï¼š
        # å…ˆè¤‡è£½åŸæœ¬çš„ recï¼Œä¸¦è£œä¸Šæ¨è«–å‡ºä¾†çš„ topicã€ä¾†æºç­‰è³‡è¨Š
        record: Dict[str, Any] = dict(rec)
        record["æ¨è«–ä¸»é¡Œ"] = topic              # çµ¦ LLM ä¸€é»èªæ„æç¤º
        record["ä¾†æºæª”æ¡ˆ"] = source_path

        try:
            rewritten = rewrite_json_record(
                record=record,
                schema_hint="academic_rules",    # å­¸ç± / ä¿®æ¥­è¦å®š / å°ˆé¡Œ / å¯¦ç¿’ ç­‰è¦å®š
                max_chars=500,                   # å¯ä»¥ç¨å¾®é•·ä¸€é»ï¼Œè¦–éœ€è¦å†èª¿æ•´
            )
        except Exception as e:
            print(f"[academic_records_to_documents] rewrite_json_record ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}")
            sys.exit(1)

        text = rewritten.strip()

        meta = {
            "source": source_path,
            "file_type": "json",
            "type": "academic_rules",
            "content_type": "academic_rule",
            "category": category,
            "topic": topic,   # å–®ä¸€å­—ä¸²ï¼Œæ–¹ä¾¿ filter
            "idx": i,
            "needs_split": False,
        }

        docs.append(Document(page_content=text, metadata=meta))

    return docs

# =========================
# schoolï¼ˆå­¸æ ¡è³‡è¨Šï¼‰ adapter
# =========================

def school_info_to_documents(obj: Any, source_path: str) -> List[Document]:
    """
    å°‡ about_school.json é€™é¡ã€Œå­¸æ ¡è³‡è¨Šã€æ•´ç†æˆä¸€ä»½ Documentï¼Œ
    ä¸¦åœ¨ metadata è£¡è£œå……å¸¸ç”¨æ¬„ä½ï¼ˆæ ¡åã€æ ¡è¨“ã€ç¶²å€ç­‰ï¼‰ã€‚
    ä¸»é«”å…§å®¹æ”¹ç‚ºäº¤çµ¦ rewrite_json_record åšè‡ªç„¶èªå¥é‡å¯«ã€‚
    """
    # é æœŸæ ¼å¼ï¼šlist[dict]
    if not isinstance(obj, list) or not obj:
        # éé æœŸæ ¼å¼å°±å…ˆèµ°åŸæœ¬çš„ç°¡å–® fallbackï¼Œä¸å‘¼å«é‡å¯«å™¨
        text = str(obj)
        meta = {
            "source": source_path,
            "file_type": "json",
            "content_type": "school",
            "needs_split": False,
            "idx": 1,
        }
        return [Document(page_content=text, metadata=meta)]

    # åšå€‹å®‰å…¨çš„ helperï¼šå¾ä¸åŒ block æŠ“ key
    def find_key(key: str, default: str = "") -> Any:
        for block in obj:
            if isinstance(block, dict) and key in block:
                return block[key]
        return default

    # ---------- æŠ½å‡ºçµæ§‹åŒ–æ¬„ä½ï¼ˆä½œç‚º metadata ç”¨ï¼‰ ----------
    # 1) åŸºæœ¬æ ¡å‹™
    name = find_key("åç¨±", "")
    name_en = find_key("è‹±æ–‡åç¨±", "")
    motto = find_key("æ ¡è¨“", "")
    founded_at = find_key("æˆç«‹æ™‚é–“", "")
    founder = find_key("å‰µè¾¦äºº", "")
    school_type = find_key("é¡å‹", "")

    # 2) è¯çµ¡è³‡è¨Š
    address = find_key("åœ°å€", "")
    phone = find_key("é›»è©±", "")
    emergency_phone = find_key("ç·Šæ€¥æ ¡å®‰å°ˆç·š", "")
    fax = find_key("å‚³çœŸ", "")
    president_phone = find_key("æ ¡é•·å®¤é›»è©±", "")
    president_fax = find_key("æ ¡é•·å®¤å‚³çœŸ", "")
    president_email = find_key("æ ¡é•·å®¤ email", "")

    # 3) å…¶ä»–æ ¡å‹™
    school_code = find_key("å­¸æ ¡ä»£ç¢¼", "")
    url = find_key("ç¶²å€", "")
    departments = find_key("ç³»æ‰€çµæ§‹", [])
    if isinstance(departments, list):
        departments_str = "ã€".join(map(str, departments))
    else:
        departments_str = str(departments) if departments else ""

    student_count = find_key("å­¸ç”Ÿäººæ•¸", "")
    mascots = find_key("æ ¡å‹å‰ç¥¥ç‰©", [])
    if isinstance(mascots, list):
        mascots_str = "ã€".join(map(str, mascots))
    else:
        mascots_str = str(mascots) if mascots else ""

    # 4) æ­·å²æ²¿é©
    prev_name = find_key("å‰èº«", "")
    reorg_at = find_key("æ”¹åˆ¶æ™‚é–“", "")
    rename_at = find_key("æ›´åæ™‚é–“", "")

    # 5) è¾¦å­¸ç‰¹è‰²
    feature = find_key("ç‰¹è‰²", "")
    focus_fields = find_key("é‡é»é ˜åŸŸ", [])
    if isinstance(focus_fields, list):
        focus_fields_str = "ã€".join(map(str, focus_fields))
    else:
        focus_fields_str = str(focus_fields) if focus_fields else ""

    philosophy = find_key("è¾¦å­¸ç†å¿µ", "")
    alliance = find_key("è¯ç›Ÿ", "")

    # ---------- æº–å‚™çµ¦é‡å¯«å™¨çš„ recordï¼ˆåŒ…å«åŸå§‹å€å¡Šï¼‰ ----------
    record: Dict[str, Any] = {
        # åŸºæœ¬æ ¡å‹™
        "åç¨±": name,
        "è‹±æ–‡åç¨±": name_en,
        "æ ¡è¨“": motto,
        "æˆç«‹æ™‚é–“": founded_at,
        "å‰µè¾¦äºº": founder,
        "é¡å‹": school_type,

        # è¯çµ¡è³‡è¨Š
        "åœ°å€": address,
        "é›»è©±": phone,
        "ç·Šæ€¥æ ¡å®‰å°ˆç·š": emergency_phone,
        "å‚³çœŸ": fax,
        "æ ¡é•·å®¤é›»è©±": president_phone,
        "æ ¡é•·å®¤å‚³çœŸ": president_fax,
        "æ ¡é•·å®¤ email": president_email,

        # å…¶ä»–æ ¡å‹™
        "å­¸æ ¡ä»£ç¢¼": school_code,
        "ç¶²å€": url,
        "ç³»æ‰€çµæ§‹": departments,      # ä¿ç•™åŸå§‹ listï¼ˆå¦‚æœæœ‰ï¼‰
        "å­¸ç”Ÿäººæ•¸": student_count,
        "æ ¡å‹å‰ç¥¥ç‰©": mascots,         # ä¿ç•™åŸå§‹ listï¼ˆå¦‚æœæœ‰ï¼‰

        # æ­·å²æ²¿é©
        "å‰èº«": prev_name,
        "æ”¹åˆ¶æ™‚é–“": reorg_at,
        "æ›´åæ™‚é–“": rename_at,

        # è¾¦å­¸ç‰¹è‰²
        "ç‰¹è‰²": feature,
        "é‡é»é ˜åŸŸ": focus_fields,      # ä¿ç•™åŸå§‹ listï¼ˆå¦‚æœæœ‰ï¼‰
        "è¾¦å­¸ç†å¿µ": philosophy,
        "è¯ç›Ÿ": alliance,

        # ä¿éšªï¼šæŠŠåŸå§‹ blocks ä¹Ÿæ”¾é€²å»ï¼Œè®“ LLM å¯ä»¥çœ‹åˆ°å®Œæ•´ JSON
        "åŸå§‹å€å¡Šåˆ—è¡¨": obj,
    }

    try:
        # max_chars å¯ä»¥è¦–æƒ…æ³èª¿æ•´ï¼Œå­¸æ ¡ç°¡ä»‹é€šå¸¸å¯ä»¥ç¨å¾®é•·ä¸€é»
        rewritten = rewrite_json_record(
            record=record,
            schema_hint="school_info",
            max_chars=500,
        )
    except Exception as e:
        print(f"[school_info_to_documents] rewrite_json_record ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}")
        sys.exit(1)

    text = rewritten.strip()

    # ---------- metadata ä¿æŒä½ åŸæœ¬çš„è¨­è¨ˆ ----------
    meta = {
        "source": source_path,
        "file_type": "json",
        "content_type": "school",

        # 1) åŸºæœ¬æ ¡å‹™
        "name": name,
        "name_en": name_en,
        "motto": motto,
        "founded_at": founded_at,
        "founder": founder,
        "school_type": school_type,

        # 2) è¯çµ¡è³‡è¨Š
        "address": address,
        "phone": phone,
        "emergency_phone": emergency_phone,
        "fax": fax,
        "president_phone": president_phone,
        "president_fax": president_fax,
        "president_email": president_email,

        # 3) å…¶ä»–æ ¡å‹™
        "school_code": school_code,
        "url": url,
        "departments": departments_str,    # å·²ç¶“æ˜¯ "ã€" ä¸²å¥½çš„å­—ä¸²
        "student_count": student_count,
        "mascots": mascots_str,

        # 4) æ­·å²æ²¿é©
        "prev_name": prev_name,
        "reorg_at": reorg_at,
        "rename_at": rename_at,

        # 5) è¾¦å­¸ç‰¹è‰²
        "feature": feature,
        "focus_fields": focus_fields_str,
        "philosophy": philosophy,
        "alliance": alliance,

        "needs_split": False,
        "idx": 1,
    }

    return [Document(page_content=text, metadata=meta)]

# =========================
# peopleï¼ˆè€å¸«åéŒ„ï¼‰ adapter
# =========================

def people_overview_to_documents(
    data: List[Dict[str, Any]],
    source_path: str,
    max_chars: int = 500,
) -> List[Document]:
    docs: List[Document] = []
    if not data:
        return docs

    # -------- helpers --------
    def count_chars(text: str) -> int:
        return len(text or "")

    def batch_lines_by_chars(
        header_lines: List[str],
        item_lines: List[str],
        tail_lines: List[str],
        max_chars: int,
    ) -> List[List[str]]:
        """
        ä¾ç…§å­—æ•¸æŠŠ header + item_lines + tail åˆ‡æˆå¤šæ‰¹ï¼Œæ¯æ‰¹å­—æ•¸ä¸è¶…é max_charsï¼ˆç›¡é‡ï¼‰ã€‚
        å›å‚³çš„æ¯å€‹ batch ä»æ˜¯ã€Œå­—ä¸²åˆ—è¡¨ã€ï¼Œæˆ‘å€‘å¾Œé¢æœƒå†å¾ä¸­è§£æå‡ºæˆå“¡æ¸…å–®ã€‚
        """
        batches: List[List[str]] = []

        fixed_text = "\n".join(header_lines + tail_lines)
        fixed_chars = count_chars(fixed_text)
        if fixed_chars >= max_chars:
            # header + tail å·²ç¶“è¶…éä¸Šé™ï¼Œå°±ä¸å†ç´°åˆ‡ï¼Œå…¨éƒ¨å¡ä¸€æ‰¹
            batches.append(header_lines + item_lines + tail_lines)
            return batches

        cur_chars = fixed_chars
        cur_items: List[str] = []

        for line in item_lines:
            lc = count_chars(line)
            if cur_items and (cur_chars + lc) > max_chars:
                batches.append(header_lines + cur_items + tail_lines)
                cur_items = []
                cur_chars = fixed_chars

            cur_items.append(line)
            cur_chars += lc

        if cur_items:
            batches.append(header_lines + cur_items + tail_lines)
        return batches

    def is_faculty_title(title: str) -> bool:
        t = title or ""
        if "ç³»å‹™åŠ©ç†" in t:
            return False
        # åªè¦å«æ•™æˆç³»è·ç¨±å°±ç®— facultyï¼ˆå«å…¼ä»»ï¼‰
        return any(k in t for k in ["è¬›åº§æ•™æˆ", "æ•™æˆ", "å‰¯æ•™æˆ", "åŠ©ç†æ•™æˆ"])

    def rank_group(title: str) -> str:
        t = title or ""
        if "è¬›åº§æ•™æˆ" in t:
            return "chair_professor"
        if "å…¼ä»»" in t and "æ•™æˆ" in t:
            return "adjunct_professor"
        # æ³¨æ„åˆ¤æ–·é †åºï¼šå…ˆå‰¯æ•™æˆ/åŠ©ç†æ•™æˆï¼Œå†æ•™æˆ
        if "å‰¯æ•™æˆ" in t:
            return "associate_professor"
        if "åŠ©ç†æ•™æˆ" in t:
            return "assistant_professor"
        if "æ•™æˆ" in t:
            return "professor"
        return "other"

    # -------- collect faculty --------
    faculty_rows = []
    dept_set = set()
    ds_set = set()

    for rec in data:
        title = str(rec.get("è·ç¨±", "") or rec.get("äººç‰©", "")).strip()
        if not is_faculty_title(title):
            continue

        name = str(rec.get("å§“å", "")).strip()
        if not name:
            # èˆŠæ ¼å¼ fallback
            name = str(rec.get("äººç‰©", "")).strip()

        dept = str(rec.get("ç³»æ‰€", "")).strip()
        if dept:
            dept_set.add(dept)

        ds = str(rec.get("è³‡æ–™ä¾†æº", "")).strip()
        if ds:
            ds_set.add(ds)

        # overview å–®è¡Œï¼ˆä¸å¯æ‹†çš„åŸå­ï¼‰ï¼Œå¾Œé¢ç”¨ä¾†åˆ‡ batch & é‚„åŸæˆå“¡åˆ—è¡¨
        line = f"{name} / {title}"
        faculty_rows.append((rank_group(title), line, name))

    if not faculty_rows:
        return docs

    data_source_str = "ï¼›".join(sorted(ds_set))
    departments_str = "ã€".join(sorted(dept_set))

    # -------- build overview scopes --------
    overview_idx = 0

    def emit_scope(scope: str, group: str, lines: List[str]):
        """
        scope: "faculty_all" æˆ– "rank_group"
        group: è·ç´šä»£ç¢¼ï¼ˆrank_group æ™‚æœ‰å€¼ï¼Œfaculty_all æ™‚ç‚º ""ï¼‰
        lines: ä¾‹å¦‚ ["å¼µä¸‰ / æ•™æˆ", "æå›› / å‰¯æ•™æˆ", ...]
        """
        nonlocal overview_idx, docs

        header = "æ•™æˆç¸½è¦½" if scope == "faculty_all" else f"{group} ç¸½è¦½"
        header_lines = [header, "æˆå“¡åˆ—è¡¨ï¼š"]
        item_lines = [f"- {ln}" for ln in lines]
        tail_lines = ["", f"è³‡æ–™ä¾†æºï¼š{data_source_str}"] if data_source_str else []

        # å…ˆç”¨åŸæœ¬çš„å­—æ•¸é‚è¼¯åˆ‡æˆå¤šå€‹ batchï¼Œå†å°æ¯å€‹ batch ä¸Ÿçµ¦é‡å¯«å™¨
        batches = batch_lines_by_chars(header_lines, item_lines, tail_lines, max_chars)
        total_chunks = len(batches)

        for chunk_i, batch_lines in enumerate(batches):
            # å¾ batch_lines ä¸­è§£æå‡ºè©² chunk çš„æˆå“¡æ¸…å–®ï¼ˆå§“å / è·ç¨±ï¼‰
            member_items = []
            for ln in batch_lines:
                ln = ln.strip()
                if not ln.startswith("- "):
                    continue
                raw = ln[2:].strip()  # å»æ‰å‰é¢çš„ "- "
                if " / " in raw:
                    name_part, title_part = raw.split(" / ", 1)
                else:
                    name_part, title_part = raw, ""
                member_items.append({
                    "å§“å": name_part.strip(),
                    "è·ç¨±": title_part.strip(),
                })

            # å¦‚æœé€™å€‹ chunk æ²’æœ‰ä»»ä½•æˆå“¡ï¼Œå°±ç•¥é
            if not member_items:
                continue

            overview_idx += 1

            # æº–å‚™çµ¦é‡å¯«å™¨ç”¨çš„ JSON record
            record = {
                "ç¸½è¦½æ¨™é¡Œ": header,
                "ç¯„åœé¡å‹": "å…¨éƒ¨æ•™æˆ" if scope == "faculty_all" else "è·ç´šåˆ†çµ„",
                "è·ç´šä»£ç¢¼": group if scope == "rank_group" else "",
                "ç³»æ‰€": departments_str,
                "æˆå“¡åˆ—è¡¨": member_items,
                "è³‡æ–™ä¾†æº": data_source_str,
            }

            try:
                text = rewrite_json_record(
                    record=record,
                    schema_hint="department_members_overview",
                    max_chars=max_chars,
                )
            except Exception as e:
                print(f"[people_overview_to_documents] rewrite_json_record ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}")
                sys.exit(1)

            names_in_chunk = [m["å§“å"] for m in member_items]

            docs.append(Document(
                page_content=text.strip(),
                metadata={
                    "source": source_path,
                    "file_type": "json",
                    "type": "people_overview",
                    "content_type": "people_overview",

                    "overview_scope": scope,
                    "rank_group": group if scope == "rank_group" else "",

                    "people_count": len(member_items),
                    "departments": departments_str,
                    "names": "ã€".join(names_in_chunk),
                    "data_source": data_source_str,

                    "idx": overview_idx,     # overview å…§å…¨åŸŸ int
                    "chunk": chunk_i,
                    "total_chunks": total_chunks,
                    "needs_split": False,
                }
            ))

    # (1) faculty_allï¼šå…¨é«”æ•™æˆ
    all_lines = [line for _, line, _ in faculty_rows]
    emit_scope("faculty_all", "", all_lines)

    # (2) rank_groupï¼šä¾è·ç´šåˆ†çµ„
    grouped: Dict[str, List[str]] = {}
    for rg, line, _name in faculty_rows:
        grouped.setdefault(rg, []).append(line)

    # å›ºå®šè¼¸å‡ºé †åº
    order = ["chair_professor", "professor", "associate_professor", "assistant_professor", "adjunct_professor"]
    for rg in order:
        lines = grouped.get(rg, [])
        if not lines:
            continue
        emit_scope("rank_group", rg, lines)

    return docs

_name_title_pat = re.compile(
    r"^\s*(?P<name>[\u4e00-\u9fa5A-Za-z0-9ï¼ãƒ»]+)\s*(?P<title>.+)?$"
)

def _parse_name_title(s: str) -> Dict[str, str]:
    m = _name_title_pat.match(s or "")
    if not m:
        return {"name": s or "", "title": ""}

    name = (m.group("name") or "").strip().replace("\u00a0", " ")
    title = (m.group("title") or "").strip(" ,ï¼Œ").replace("\u00a0", " ")

    # æœ‰äº›ä¾†æºæœƒæŠŠã€Œè·ç¨±/è·å‹™ï¼šã€é€™æ®µä¹Ÿå¡é€²ä¾†ï¼Œé€™é‚Šé †ä¾¿æ¸…æ‰é–‹é ­çš„æ¨™ç±¤
    if title.startswith("è·ç¨±/è·å‹™"):
        title = re.sub(r"^è·ç¨±/è·å‹™[:ï¼š]?\s*", "", title)

    return {"name": name, "title": title}

def people_records_to_documents(
    data: List[Dict[str, Any]], source_path: str
) -> List[Document]:
    docs: List[Document] = []
    for i, rec in enumerate(data, 1):
        # ===== é€™ä¸€æ®µæ˜¯ä½ åŸæœ¬æŠ“å§“å/è·ç¨±/ç³»æ‰€/ä¾†æº =====
        if "å§“å" in rec:
            who = {"name": rec.get("å§“å", "").strip(), "title": rec.get("è·ç¨±", "").strip()}
        else:
            who = _parse_name_title(rec.get("äººç‰©", ""))

        dept = rec.get("ç³»æ‰€") or rec.get("department") or "å¤§åŒå¤§å­¸ è³‡è¨Šå·¥ç¨‹å­¸ç³»"
        src_url = rec.get("è³‡æ–™ä¾†æº") or rec.get("source_url") or ""

        # ===== æ–°å¢ï¼šç”¨ LLM æŠŠé€™ä¸€ç­† JSON è½‰æˆæ•˜è¿°å¥ =====
        try:
            rewritten = rewrite_json_record(
                record=rec,
                schema_hint="department_members",   # æˆ– "è³‡å·¥ç³»å¸«è³‡åå–®"
                max_chars=400,
            )
        except Exception as e:
            print(f"rewrite_json_record ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            sys.exit(1)

        # ===== çµ„æˆ Document =====
        content = rewritten.strip()

        metadata = {
            "source": source_path,
            "idx": i,
            "name": who["name"],
            "title": who["title"],
            "department": dept,
            "url": src_url,
            "content_type": "people",
        }

        docs.append(Document(page_content=content, metadata=metadata))

    return docs

# =========================
# newsï¼ˆç³»ç¶²æ–°èï¼‰ adapter
# =========================
def _fmt_news_page_content(meta: Dict[str, Any], content: str) -> str:
    return "\n".join([
        f"é¡åˆ¥ï¼š{meta.get('category','')}",   # â† æ–°å¢        
        f"æ¨™é¡Œï¼š{meta.get('title','')}",
        f"æ—¥æœŸï¼š{meta.get('published_at','')}",
        f"é€£çµï¼š{meta.get('url','')}",        # â† æ–°å¢ï¼ˆæ–¹ä¾¿ LLM/æª¢ç´¢çŸ¥é“ä¾†æºï¼‰
        "å…§æ–‡ï¼š",
        content or "",
    ])

def news_records_to_documents(data: List[Dict[str, Any]], source_path: str) -> List[Document]:
    docs: List[Document] = []

    TARGET_CHARS = 1000
    OVERLAP_CHARS = 80

    total = len(data)
    print(f"[news] {source_path}ï¼šå…± {total} ç­†æ–°èï¼Œé–‹å§‹é‡å¯«â€¦")

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=TARGET_CHARS,
        chunk_overlap=OVERLAP_CHARS,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ã€", "ï¼š", "â€”â€”", " ", ",", ".", "ï¼Œ", ":"]
    )

    def to_ts(s: str | None) -> int | None:
        if not s:
            return None
        for fmt in ("%Y-%m-%d", "%Y/%m/%d", "%Y-%m-%d %H:%M", "%Y/%m/%d %H:%M"):
            try:
                return int(datetime.strptime(s, fmt).timestamp())
            except Exception:
                pass
        return None

    for i, rec in enumerate(data, 1):
        if not isinstance(rec, dict):
            rec = {"value": rec}

        title = rec.get("title") or ""
        content = rec.get("content") or ""
        published_at = rec.get("published_at")
        published_ts = to_ts(published_at)
        url = rec.get("url")
        category = rec.get("category") or ""   # å°æ‡‰ ttu_cse_news.sorted.json

        # ç‚ºæ¯ç¯‡æ–°èç”¢ç”Ÿç©©å®š article_idï¼ˆåˆ©æ–¼é‡çµ„èˆ‡å»é‡ï¼‰
        article_key = f"{source_path}|{url or title}|{published_at or ''}|{i}"
        article_id = hashlib.sha1(article_key.encode("utf-8")).hexdigest()[:12]

        base_meta = {
            "source": source_path,
            "file_type": "json",

            "type": "news",
            "content_type": "news",

            "url": url,
            "title": title,
            "category": category,
            "published_at": published_at,
            "published_at_ts": published_ts,

            "idx": i,
            "article_id": article_id,
            "needs_split": False,
        }

        # === æƒ…æ³ä¸€ï¼šå…§æ–‡é•·åº¦ä¸è¶…é TARGET_CHARSï¼Œæ•´ç¯‡ç•¶ä¸€å€‹ doc é‡å¯« ===
        if len(content) <= TARGET_CHARS:
            record: Dict[str, Any] = {
                "æ¨™é¡Œ": title,
                "åˆ†é¡": category,
                "ç™¼å¸ƒæ™‚é–“": published_at,
                "ç¶²å€": url,
                "æ–‡ç« å…§å®¹": content,
                "ä¾†æºæª”æ¡ˆ": source_path,
                "article_id": article_id,
                "published_at_ts": published_ts,
            }

            try:
                rewritten = rewrite_json_record(
                    record=record,
                    schema_hint="news_article",
                    max_chars=TARGET_CHARS,
                )
            except Exception as e:
                print(f"[news_records_to_documents] rewrite_json_record ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}")
                sys.exit(1)

            docs.append(Document(
                page_content=rewritten.strip(),
                metadata=base_meta
            ))
            if i == 1 or i % 10 == 0 or i == total:
                print(f"[news] {source_path}ï¼šå·²å®Œæˆ {i}/{total} ç­†ï¼ˆçŸ­æ–‡ï¼‰")
            continue

        # === æƒ…æ³äºŒï¼šå…§æ–‡å¤ªé•· â†’ å…ˆåˆ‡æˆå¤šå€‹ chunkï¼Œå†é€ chunk é‡å¯« ===
        parts = splitter.split_text(content)

        # å¦‚æœæœ€å¾Œä¸€å¡Šå¤ªçŸ­ï¼Œä½µå›å‰ä¸€å¡Šï¼ˆä½ åŸæœ¬çš„é‚è¼¯ï¼‰
        if len(parts) >= 2 and len(parts[-1]) < TARGET_CHARS // 3:
            parts[-2] = parts[-2] + ("\n" if not parts[-2].endswith("\n") else "") + parts[-1]
            parts.pop()

        for j, part in enumerate(parts):
            meta = dict(base_meta)
            meta.update({"chunk": j})

            # é‡å°ã€Œæ–‡ç« æŸä¸€æ®µã€çµ„æˆ recordï¼Œè®“ LLM çŸ¥é“é€™æ˜¯åŒä¸€ç¯‡æ–°èçš„å…¶ä¸­ä¸€éƒ¨åˆ†
            record_chunk: Dict[str, Any] = {
                "æ¨™é¡Œ": title,
                "åˆ†é¡": category,
                "ç™¼å¸ƒæ™‚é–“": published_at,
                "ç¶²å€": url,
                "æ–‡ç« å…§å®¹ç‰‡æ®µ": part,
                "æ‰€å±¬ç¯‡ç«  article_id": article_id,
                "chunk_index": j,
                "ä¾†æºæª”æ¡ˆ": source_path,
                "published_at_ts": published_ts,
            }

            try:
                rewritten_chunk = rewrite_json_record(
                    record=record_chunk,
                    schema_hint="news_article_chunk",
                    max_chars=TARGET_CHARS,
                )
            except Exception as e:
                print(f"[news_records_to_documents] rewrite_json_record (chunk) ç™¼ç”ŸéŒ¯èª¤ï¼ˆç¨‹å¼çµ‚æ­¢ï¼‰ï¼š{e}")
                sys.exit(1)

            docs.append(Document(
                page_content=rewritten_chunk.strip(),
                metadata=meta
            ))

            # ä¸€ç¯‡é•·æ–‡æ‰€æœ‰ chunk éƒ½è™•ç†å®Œ
            if i == 1 or i % 10 == 0 or i == total:
                print(f"[news] {source_path}ï¼šå·²å®Œæˆ {i}/{total} ç­†ï¼ˆé•·æ–‡å¤š chunkï¼‰")

    return docs



# =========================
# JSON è¼‰å…¥èˆ‡åˆ†æ´¾
# =========================
def load_json_as_documents(path: Path) -> List[Document]:
    with open(path, "r", encoding="utf-8") as f:
        obj = json.load(f)

    schema = detect_schema(obj)
    if schema == "people":
        # 1) å–å‡º member_listï¼ˆæ”¯æ´å¤šç¨® people JSON æ ¼å¼ï¼‰
        if isinstance(obj, dict) and "ç¸½è¦½" in obj:
            overview = obj["ç¸½è¦½"] or {}
            member_list = overview.get("æˆå“¡åˆ—è¡¨", []) or []
        elif isinstance(obj, dict) and "æˆå“¡åˆ—è¡¨" in obj:
            member_list = obj.get("æˆå“¡åˆ—è¡¨", []) or []
        else:
            member_list = obj if isinstance(obj, list) else [obj]

        # 2) å€‹åˆ¥è€å¸« documents
        docs = people_records_to_documents(member_list, str(path))

        # 3) âœ… æ•™æˆç¸½è¦½ documentsï¼ˆä¸ç®¡æ ¼å¼éƒ½åŠ ï¼‰
        docs.extend(
            people_overview_to_documents(member_list, str(path), max_chars=500)
        )

        return docs
    elif schema == "news":
        data = obj if isinstance(obj, list) else [obj]
        return news_records_to_documents(data, str(path))
    elif schema == "school":
        # ç›´æ¥äº¤çµ¦ school adapter è™•ç†ï¼ˆä¸å† flattenï¼‰
        return school_info_to_documents(obj, str(path))
    elif schema == "academic_rules":
        data = obj if isinstance(obj, list) else [obj]
        return academic_records_to_documents(data, str(path))
    elif schema == "contacts":
        data = obj if isinstance(obj, list) else [obj]
        return contact_records_to_documents(data, str(path))
        # --- âœ… æ–°æ ¼å¼å·¢ç‹€èª²ç¨‹æ­·å² ---
    elif schema == "course_history_nested":
        return course_history_nested_to_documents(obj, str(path))
    elif schema == "course_overview":
        data = obj if isinstance(obj, list) else [obj]
        return course_overview_to_documents(data, str(path))
    elif schema == "program_courses":
        data = obj if isinstance(obj, list) else [obj]
        return program_courses_to_documents(data, str(path))
    elif schema == "calendar":
        data = obj if isinstance(obj, list) else [obj]
        docs = []
        docs.extend(calendar_months_to_documents(data, str(path)))   # æœˆç¸½è¦½ï¼ˆåŸæœ¬çš„ï¼‰
        docs.extend(calendar_events_to_documents(data, str(path)))   # âœ… æ–°å¢ï¼šå–®ç­†æ´»å‹•
        return docs


        # å…¶ä»–å·²çŸ¥ schema éƒ½åœ¨ä¸Šé¢è™•ç†å®Œ
    else:
        # --- é€šç”¨ï¼šç”¨ LLM å…ˆæŠŠã€Œä¸€ç­† JSON è¨˜éŒ„ã€æ”¹å¯«æˆè‡ªç„¶èªå¥ï¼Œå†ç•¶æˆ doc ---
        # æ­£è¦åŒ–æˆ list
        if isinstance(obj, list):
            json_data = obj
        else:
            json_data = [obj]

        docs: List[Document] = []
        for idx, row in enumerate(json_data):
            if not isinstance(row, dict):
                row = {"value": row}

            text = rewrite_json_record(
                row,
                schema_hint=schema or path.stem,   # ä¾‹å¦‚ "faculty", "scholarship"
                max_chars=400,
            )

            docs.append(
                Document(
                    page_content=text,
                    metadata={
                        "source": str(path),        # é€™è£¡ç›´æ¥ç”¨ path å°±å¥½
                        "idx": idx,
                        "type": "json",
                        "schema": schema or "unknown",
                    },
                )
            )

        return docs


# =========================
# å…¶ä»–æ ¼å¼è¼‰å…¥
# =========================
def load_documents(data_dir: Path) -> List[Document]:
    docs: List[Document] = []
    for path in data_dir.rglob("*"):
        suf = path.suffix.lower()
        if suf == ".pdf":
            loader = PyPDFLoader(str(path))
            # PDF loader å·²ç¶“ä¸€é ä¸€ä»½ï¼Œå¾Œé¢ä»å»ºè­°åˆ‡ä¸€ä¸‹é•·é ï¼ˆäº¤ç”±ä¸»æµç¨‹ï¼‰
            for i, d in enumerate(loader.load(), 1):
                d.metadata.update({"source": str(path), "type": "pdf", "needs_split": True, "idx": i})
                docs.append(d)
        elif suf == ".docx":
            loader = Docx2txtLoader(str(path))
            for i, d in enumerate(loader.load(), 1):
                d.metadata.update({"source": str(path), "type": "docx", "needs_split": True, "idx": i})
                docs.append(d)
        elif suf == ".txt":
            with open(path, "r", encoding="utf-8") as f:
                text = f.read()
            docs.append(Document(page_content=text, metadata={"source": str(path), "type": "txt", "needs_split": True,"idx": 1}))
        elif suf == ".csv":
            loader = CSVLoader(file_path=str(path), encoding="utf-8")
            for i, d in enumerate(loader.load(), 1):
                d.metadata.update({"source": str(path), "type": "csv", "needs_split": True, "idx": i})
                docs.append(d)
        elif suf in {".json", ".jsonl"}:
            # ä½ çš„å…©å€‹ JSONï¼ˆdepartment_members.json, ttu_cse_news.sorted.jsonï¼‰æœƒèµ°é€™æ¢
            docs.extend(load_json_as_documents(path))
        else:
            # å¿½ç•¥å…¶ä»–æ ¼å¼
            continue
    return docs


# =========================
# ä¸»æµç¨‹ï¼ˆå»ºç«‹/æ›´æ–°ç´¢å¼•ï¼‰
# =========================
def main():
    assert DATA_DIR.exists(), "è«‹å…ˆæŠŠæª”æ¡ˆæ”¾é€² data/ ç›®éŒ„ï¼ˆæ”¯æ´ JSON/PDF/DOCX/CSV/TXTï¼‰"

    print("â–¶ è®€å–æª”æ¡ˆâ€¦")
    docs = load_documents(DATA_DIR)
    print(f"â–¶ è®€åˆ° {len(docs)} ä»½åŸå§‹æ–‡ä»¶/ç‰‡æ®µï¼ˆå«å·²åˆ‡å¥½çš„ JSON æ–‡ä»¶ï¼‰")

    # å°‡éœ€è¦å†åˆ‡å¡Šçš„æ–‡ä»¶æŒ‘å‡ºä¾†åˆ‡ï¼Œå…¶ä»–ç›´æ¥ä¿ç•™
    need_split: List[Document] = [d for d in docs if d.metadata.get("needs_split", False)]
    keep: List[Document] = [d for d in docs if not d.metadata.get("needs_split", False)]

    if need_split:
        print(f"â–¶ å° {len(need_split)} ä»½æ–‡ä»¶é€²ä¸€æ­¥åˆ‡å¡Šâ€¦")
        splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=200)
        
        split_more: List[Document] = []
        for doc in need_split:
            parts = splitter.split_documents([doc])  # å°å–®ä¸€æ–‡ä»¶åˆ‡ï¼Œä¿æŒåˆ†çµ„
            for j, part in enumerate(parts):
                part.metadata["needs_split"] = False
                part.metadata["chunk"] = j          # æ¯ä»½åŸå§‹æ–‡ä»¶çš„å¡Šåº
            split_more.extend(parts)

        final_docs = keep + split_more
        print(f"â–¶ ç”¢ç”Ÿ {len(split_more)} å€‹åˆ‡å¡Šï¼›åˆè¨ˆ {len(final_docs)} ä»½å¯å…¥åº«æ–‡ä»¶")
    else:
        final_docs = keep
        print(f"â–¶ ç„¡éœ€é¡å¤–åˆ‡å¡Šï¼›åˆè¨ˆ {len(final_docs)} ä»½å¯å…¥åº«æ–‡ä»¶")

    # çµ±è¨ˆå„ type æ–¹ä¾¿ä½ æª¢æŸ¥
    from collections import Counter
    ctype_count = Counter(
        d.metadata.get("content_type", d.metadata.get("type", "unknown"))
        for d in final_docs
    )
    print("â–¶ é¡å‹çµ±è¨ˆï¼š", dict(ctype_count))

    print("â–¶ æº–å‚™åµŒå…¥æ¨¡å‹(å¤šèª)â€¦")
    embeddings = HuggingFaceEmbeddings(
        model_name="BAAI/bge-m3",
        # model_kwargs={"device": "cpu"},
        model_kwargs={"device": "cuda"},
        # model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True},  # ğŸ‘ˆ è·Ÿ query.py ä¸€æ¨£
    )

    print("â–¶ å»ºç«‹/æ›´æ–° Chroma å‘é‡åº«â€¦")
    vectordb = Chroma(
        collection_name=COLL_NAME,
        embedding_function=embeddings,
        persist_directory=DB_DIR,
        collection_metadata={"hnsw:space": "cosine"},   # è·é›¢åº¦é‡ï¼šcosine / l2 / ip
    )

    # ç©©å®š IDï¼šé¿å…é‡è¤‡å¯«å…¥ï¼›åŒå…§å®¹+åŒä¾†æºæœƒå¾—åˆ°ç›¸åŒ id
    def stable_id(doc: Document) -> str:
        src = str(doc.metadata.get("source", ""))
        typ = str(doc.metadata.get("type", ""))
        file_type = str(doc.metadata.get("file_type", ""))
        content_type = str(doc.metadata.get("content_type", doc.metadata.get("type", "")))  # èˆ‡èˆŠ 'type' ç›¸å®¹
        aid = str(doc.metadata.get("article_id", ""))  # people/å…¶ä»–å‹åˆ¥æ²’æœ‰å°±ç•™ç©º
        idx = str(doc.metadata.get("idx", ""))
        chk = str(doc.metadata.get("chunk", 0))
        raw = f"{src}|{file_type}|{content_type}|{aid}|{idx}|{chk}".encode("utf-8")
        return hashlib.sha1(raw).hexdigest()

    ids = [stable_id(d) for d in final_docs]
    # é™¤éŒ¯
    from collections import Counter
    dups = [k for k, v in Counter(ids).items() if v > 1]
    if dups:
        raise RuntimeError(f"stable_id é‡è¦† {len(dups)} ç­†ï¼Œä¾‹ï¼š{dups[:3]}")

    vectordb.add_documents(final_docs, ids=ids)

    print("âœ… å®Œæˆç´¢å¼•å»ºç«‹/æ›´æ–°ï¼è³‡æ–™åº«ä½ç½®ï¼š", DB_DIR)
    print(f"âœ… collection = {COLL_NAME}ï¼Œå…±æ–°å¢/å»é‡å¾Œå¯«å…¥ {len(final_docs)} ä»½æ–‡ä»¶")


if __name__ == "__main__":
    main()
