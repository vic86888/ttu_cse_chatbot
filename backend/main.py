from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
import asyncio
import json
import re
from typing import AsyncGenerator
from datetime import datetime
from zoneinfo import ZoneInfo
from contextlib import asynccontextmanager

# RAG / LangChain ç›¸é—œ
from operator import itemgetter
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.runnables import RunnableLambda
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from sentence_transformers import CrossEncoder
from langsmith import traceable

# ========= å…¨åŸŸè¨­å®š =========
# æ³¨æ„ï¼šé€™è£¡ä¿ç•™åŸæœ¬å¾Œç«¯ç”¨çš„ç›¸å°è·¯å¾‘
DB_DIR = "../storage/chroma"
COLL_NAME = "campus_rag"
RERANK_MODEL_NAME = "BAAI/bge-reranker-base"

# å…¨åŸŸç‰©ä»¶ï¼ˆå•Ÿå‹•æ™‚å»ºç«‹ï¼‰
reranker: CrossEncoder | None = None
chains: dict | None = None  # {"rag": ..., "rewrite": ...}

# ========= Rerank / Retriever =========
def rerank_docs(query: str, docs: list[Document], top_n: int) -> list[Document]:
    """ç”¨ cross-encoder å°å€™é¸æ–‡ä»¶é‡æ–°æ’åºï¼Œåªä¿ç•™å‰ top_nã€‚"""
    global reranker
    if not docs:
        return []
    if reranker is None:
        raise RuntimeError("reranker å°šæœªåˆå§‹åŒ–")

    pairs = [[query, d.page_content] for d in docs]
    scores = reranker.predict(pairs)  # numpy array

    scored = sorted(
        zip(docs, scores),
        key=lambda x: float(x[1]),
        reverse=True,
    )

    out: list[Document] = []
    for doc, s in scored[:top_n]:
        md = dict(doc.metadata) if doc.metadata else {}
        md["rerank_score"] = float(s)
        out.append(Document(page_content=doc.page_content, metadata=md))

    return out


def make_scored_retriever(vdb, k: int = 10):
    """Chroma + relevance score + CrossEncoder rerank"""
    # å…ˆæŠ“æ¯”è¼ƒå¤šï¼Œå†çµ¦ reranker æŒ‘å‰ k
    k_retrieve = max(k * 4, 100)

    def _retrieve(query: str):
        def as_docs(pairs):
            out = []
            for doc, score in pairs:
                md = dict(doc.metadata) if doc.metadata else {}
                md["relevance"] = float(score)
                out.append(Document(page_content=doc.page_content, metadata=md))
            return out

        pairs = vdb.similarity_search_with_relevance_scores(
            query,
            k=k_retrieve,
        )
        docs = as_docs(pairs)

        # ç”¨ cross-encoder é‡æ–°æ’åºï¼Œåªä¿ç•™å‰ k å€‹
        docs = rerank_docs(query, docs, top_n=k)
        return docs

    return RunnableLambda(_retrieve).with_config({
        "run_name": "ChromaRetriever+Reranker",
        "tags": ["retriever", "chroma", "with-scores", "rerank"],
        "metadata": {"k": k},
    })


# ========= Query Rewriter / RAG Chain =========
def build_chain():
    """å»ºç«‹å…©æ¢éˆï¼šrewrite_chainï¼ˆè™•ç†æ™‚é–“ï¼‰ + rag_chainï¼ˆçœŸæ­£å›ç­”ï¼‰"""

    # 1) LLM
    llm = ChatOllama(
        model="qwen3:latest",
        temperature=0,
    ).with_config({
        "run_name": "Ollama-LLM",
        "tags": ["ollama", "qwen3", "local"],
        "metadata": {"provider": "ollama"},
    })

    # 1.5) âœ¨ æ™‚é–“ç›¸é—œ Query Rewriter
    rewrite_prompt = ChatPromptTemplate.from_messages([
        (
            "system",
            "ä½ æ˜¯ä¸€å€‹æŸ¥è©¢æ”¹å¯«å™¨ã€‚\n"
            "ä½ çŸ¥é“ç¾åœ¨æ™‚é–“æ˜¯ï¼š{now}ï¼ˆæ™‚å€ï¼šAsia/Taipeiï¼‰ã€‚\n"
            "ç›®å‰å­¸æœŸï¼š{acad_term}\n\n"
            "å­¸å¹´ç­‰æ–¼æ°‘åœ‹ç´€å¹´,114å­¸å¹´å°±æ˜¯2025å¹´ã€‚"
            "2025å¹´8æœˆ1æ—¥~2026å¹´7æœˆ31æ—¥éƒ½æ˜¯114å­¸å¹´ä»¥æ­¤é¡æ¨ã€‚\n"
            "è«‹é–±è®€ä½¿ç”¨è€…çš„å•é¡Œï¼Œå°‡å…¶ä¸­çš„ã€Œç›¸å°æ™‚é–“ã€"
            "ï¼ˆä¾‹å¦‚ï¼šä»Šå¤©ã€æ˜å¤©ã€å¾Œå¤©ã€é€™é€±ã€ä¸‹é€±ã€ä¸Šé€±ã€ä¸Šå€‹æœˆã€ä¸‹å€‹æœˆã€æœ€è¿‘å¹¾å¤©ã€é€™å­¸æœŸã€ä¸‹å­¸æœŸã€ä»Šå¹´ã€æ˜å¹´ç­‰ï¼‰\n"
            "æ›ç®—æˆã€Œæ˜ç¢ºçš„æ—¥æœŸæˆ–å¹´æœˆã€å¾Œï¼Œæ”¹å¯«æˆä¸€å€‹æ–°çš„å•é¡Œå¥å­ã€‚\n"
            "è¦å‰‡ï¼š\n"
            "1. å¦‚æœå•é¡Œä¸­æ²’æœ‰ç›¸å°æ™‚é–“ï¼Œå°±åŸå°ä¸å‹•è¼¸å‡ºåŸå§‹å•é¡Œã€‚\n"
            "2. ä¸€å¾‹ä½¿ç”¨è¥¿å…ƒå¹´ä»½ï¼ˆä¾‹å¦‚ï¼š2025å¹´10æœˆï¼‰ï¼Œä¸è¦ä½¿ç”¨æ°‘åœ‹å¹´ï¼ˆä¾‹å¦‚ï¼šæ°‘åœ‹114å¹´ï¼‰ã€‚\n"
            "3. ä¸è¦åŠ å…¥ã€Œå­¸å¹´åº¦ã€ã€Œå­¸å¹´ã€ã€Œå­¸æœŸã€ç­‰å­—çœ¼ï¼Œé™¤éä½¿ç”¨è€…åŸæœ¬å•é¡Œå°±æœ‰ã€‚\n"
            "4. åƒ…è¼¸å‡ºæ”¹å¯«å¾Œçš„å•é¡Œï¼Œä¸è¦ä»»ä½•è§£é‡‹ã€ä¸è¦åŠ å‰ç¶´ã€ä¸è¦å¤šè¡Œèªªæ˜ã€‚\n"
        ),
        ("human", "{query}"),
    ]).with_config({
        "run_name": "TemporalQueryRewriter",
        "tags": ["query-rewrite", "temporal"],
    })

    rewrite_chain = (rewrite_prompt | llm | StrOutputParser()).with_config({
        "run_name": "RewriteChain",
        "tags": ["chain", "rewrite"],
    })

    # 2) æç¤ºè©
    prompt = ChatPromptTemplate.from_messages([
        ("system",
        "ç¾åœ¨æ™‚é–“ï¼š{now}\n\n"
        "ç›®å‰å­¸æœŸï¼š{acad_term}\n\n"
        "å­¸å¹´ç­‰æ–¼æ°‘åœ‹ç´€å¹´,114å­¸å¹´å°±æ˜¯2025å¹´ã€‚"
        "2025å¹´8æœˆ1æ—¥~2026å¹´7æœˆ31æ—¥éƒ½æ˜¯114å­¸å¹´ä»¥æ­¤é¡æ¨ã€‚\n"
        "ä½ æ˜¯å¤§åŒå¤§å­¸è³‡å·¥ç³»å•ç­”æ©Ÿå™¨äººã€‚\n"
        "ä½ æœƒå¾—åˆ°è·Ÿå•é¡Œç›¸é—œçš„æ–‡ä»¶ä»¥åŠç³»çµ±æä¾›çš„è³‡è¨Šï¼ˆå¦‚ç¾åœ¨æ™‚é–“ã€ç›®å‰å­¸æœŸï¼‰,ä½ åªä¾æ“šæä¾›çš„æ–‡ä»¶å…§å®¹å›ç­”å•é¡Œ,"
        "å›ç­”çµå°¾éœ€é™„ä¸Šå›ç­”æ™‚åƒè€ƒè³‡æ–™çš„ä¾†æºç¶²å€ï¼Œè‹¥æ²’æœ‰å‰‡é™„ä¸Šä¾†æºæ–‡ä»¶ã€‚"
        "è‹¥ç„¡æ³•å¾æ–‡ä»¶ä¸­æ‰¾åˆ°ç­”æ¡ˆ,è«‹æ¸…æ¥šèªªæ˜ã€‚\n\n"
        "è«‹ä»¥ç¹é«”ä¸­æ–‡ä½œç­”ï¼Œä¸¦ä½¿ç”¨ Markdown æ ¼å¼ä¾†çµ„ç¹”ç­”æ¡ˆï¼š\n"
        "- ä½¿ç”¨ ## äºŒç´šæ¨™é¡Œä¾†åˆ†éš”ä¸åŒä¸»é¡Œ\n"
        "- ä½¿ç”¨åˆ—è¡¨ (- æˆ– 1.) ä¾†å‘ˆç¾å¤šå€‹é …ç›®\n"
        "- ä½¿ç”¨ **ç²—é«”** ä¾†å¼·èª¿é‡è¦è³‡è¨Š\n"
        "- ä½¿ç”¨ç¨‹å¼ç¢¼å€å¡Š ```èªè¨€ ä¾†å±•ç¤ºç¨‹å¼ç¢¼\n"
        "- ä½¿ç”¨è¡¨æ ¼ä¾†å‘ˆç¾çµæ§‹åŒ–è³‡æ–™\n\n"
        "{context}"),
        ("human", "{input}")
    ])

    # 3) combine documents chain
    doc_chain = create_stuff_documents_chain(
        llm=llm,
        prompt=prompt,
    ).with_config({
        "run_name": "StuffDocumentsChain",
        "tags": ["chain", "stuff"],
    })

    # 4) å‘é‡åº« & æª¢ç´¢å™¨ï¼ˆå«åˆ†æ•¸ï¼‰
    embeddings = HuggingFaceEmbeddings(
        model_name="BAAI/bge-m3",
        model_kwargs={"device": "cuda"},  # è‹¥æ²’æœ‰ GPU å¯æ”¹æˆ "cpu"
        encode_kwargs={"normalize_embeddings": True},
    )
    vectordb = Chroma(
        collection_name=COLL_NAME,
        embedding_function=embeddings,
        persist_directory=DB_DIR,
    )

    scored_retriever = make_scored_retriever(vectordb, k=10)
    # create_retrieval_chain æœŸæœ› retriever æ¥æ”¶æ•´å€‹ input dict
    retriever_runnable = itemgetter("input") | scored_retriever

    # 5) RAG éˆï¼ˆretriever + combine_docsï¼‰
    rag_chain = create_retrieval_chain(retriever_runnable, doc_chain).with_config({
        "run_name": "CampusRAG",
        "tags": ["campus-rag", "api"],
    })

    return {
        "rag": rag_chain,
        "rewrite": rewrite_chain,
    }


# ========= å·¥å…·å‡½å¼ =========
def extract_clean_query(text: str) -> str:
    """å¾ rewriter è¼¸å‡ºä¸­æŠ½å‡ºã€çœŸæ­£è¦æ‹¿å»ç•¶ query çš„é‚£å¥è©±ã€"""
    if not text:
        return ""

    # 1) è‹¥æœ‰ <answer>...</answer>ï¼Œå„ªå…ˆç”¨è£¡é¢çš„å…§å®¹
    m = re.search(r"<answer>(.*?)</answer>", text, re.DOTALL)
    if m:
        return m.group(1).strip()

    # 2) æŠŠ <think>...</think> æ•´å¡Šç æ‰
    text = re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL).strip()

    # 3) åªç•™æœ€å¾Œä¸€è¡Œï¼ˆé€šå¸¸æ˜¯å¯¦éš›å•é¡Œï¼‰
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    if not lines:
        return ""
    return lines[-1]


def remove_thinking_tags(text: str) -> str:
    """å¾å›æ‡‰æ–‡æœ¬ä¸­ç§»é™¤æ¨¡å‹çš„æ€è€ƒéç¨‹ï¼ˆ<think>...</think> æ¨™ç±¤ï¼‰"""
    if not text:
        return ""
    # ç§»é™¤æ‰€æœ‰ <think>...</think> æ¨™ç±¤åŠå…¶å…§å®¹
    text = re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL).strip()
    return text


@traceable(name="API-Ask", run_type="chain", metadata={"app": "campus_rag_api"})
def ask(chain_dict: dict, q: str):
    """åŸ·è¡ŒæŸ¥è©¢ï¼šå…ˆç”¨ LLM åšæ™‚é–“æ”¹å¯«ï¼Œå†ä¸Ÿçµ¦ RAG"""
    rag_chain = chain_dict["rag"]
    rewrite_chain = chain_dict["rewrite"]

    now = datetime.now(ZoneInfo("Asia/Taipei"))
    roc_year = now.year - 1911
    m, d = now.month, now.day

    # å­¸å¹´å­¸æœŸè¨ˆç®—ï¼š8/1 æ–°å­¸å¹´ï¼›2/1 ç¬¬äºŒå­¸æœŸ
    if (m, d) >= (8, 1):
        acad_year = roc_year
        sem = "ç¬¬ä¸€å­¸æœŸ"
    elif (m, d) >= (2, 1):
        acad_year = roc_year - 1
        sem = "ç¬¬äºŒå­¸æœŸ"
    else:
        acad_year = roc_year - 1
        sem = "ç¬¬ä¸€å­¸æœŸ"

    acad_term = f"{acad_year}å­¸å¹´{sem}"
    now_str = f"æ°‘åœ‹{roc_year}å¹´{m}æœˆ{d}æ—¥ {now.strftime('%H:%M')}"

    # å…ˆè®“ rewrite_chain æŠŠç›¸å°æ™‚é–“æ”¹å¯«
    try:
        rewritten_q = rewrite_chain.invoke({
            "query": q,
            "now": now_str,
        }).strip()
        rewritten_q = extract_clean_query(rewritten_q)
    except Exception:
        rewritten_q = q

    if not rewritten_q:
        rewritten_q = q

    print(f"[DEBUG] rewritten query: {rewritten_q!r}")

    # å†æŠŠæ”¹å¯«å¾Œçš„ query ä¸Ÿé€² RAG
    result = rag_chain.invoke({
        "input": rewritten_q,
        "now": now_str,
        "acad_term": acad_term,
        "original_query": q,
        "rewritten_query": rewritten_q,
    })

    # â­â­ åœ¨é€™è£¡æŠŠ <think>...</think> ç§»æ‰ï¼Œåªç•™ä¸‹çœŸæ­£è¦é¡¯ç¤ºçš„å›ç­”
    raw_answer = result.get("answer", "")
    clean_answer = remove_thinking_tags(raw_answer)
    result["answer"] = clean_answer

    return result


# ========= FastAPI App & Lifespan =========
@asynccontextmanager
async def lifespan(app: FastAPI):
    """æ‡‰ç”¨å•Ÿå‹• / é—œé–‰ç”Ÿå‘½é€±æœŸ"""
    global reranker, chains

    print("ğŸ”„ è¼‰å…¥ reranker æ¨¡å‹...")
    reranker = CrossEncoder(RERANK_MODEL_NAME, device="cuda")
    print("ğŸ”„ å»ºç«‹ RAG chains...")
    chains = build_chain()
    print("âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼")

    yield

    print("ğŸ‘‹ é—œé–‰æ‡‰ç”¨...")


app = FastAPI(title="TTU CSE Chatbot API", lifespan=lifespan)

# CORS è¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:5173",
        "http://localhost:5174",
        "http://localhost:3000",
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ========= åŸºæœ¬è·¯ç”± =========
@app.get("/")
async def root():
    return {"message": "TTU CSE Chatbot API is running"}


@app.get("/favicon.ico")
async def favicon():
    """é˜²æ­¢ favicon 404 éŒ¯èª¤"""
    return {"message": "No favicon"}


@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "rag_ready": chains is not None,
    }


# ========= SSE ä¸²æµ =========
async def generate_stream(message: str) -> AsyncGenerator[str, None]:
    """ç”Ÿæˆ SSE ä¸²æµå›æ‡‰ï¼ˆä½¿ç”¨æ–°çš„ RAG + rewriter é‚è¼¯ï¼‰"""
    try:
        global chains
        if chains is None:
            raise RuntimeError("RAG chains å°šæœªåˆå§‹åŒ–")

        # åœ¨èƒŒæ™¯ thread è·‘å®Œæ•´ askï¼ˆå« rewrite + RAGï¼‰
        result = await asyncio.to_thread(ask, chains, message)
        response_text = result.get("answer", "")

        # é€å­—ä¸²æµ
        for char in response_text:
            payload = {"content": char, "done": False}
            yield f"data: {json.dumps(payload, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.02)

        # ç™¼é€å®Œæˆè¨Šè™Ÿ
        yield f"data: {json.dumps({'content': '', 'done': True}, ensure_ascii=False)}\n\n"

    except Exception as e:
        error_msg = f"æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„å•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}"
        payload = {"content": error_msg, "done": True, "error": True}
        yield f"data: {json.dumps(payload, ensure_ascii=False)}\n\n"


@app.get("/api/chat/stream")
async def chat_stream(message: str):
    """SSE ä¸²æµèŠå¤©ç«¯é»ï¼ˆGET /api/chat/stream?message=...ï¼‰"""
    return StreamingResponse(
        generate_stream(message),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        },
    )


# ========= éä¸²æµ API =========
@app.post("/api/chat")
async def chat(request: dict):
    """éä¸²æµèŠå¤©ç«¯é»ï¼ˆä½¿ç”¨æ–°çš„ RAG + rewriter é‚è¼¯ï¼‰"""
    try:
        global chains
        if chains is None:
            raise RuntimeError("RAG chains å°šæœªåˆå§‹åŒ–")

        message = request.get("message", "")

        result = await asyncio.to_thread(ask, chains, message)

        return {
            "response": result.get("answer", ""),
            "original_query": result.get("original_query", message),
            "rewritten_query": result.get("rewritten_query", message),
            "now": result.get("now"),
            "acad_term": result.get("acad_term"),
            "sources": [
                {
                    "content": doc.page_content[:200],
                    "source": doc.metadata.get("source", "unknown"),
                    "relevance": doc.metadata.get("relevance"),
                    "rerank_score": doc.metadata.get("rerank_score"),
                }
                for doc in result.get("context", [])
            ],
        }
    except Exception as e:
        return {
            "response": f"æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„å•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}",
            "error": True,
        }


if __name__ == "__main__":
    import uvicorn

    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=False)
