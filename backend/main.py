from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
import asyncio
import json
from typing import AsyncGenerator
from datetime import datetime
from zoneinfo import ZoneInfo
from contextlib import asynccontextmanager

# RAG ç›¸é—œå¥—ä»¶
from operator import itemgetter
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.runnables import RunnableLambda
from langchain_core.documents import Document
from sentence_transformers import CrossEncoder

# å…¨åŸŸè®Šæ•¸
DB_DIR = "../storage/chroma"
COLL_NAME = "campus_rag"
EVENT_KEYWORDS = [
    "æ–°è", "æ¶ˆæ¯", "news", "æœ€æ–°",
    "æœ€è¿‘", "æ´»å‹•", "èªªæ˜æœƒ", "è¬›åº§", "è«–å£‡", "ç‡ŸéšŠ", "å¾µæ‰"
]
RERANK_MODEL_NAME = "BAAI/bge-reranker-base"

# åˆå§‹åŒ– reranker å’Œ chainï¼ˆå•Ÿå‹•æ™‚è¼‰å…¥ï¼‰
reranker = None
rag_chain = None

def rerank_docs(query: str, docs: list[Document], top_n: int) -> list[Document]:
    """ç”¨ cross-encoder å°å€™é¸æ–‡ä»¶é‡æ–°æ’åº"""
    if not docs:
        return []
    
    pairs = [[query, d.page_content] for d in docs]
    scores = reranker.predict(pairs)
    
    scored = sorted(zip(docs, scores), key=lambda x: float(x[1]), reverse=True)
    
    out: list[Document] = []
    for doc, s in scored[:top_n]:
        md = dict(doc.metadata) if doc.metadata else {}
        md["rerank_score"] = float(s)
        out.append(Document(page_content=doc.page_content, metadata=md))
    
    return out

def make_scored_retriever(vdb, k: int = 10):
    """å»ºç«‹åŒ…å«åˆ†æ•¸çš„æª¢ç´¢å™¨"""
    k_retrieve = max(k * 4, 20)
    
    def _retrieve(query: str):
        def as_docs(pairs):
            out = []
            for doc, score in pairs:
                md = dict(doc.metadata) if doc.metadata else {}
                md["relevance"] = float(score)
                out.append(Document(page_content=doc.page_content, metadata=md))
            return out
        
        q = (query or "").lower()
        prefer_news = any(kw in q for kw in EVENT_KEYWORDS)
        
        docs: list[Document] = []
        if prefer_news:
            news_pairs = vdb.similarity_search_with_relevance_scores(
                query, k=k_retrieve, filter={"content_type": "news"}
            )
            docs.extend(as_docs(news_pairs))
            
            if len(docs) < k_retrieve:
                more_pairs = vdb.similarity_search_with_relevance_scores(query, k=k_retrieve)
                docs.extend(as_docs(more_pairs))
            
            # å»é‡
            seen = set()
            uniq = []
            for d in docs:
                md = d.metadata or {}
                key = (
                    ("article", md.get("source"), md.get("article_id"))
                    if md.get("article_id")
                    else ("row", md.get("source"), md.get("idx"))
                )
                if key in seen:
                    continue
                seen.add(key)
                uniq.append(d)
            docs = uniq
        else:
            pairs = vdb.similarity_search_with_relevance_scores(query, k=k_retrieve)
            docs = as_docs(pairs)
        
        # é‡æ–°æ’åº
        docs = rerank_docs(query, docs, top_n=k)
        return docs
    
    return RunnableLambda(_retrieve)

def build_chain():
    """å»ºç«‹ RAG chain"""
    # 1) LLM
    llm = ChatOllama(
        model="qwen3:latest",
        temperature=0,
    )
    
    # 2) æç¤ºè©
    prompt = ChatPromptTemplate.from_messages([
        ("system",
        "ä½ æ˜¯å¤§åŒå¤§å­¸è³‡å·¥ç³»å•ç­”æ©Ÿå™¨äººã€‚\n"
        "å­¸å¹´ç­‰æ–¼æ°‘åœ‹ç´€å¹´,114å­¸å¹´å°±æ˜¯2025å¹´ã€‚"
        "ä½ æœƒå¾—åˆ°è·Ÿå•é¡Œç›¸é—œçš„æ–‡ä»¶,ä½ åªä¾æ“šæä¾›çš„æ–‡ä»¶å…§å®¹å›ç­”å•é¡Œ,"
        "è‹¥ç„¡æ³•å¾æ–‡ä»¶ä¸­æ‰¾åˆ°ç­”æ¡ˆ,è«‹æ¸…æ¥šèªªæ˜ã€‚\n\n"
        "è«‹ä»¥ç¹é«”ä¸­æ–‡ä½œç­”ï¼Œä¸¦ä½¿ç”¨ Markdown æ ¼å¼ä¾†çµ„ç¹”ç­”æ¡ˆï¼š\n"
        "- ä½¿ç”¨ ## äºŒç´šæ¨™é¡Œä¾†åˆ†éš”ä¸åŒä¸»é¡Œ\n"
        "- ä½¿ç”¨åˆ—è¡¨ (- æˆ– 1.) ä¾†å‘ˆç¾å¤šå€‹é …ç›®\n"
        "- ä½¿ç”¨ **ç²—é«”** ä¾†å¼·èª¿é‡è¦è³‡è¨Š\n"
        "- ä½¿ç”¨ç¨‹å¼ç¢¼å€å¡Š ```èªè¨€ ä¾†å±•ç¤ºç¨‹å¼ç¢¼\n"
        "- ä½¿ç”¨è¡¨æ ¼ä¾†å‘ˆç¾çµæ§‹åŒ–è³‡æ–™\n\n"
        "{context}"),
        ("human", "{input}")
    ])
    
    # 3) stuff chain
    doc_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)
    
    # 4) å‘é‡åº« & æª¢ç´¢å™¨
    embeddings = HuggingFaceEmbeddings(
        model_name="BAAI/bge-m3",
        model_kwargs={"device": "cuda"},
        encode_kwargs={"normalize_embeddings": True},
    )
    vectordb = Chroma(
        collection_name=COLL_NAME,
        embedding_function=embeddings,
        persist_directory=DB_DIR,
    )
    
    scored_retriever = make_scored_retriever(vectordb, k=10)
    retriever_runnable = itemgetter("input") | scored_retriever
    
    # 5) RAG éˆ
    rag_chain = create_retrieval_chain(retriever_runnable, doc_chain)
    return rag_chain

@asynccontextmanager
async def lifespan(app: FastAPI):
    """æ‡‰ç”¨ç”Ÿå‘½é€±æœŸç®¡ç†"""
    global reranker, rag_chain
    print("ğŸ”„ è¼‰å…¥ reranker æ¨¡å‹...")
    reranker = CrossEncoder(RERANK_MODEL_NAME, device="cuda")
    print("ğŸ”„ å»ºç«‹ RAG chain...")
    rag_chain = build_chain()
    print("âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼")
    yield
    print("ğŸ‘‹ é—œé–‰æ‡‰ç”¨...")

app = FastAPI(title="TTU CSE Chatbot API", lifespan=lifespan)

# CORS è¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://localhost:3000"],  # Vite é è¨­åŸ 
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
async def root():
    return {"message": "TTU CSE Chatbot API is running"}

@app.get("/health")
async def health_check():
    return {"status": "healthy", "rag_ready": rag_chain is not None}

async def generate_stream(message: str) -> AsyncGenerator[str, None]:
    """ç”Ÿæˆ SSE ä¸²æµå›æ‡‰ï¼ˆä½¿ç”¨ RAGï¼‰"""
    try:
        # åŠ ä¸Šç•¶å‰æ™‚é–“
        now = datetime.now(ZoneInfo("Asia/Taipei"))
        roc_year = now.year - 1911
        today_roc = f"{roc_year}å¹´{now.month}æœˆ{now.day}æ—¥"
        timestamped_q = f"[ç•¶å‰æ™‚é–“: {today_roc}] {message}"
        
        # åŸ·è¡Œ RAG æŸ¥è©¢
        result = await asyncio.to_thread(
            rag_chain.invoke,
            {"input": timestamped_q}
        )
        
        response_text = result["answer"]
        
        # é€å­—ä¸²æµ
        for char in response_text:
            yield f"data: {json.dumps({'content': char, 'done': False}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.02)
        
        # ç™¼é€å®Œæˆè¨Šè™Ÿ
        yield f"data: {json.dumps({'content': '', 'done': True})}\n\n"
        
    except Exception as e:
        error_msg = f"æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„å•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}"
        yield f"data: {json.dumps({'content': error_msg, 'done': True, 'error': True}, ensure_ascii=False)}\n\n"

@app.get("/api/chat/stream")
async def chat_stream(message: str):
    """SSE ä¸²æµèŠå¤©ç«¯é»"""
    return StreamingResponse(
        generate_stream(message),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        }
    )

@app.post("/api/chat")
async def chat(request: dict):
    """éä¸²æµèŠå¤©ç«¯é»ï¼ˆä½¿ç”¨ RAGï¼‰"""
    try:
        message = request.get("message", "")
        
        # åŠ ä¸Šç•¶å‰æ™‚é–“
        now = datetime.now(ZoneInfo("Asia/Taipei"))
        roc_year = now.year - 1911
        today_roc = f"{roc_year}å¹´{now.month}æœˆ{now.day}æ—¥"
        timestamped_q = f"[ç•¶å‰æ™‚é–“: {today_roc}] {message}"
        
        # åŸ·è¡Œ RAG æŸ¥è©¢
        result = await asyncio.to_thread(
            rag_chain.invoke,
            {"input": timestamped_q}
        )
        
        return {
            "response": result["answer"],
            "sources": [
                {
                    "content": doc.page_content[:200],
                    "source": doc.metadata.get("source", "unknown"),
                    "relevance": doc.metadata.get("relevance"),
                    "rerank_score": doc.metadata.get("rerank_score")
                }
                for doc in result.get("context", [])
            ]
        }
    except Exception as e:
        return {"response": f"æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„å•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}", "error": True}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)
